<!DOCTYPE html><html lang="[&quot;zh-CN&quot;,&quot;en&quot;,&quot;default&quot;]" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>PyTorch教学 | butyuhao</title><meta name="description" content="PyTorch教学"><meta name="keywords" content="李宏毅"><meta name="author" content="butyuhao"><meta name="copyright" content="butyuhao"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/code.png"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="twitter:card" content="summary"><meta name="twitter:title" content="PyTorch教学"><meta name="twitter:description" content="PyTorch教学"><meta name="twitter:image" content="http://butyuhao.xyz/cover_img/tree-eval.png"><meta property="og:type" content="article"><meta property="og:title" content="PyTorch教学"><meta property="og:url" content="http://butyuhao.xyz/2020/03/21/PyTorch%E6%95%99%E5%AD%A6/"><meta property="og:site_name" content="butyuhao"><meta property="og:description" content="PyTorch教学"><meta property="og:image" content="http://butyuhao.xyz/cover_img/tree-eval.png"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>const autoChangeMode = '1'
var t = Cookies.get("theme")
if (autoChangeMode == '1'){
  const isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
  const isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
  const isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
  const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined){
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport){
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      now = new Date();
      hour = now.getHours();
      isNight = hour < 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
  }
  } else if (t == 'light') activateLightMode()
  else activateDarkMode()

} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="canonical" href="http://butyuhao.xyz/2020/03/21/PyTorch%E6%95%99%E5%AD%A6/"><link rel="next" title="Pandas入门" href="http://butyuhao.xyz/2020/03/20/Pandas%E5%85%A5%E9%97%A8/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"簡","msgToSimplifiedChinese":"簡"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    title: 'Snackbar.bookmark.title',
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  highlightShrink: false,
  isFontAwesomeV5: false
  
}</script><script>var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false
}</script><meta name="generator" content="Hexo 4.2.0"></head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/avatar.JPG" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">2</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">2</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-list" aria-hidden="true"></i><span> List</span><i class="fa fa-chevron-down menus-expand" aria-hidden="true"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fa fa-music"></i><span> Music</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fa fa-film"></i><span> Movie</span></a></li></ul></div></div></div></div><i class="fa fa-arrow-right on" id="toggle-sidebar" aria-hidden="true">     </i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#PyTorch-介绍"><span class="toc-number">1.</span> <span class="toc-text">PyTorch 介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Tensor与numpy的关系"><span class="toc-number">2.</span> <span class="toc-text">Tensor与numpy的关系</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Tensor-view"><span class="toc-number">3.</span> <span class="toc-text">Tensor.view</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#计算图"><span class="toc-number">4.</span> <span class="toc-text">计算图</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#使用Pytorch来计算梯度"><span class="toc-number">5.</span> <span class="toc-text">使用Pytorch来计算梯度</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#梯度下降"><span class="toc-number">6.</span> <span class="toc-text">梯度下降</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#线性回归"><span class="toc-number">7.</span> <span class="toc-text">线性回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#验证梯度计算的正确性"><span class="toc-number">8.</span> <span class="toc-text">验证梯度计算的正确性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#使用梯度下降算法，基于自动求导功能的线性回归"><span class="toc-number">9.</span> <span class="toc-text">使用梯度下降算法，基于自动求导功能的线性回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#torch-nn-Module"><span class="toc-number">10.</span> <span class="toc-text">torch.nn.Module</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Linear-Module"><span class="toc-number">11.</span> <span class="toc-text">Linear Module</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#激活函数"><span class="toc-number">12.</span> <span class="toc-text">激活函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Sequential"><span class="toc-number">13.</span> <span class="toc-text">Sequential</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#损失函数"><span class="toc-number">14.</span> <span class="toc-text">损失函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#torch-optim"><span class="toc-number">15.</span> <span class="toc-text">torch.optim</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#用梯度下降、自动求导与PyTorch模块实现线性回归"><span class="toc-number">16.</span> <span class="toc-text">用梯度下降、自动求导与PyTorch模块实现线性回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#使用SGD完成线性回归"><span class="toc-number">17.</span> <span class="toc-text">使用SGD完成线性回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CrossEntropyLoss"><span class="toc-number">18.</span> <span class="toc-text">CrossEntropyLoss</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#动态改变学习速率"><span class="toc-number">19.</span> <span class="toc-text">动态改变学习速率</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#卷积"><span class="toc-number">20.</span> <span class="toc-text">卷积</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#一些蛮有用的链接"><span class="toc-number">21.</span> <span class="toc-text">一些蛮有用的链接</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#数据类"><span class="toc-number">22.</span> <span class="toc-text">数据类</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#混合精度训练"><span class="toc-number">23.</span> <span class="toc-text">混合精度训练</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Apex"><span class="toc-number">24.</span> <span class="toc-text">Apex</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#apex-amp"><span class="toc-number">25.</span> <span class="toc-text">apex.amp</span></a></li></ol></div></div></div><div id="body-wrap"><div class="post-bg" id="nav" style="background-image: url(/cover_img/tree-eval.png)"><div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">butyuhao</a></span><span class="toggle-menu pull_right close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-list" aria-hidden="true"></i><span> List</span><i class="fa fa-chevron-down menus-expand" aria-hidden="true"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fa fa-music"></i><span> Music</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fa fa-film"></i><span> Movie</span></a></li></ul></div></div></span><span class="pull_right" id="search_button"><a class="site-page social-icon search"><i class="fa fa-search fa-fw"></i><span> 搜索</span></a></span></div><div id="post-info"><div id="post-title"><div class="posttitle">PyTorch教学</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 发表于 2020-03-21<span class="post-meta__separator">|</span><i class="fa fa-history" aria-hidden="true"></i> 更新于 2020-03-25</time><div class="post-meta-wordcount"><div class="post-meta-pv-cv"><i class="fa fa-eye post-meta__icon" aria-hidden="true"> </i><span>阅读量:</span><span id="busuanzi_value_page_pv"></span></div></div></div></div></div><main class="layout_post" id="content-inner"><article id="post"><div id="article-container"><h2 id="PyTorch-介绍"><a href="#PyTorch-介绍" class="headerlink" title="PyTorch 介绍"></a>PyTorch 介绍</h2><p>PyTorch是一款强有力的深度学习框架，不管是科研还是生产部署的需求它都能满足。<br>本教程仅提供粗略的介绍，如果有任何疑问可以上网查查或是去咨询朋友。（小声bb：多google少百度）<br>一些用得上的point：</p>
<ul>
<li><p>自动differentiation工具非常强大</p>
</li>
<li><p>PyTorch为你实现好了深度学习中的常用功能</p>
</li>
<li><p>使用PyTorch DataSet来处理数据</p>
</li>
</ul>
<h2 id="Tensor与numpy的关系"><a href="#Tensor与numpy的关系" class="headerlink" title="Tensor与numpy的关系"></a>Tensor与numpy的关系</h2><p>PyTorch的基本组成部分，block与numpy的ndarray相似。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># we create tensors in a similar way to numpy nd arrays</span></span><br><span class="line">x_numpy = np.array([<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>])</span><br><span class="line">x_torch = torch.tensor([<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>])</span><br><span class="line">print(<span class="string">'x_numpy, x_torch'</span>)</span><br><span class="line">print(x_numpy, x_torch)</span><br><span class="line">print()</span><br><span class="line"></span><br><span class="line"><span class="comment"># to and from numpy, pytorch</span></span><br><span class="line">print(<span class="string">'to and from numpy and pytorch'</span>)</span><br><span class="line">print(torch.from_numpy(x_numpy), x_torch.numpy())</span><br><span class="line">print()</span><br><span class="line"></span><br><span class="line"><span class="comment"># we can do basic operations like +-*/</span></span><br><span class="line">y_numpy = np.array([<span class="number">3</span>,<span class="number">4</span>,<span class="number">5.</span>])</span><br><span class="line">y_torch = torch.tensor([<span class="number">3</span>,<span class="number">4</span>,<span class="number">5.</span>])</span><br><span class="line">print(<span class="string">"x+y"</span>)</span><br><span class="line">print(x_numpy + y_numpy, x_torch + y_torch)</span><br><span class="line">print()</span><br><span class="line"></span><br><span class="line"><span class="comment"># many functions that are in numpy are also in pytorch</span></span><br><span class="line">print(<span class="string">"norm"</span>)</span><br><span class="line">print(np.linalg.norm(x_numpy), torch.norm(x_torch))</span><br><span class="line">print()</span><br><span class="line"></span><br><span class="line"><span class="comment"># to apply an operation along a dimension,</span></span><br><span class="line"><span class="comment"># we use the dim keyword argument instead of axis</span></span><br><span class="line">print(<span class="string">"mean along the 0th dimension"</span>)</span><br><span class="line">x_numpy = np.array([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4.</span>]])</span><br><span class="line">x_torch = torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4.</span>]])</span><br><span class="line">print(np.mean(x_numpy, axis=<span class="number">0</span>), torch.mean(x_torch, dim=<span class="number">0</span>))</span><br></pre></td></tr></table></figure>
<h2 id="Tensor-view"><a href="#Tensor-view" class="headerlink" title="Tensor.view"></a>Tensor.view</h2><p>和numpy.reshape()一样，我们可以使用tensor.view()来改变tensor的形状。当使用-1时，其自动计算剩下维度的值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># "MNIST"</span></span><br><span class="line">N, C, W, H = <span class="number">10000</span>, <span class="number">3</span>, <span class="number">28</span>, <span class="number">28</span></span><br><span class="line">X = torch.randn((N, C, W, H))</span><br><span class="line"></span><br><span class="line">print(X.shape)</span><br><span class="line">print(X.view(N, C, <span class="number">784</span>).shape)</span><br><span class="line">print(X.view(<span class="number">-1</span>, C, <span class="number">784</span>).shape) <span class="comment"># automatically choose the 0th dimension</span></span><br></pre></td></tr></table></figure>
<h2 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h2><p>PyTorch的tensor有一个特别的地方，当tensor对象时，Pytorch自动在后台创建“计算图”。计算图是使用图的方式来表达一个数学表达式。其带有一种算法来计算一个计算图中所有变量的梯度，计算顺序和函数本身的顺序一致。<br>假设表达式为$e=(a+b)*(b+1)$,其中，$a=2, b=1$我们可以画出如下的计算图：</p>
<p><img src="/" alt class="lazyload" data-src="/2020/03/21/PyTorch%E6%95%99%E5%AD%A6/tree-eval.png"></p>
<p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor(<span class="number">2.0</span>, requires_grad=<span class="literal">True</span>) <span class="comment"># we set requires_grad=True to let PyTorch know to keep the graph</span></span><br><span class="line">b = torch.tensor(<span class="number">1.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">c = a + b</span><br><span class="line">d = b + <span class="number">1</span></span><br><span class="line">e = c * d</span><br><span class="line">print(<span class="string">'c'</span>, c)</span><br><span class="line">print(<span class="string">'d'</span>, d)</span><br><span class="line">print(<span class="string">'e'</span>, e)</span><br></pre></td></tr></table></figure>

<p>执行结果为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;</span><br><span class="line">c tensor(<span class="number">3.</span>, grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">d tensor(<span class="number">2.</span>, grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">e tensor(<span class="number">6.</span>, grad_fn=&lt;MulBackward0&gt;)</span><br></pre></td></tr></table></figure>

<p>可以看到，PyTorch为我们记录了计算图。</p>
<h2 id="使用Pytorch来计算梯度"><a href="#使用Pytorch来计算梯度" class="headerlink" title="使用Pytorch来计算梯度"></a>使用Pytorch来计算梯度</h2><p>我们已经发现PyTorch会自动为我们记录计算图，现在，让我们用它来求一求梯度。</p>
<p>考虑这样一个方程：$f(x)=(x-2)^2$。</p>
<p>问题：计算${df(x)}\over{dx}$，求$f’(1)$。</p>
<p>我们对leaf variable <code>y</code>调用<code>backward()</code>方法，一次性计算<code>y</code>的所有梯度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (x<span class="number">-2</span>)**<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fp</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span>*(x<span class="number">-2</span>)</span><br><span class="line"></span><br><span class="line">x = torch.tensor([<span class="number">1.0</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">y = f(x)</span><br><span class="line">y.backward()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Analytical f\'(x):'</span>, fp(x))</span><br><span class="line">print(<span class="string">'PyTorch\'s f\'(x):'</span>, x.grad)</span><br></pre></td></tr></table></figure>

<p>输出为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;</span><br><span class="line">Analytical <span class="string">f'(x): tensor([-2.], grad_fn=&lt;MulBackward0&gt;) #带有grad_fn</span></span><br><span class="line"><span class="string">PyTorch'</span>s <span class="string">f'(x): tensor([-2.])</span></span><br></pre></td></tr></table></figure>

<p><code>backward()</code>也可以计算带有数学函数的表达式的梯度：</p>
<p>现有$w=[w_1,w_2]^T$，</p>
<p>$g(w)=2w_1w_2+w_2cos(w_1)$</p>
<p>问题：计算&nabla;$_w g(w)$然后验证&nabla;$_w g([\pi,1]) = [2,\pi-1]^T$。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">g</span><span class="params">(w)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span>*w[<span class="number">0</span>]*w[<span class="number">1</span>] + w[<span class="number">1</span>]*torch.cos(w[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grad_g</span><span class="params">(w)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> torch.tensor([<span class="number">2</span>*w[<span class="number">1</span>] - w[<span class="number">1</span>]*torch.sin(w[<span class="number">0</span>]), <span class="number">2</span>*w[<span class="number">0</span>] + torch.cos(w[<span class="number">0</span>])])</span><br><span class="line"></span><br><span class="line">w = torch.tensor([np.pi, <span class="number">1</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">z = g(w)</span><br><span class="line">z.backward()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Analytical grad g(w)'</span>, grad_g(w))</span><br><span class="line">print(<span class="string">'PyTorch\'s grad g(w)'</span>, w.grad)</span><br></pre></td></tr></table></figure>

<p>输出如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Analytical grad g(w) tensor([<span class="number">2.0000</span>, <span class="number">5.2832</span>])</span><br><span class="line">PyTorch<span class="string">'s grad g(w) tensor([2.0000, 5.2832])</span></span><br></pre></td></tr></table></figure>

<h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>有了梯度，咱们就可以试试看梯度下降：</p>
<p>假设$f$就是上部分定义的函数</p>
<p>问题：找出使得$f$最小的$x$。</p>
<p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">5.0</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">step_size = <span class="number">0.25</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'iter,\tx,\tf(x),\tf\'(x),\tf\'(x) pytorch'</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">15</span>):</span><br><span class="line">    y = f(x)</span><br><span class="line">    y.backward() <span class="comment"># compute the gradient</span></span><br><span class="line">    </span><br><span class="line">    print(<span class="string">'&#123;&#125;,\t&#123;:.3f&#125;,\t&#123;:.3f&#125;,\t&#123;:.3f&#125;,\t&#123;:.3f&#125;'</span>.format(i, x.item(), f(x).item(), fp(x).item(), x.grad.item()))</span><br><span class="line">    </span><br><span class="line">    x.data = x.data - step_size * x.grad <span class="comment"># perform a GD update step</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># We need to zero the grad variable since the backward()</span></span><br><span class="line">    <span class="comment"># call accumulates the gradients in .grad instead of overwriting.</span></span><br><span class="line">    <span class="comment"># The detach_() is for efficiency. You do not need to worry too much about it.</span></span><br><span class="line">    x.grad.detach_()</span><br><span class="line">    x.grad.zero_()</span><br></pre></td></tr></table></figure>

<p>结果为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">iter,	x,	f(x),	f<span class="string">'(x),	f'</span>(x) pytorch</span><br><span class="line">0,	5.000,	9.000,	6.000,	6.000</span><br><span class="line">1,	3.500,	2.250,	3.000,	3.000</span><br><span class="line">2,	2.750,	0.562,	1.500,	1.500</span><br><span class="line">3,	2.375,	0.141,	0.750,	0.750</span><br><span class="line">4,	2.188,	0.035,	0.375,	0.375</span><br><span class="line">5,	2.094,	0.009,	0.188,	0.188</span><br><span class="line">6,	2.047,	0.002,	0.094,	0.094</span><br><span class="line">7,	2.023,	0.001,	0.047,	0.047</span><br><span class="line">8,	2.012,	0.000,	0.023,	0.023</span><br><span class="line">9,	2.006,	0.000,	0.012,	0.012</span><br><span class="line">10,	2.003,	0.000,	0.006,	0.006</span><br><span class="line">11,	2.001,	0.000,	0.003,	0.003</span><br><span class="line">12,	2.001,	0.000,	0.001,	0.001</span><br><span class="line">13,	2.000,	0.000,	0.001,	0.001</span><br><span class="line">14,	2.000,	0.000,	0.000,	0.000</span><br></pre></td></tr></table></figure>

<h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><p>刚刚咱们最小化了一个瞎掰出来的函数，现在让我们最小化一个loss函数，并放入一些瞎掰的data。</p>
<p>现在咱用梯度下降解决一下线性回归问题：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># make a simple linear dataset with some noise</span></span><br><span class="line"></span><br><span class="line">d = <span class="number">2</span></span><br><span class="line">n = <span class="number">50</span></span><br><span class="line">X = torch.randn(n,d)</span><br><span class="line">true_w = torch.tensor([[<span class="number">-1.0</span>], [<span class="number">2.0</span>]])</span><br><span class="line">y = X @ true_w + torch.randn(n,<span class="number">1</span>) * <span class="number">0.1</span> <span class="comment"># @是矩阵乘法，*是元素逐个相乘。</span></span><br><span class="line">print(<span class="string">'X shape'</span>, X.shape)</span><br><span class="line">print(<span class="string">'y shape'</span>, y.shape)</span><br><span class="line">print(<span class="string">'w shape'</span>, true_w.shape)</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X shape torch.Size([<span class="number">50</span>, <span class="number">2</span>])</span><br><span class="line">y shape torch.Size([<span class="number">50</span>, <span class="number">1</span>])</span><br><span class="line">w shape torch.Size([<span class="number">2</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<h2 id="验证梯度计算的正确性"><a href="#验证梯度计算的正确性" class="headerlink" title="验证梯度计算的正确性"></a>验证梯度计算的正确性</h2><p>本部分我们验证PyTorch梯度计算的正确性。</p>
<p>Loss的梯度公式如下：</p>
<p>$\nabla _w \frac{1}{n}||y-Xw||_2^2=-\frac{2}{n}X^T(y-Xw)$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># define a linear model with no bias</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X, w)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> X @ w</span><br><span class="line"></span><br><span class="line"><span class="comment"># the residual sum of squares loss function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rss</span><span class="params">(y, y_hat)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> torch.norm(y - y_hat)**<span class="number">2</span> / n <span class="comment">#torch.norm是范数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># analytical expression for the gradient</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grad_rss</span><span class="params">(X, y, w)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">-2</span>*X.t() @ (y - X @ w) / n</span><br><span class="line"></span><br><span class="line">w = torch.tensor([[<span class="number">1.</span>], [<span class="number">0</span>]], requires_grad=<span class="literal">True</span>)</span><br><span class="line">y_hat = model(X, w)</span><br><span class="line"></span><br><span class="line">loss = rss(y, y_hat)</span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Analytical gradient'</span>, grad_rss(X, y, w).detach().view(<span class="number">2</span>).numpy())</span><br><span class="line">print(<span class="string">'PyTorch\'s gradient'</span>, w.grad.view(<span class="number">2</span>).numpy())</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Analytical gradient [ <span class="number">3.974099</span> <span class="number">-4.494481</span>]</span><br><span class="line">PyTorch<span class="string">'s gradient [ 3.9740987 -4.4944816]</span></span><br></pre></td></tr></table></figure>

<p>从结果中，我们可以看出PyTorch计算出了正确的梯度，现在让我们把梯度用起来吧。</p>
<h2 id="使用梯度下降算法，基于自动求导功能的线性回归"><a href="#使用梯度下降算法，基于自动求导功能的线性回归" class="headerlink" title="使用梯度下降算法，基于自动求导功能的线性回归"></a>使用梯度下降算法，基于自动求导功能的线性回归</h2><p>现在咱们用得出的梯度来实现梯度下降算法。</p>
<p>Note:这个例子只是单纯展示如何用PyTorch将之前的想法实现出来，后面还会介绍如何用PyTorchic的方式来做同样的事情。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">step_size = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'iter,\tloss,\tw'</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">    y_hat = model(X, w)</span><br><span class="line">    loss = rss(y, y_hat)</span><br><span class="line">    </span><br><span class="line">    loss.backward() <span class="comment"># compute the gradient of the loss</span></span><br><span class="line">    </span><br><span class="line">    w.data = w.data - step_size * w.grad <span class="comment"># do a gradient descent step</span></span><br><span class="line">    </span><br><span class="line">    print(<span class="string">'&#123;&#125;,\t&#123;:.2f&#125;,\t&#123;&#125;'</span>.format(i, loss.item(), w.view(<span class="number">2</span>).detach().numpy()))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># We need to zero the grad variable since the backward()</span></span><br><span class="line">    <span class="comment"># call accumulates the gradients in .grad instead of overwriting.</span></span><br><span class="line">    <span class="comment"># The detach_() is for efficiency. You do not need to worry too much about it.</span></span><br><span class="line">    w.grad.detach()</span><br><span class="line">    w.grad.zero_()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'\ntrue w\t\t'</span>, true_w.view(<span class="number">2</span>).numpy())</span><br><span class="line">print(<span class="string">'estimated w\t'</span>, w.view(<span class="number">2</span>).detach().numpy())</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">iter,	loss,	w</span><br><span class="line"><span class="number">0</span>,	<span class="number">8.47</span>,	[<span class="number">0.20518023</span> <span class="number">0.89889634</span>]</span><br><span class="line"><span class="number">1</span>,	<span class="number">2.80</span>,	[<span class="number">-0.03049211</span>  <span class="number">1.1496693</span> ]</span><br><span class="line"><span class="number">2</span>,	<span class="number">1.75</span>,	[<span class="number">-0.21864393</span>  <span class="number">1.3446302</span> ]</span><br><span class="line"><span class="number">3</span>,	<span class="number">1.09</span>,	[<span class="number">-0.3690024</span>  <span class="number">1.4960643</span>]</span><br><span class="line"><span class="number">4</span>,	<span class="number">0.68</span>,	[<span class="number">-0.4892798</span>  <span class="number">1.6135726</span>]</span><br><span class="line"><span class="number">5</span>,	<span class="number">0.43</span>,	[<span class="number">-0.5855947</span>  <span class="number">1.7046558</span>]</span><br><span class="line"><span class="number">6</span>,	<span class="number">0.27</span>,	[<span class="number">-0.66280454</span>  <span class="number">1.7751708</span> ]</span><br><span class="line"><span class="number">7</span>,	<span class="number">0.18</span>,	[<span class="number">-0.7247683</span>  <span class="number">1.8296891</span>]</span><br><span class="line"><span class="number">8</span>,	<span class="number">0.11</span>,	[<span class="number">-0.7745538</span>  <span class="number">1.8717768</span>]</span><br><span class="line"><span class="number">9</span>,	<span class="number">0.08</span>,	[<span class="number">-0.8146021</span>  <span class="number">1.9042141</span>]</span><br><span class="line"><span class="number">10</span>,	<span class="number">0.05</span>,	[<span class="number">-0.84685683</span>  <span class="number">1.9291675</span> ]</span><br><span class="line"><span class="number">11</span>,	<span class="number">0.04</span>,	[<span class="number">-0.87286717</span>  <span class="number">1.9483235</span> ]</span><br><span class="line"><span class="number">12</span>,	<span class="number">0.03</span>,	[<span class="number">-0.89386874</span>  <span class="number">1.9629945</span> ]</span><br><span class="line"><span class="number">13</span>,	<span class="number">0.02</span>,	[<span class="number">-0.91084814</span>  <span class="number">1.9742005</span> ]</span><br><span class="line"><span class="number">14</span>,	<span class="number">0.02</span>,	[<span class="number">-0.9245938</span>  <span class="number">1.982734</span> ]</span><br><span class="line"><span class="number">15</span>,	<span class="number">0.02</span>,	[<span class="number">-0.93573654</span>  <span class="number">1.9892098</span> ]</span><br><span class="line"><span class="number">16</span>,	<span class="number">0.01</span>,	[<span class="number">-0.9447814</span>  <span class="number">1.9941043</span>]</span><br><span class="line"><span class="number">17</span>,	<span class="number">0.01</span>,	[<span class="number">-0.9521335</span>  <span class="number">1.9977864</span>]</span><br><span class="line"><span class="number">18</span>,	<span class="number">0.01</span>,	[<span class="number">-0.9581177</span>  <span class="number">2.0005412</span>]</span><br><span class="line"><span class="number">19</span>,	<span class="number">0.01</span>,	[<span class="number">-0.96299535</span>  <span class="number">2.002589</span>  ]</span><br><span class="line"></span><br><span class="line">true w		 [<span class="number">-1.</span>  <span class="number">2.</span>]</span><br><span class="line">estimated w	 [<span class="number">-0.96299535</span>  <span class="number">2.002589</span>  ]</span><br></pre></td></tr></table></figure>

<h2 id="torch-nn-Module"><a href="#torch-nn-Module" class="headerlink" title="torch.nn.Module"></a>torch.nn.Module</h2><p><code>Module</code>是PyTorch对tensor施加操作的一种方式，各模块被作为torch.nn.Module的子类实现。所有模块都可以被调用，并可以被组合起来形成更加复杂的功能。</p>
<p><code>torch.nn</code><a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">docs</a></p>
<p>Note：多数为module实现的功能也可以通过<code>torch.nn.functional</code>来访问，但需要用户自己维护权重tensor。</p>
<p><code>torch.nn.functional</code><a href="https://pytorch.org/docs/stable/nn.html#torch-nn-functional" target="_blank" rel="noopener">docs</a></p>
<h2 id="Linear-Module"><a href="#Linear-Module" class="headerlink" title="Linear Module"></a>Linear Module</h2><p>Linear Module是一个常用模块，它很方便地实现带有一个bias的线性变换模型。你只需要指定输入结点的个数与输出节点的个数，它将自动为你生成中间权重与bias，创建出相应的线性模型。</p>
<p>和我们手动初始化$w$不同的是，Linear Module自动为我们初始化权重。对于最小化一个非凸的loss函数来说，权重的初始化是非常重要的。如果训练出来的模型没有想象中的好，可以尝试手动初始化权重，使其与默认的权重不同。<code>torch.nn.init</code>中有一些常用的参数初始化方式。</p>
<p><code>torch.nn.init</code><a href="https://pytorch.org/docs/stable/nn.html#torch-nn-init" target="_blank" rel="noopener">docs</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">d_in = <span class="number">3</span></span><br><span class="line">d_out = <span class="number">4</span></span><br><span class="line">linear_module = nn.Linear(d_in, d_out)</span><br><span class="line"></span><br><span class="line">example_tensor = torch.tensor([[<span class="number">1.</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line"><span class="comment"># applys a linear transformation to the data</span></span><br><span class="line">transformed = linear_module(example_tensor)</span><br><span class="line">print(<span class="string">'example_tensor'</span>, example_tensor.shape)</span><br><span class="line">print(<span class="string">'transormed'</span>, transformed.shape)</span><br><span class="line">print()</span><br><span class="line">print(<span class="string">'We can see that the weights exist in the background\n'</span>)</span><br><span class="line">print(<span class="string">'W:'</span>, linear_module.weight)</span><br><span class="line">print(<span class="string">'b:'</span>, linear_module.bias)</span><br></pre></td></tr></table></figure>



<p>结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;</span><br><span class="line">print(<span class="string">'We can see that the weights exist in the background\n'</span>)</span><br><span class="line">print(<span class="string">'W:'</span>, linear_module.weight)</span><br><span class="line">print(<span class="string">'b:'</span>, linear_module.bias)</span><br><span class="line">example_tensor torch.Size([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">transormed torch.Size([<span class="number">2</span>, <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">We can see that the weights exist <span class="keyword">in</span> the background</span><br><span class="line"></span><br><span class="line">W: Parameter containing:</span><br><span class="line">tensor([[ <span class="number">0.2383</span>, <span class="number">-0.0450</span>,  <span class="number">0.2986</span>],</span><br><span class="line">        [<span class="number">-0.0828</span>, <span class="number">-0.0900</span>,  <span class="number">0.2475</span>],</span><br><span class="line">        [<span class="number">-0.4174</span>,  <span class="number">0.3788</span>,  <span class="number">0.5005</span>],</span><br><span class="line">        [<span class="number">-0.3601</span>, <span class="number">-0.4104</span>, <span class="number">-0.0584</span>]], requires_grad=<span class="literal">True</span>)</span><br><span class="line">b: Parameter containing:</span><br><span class="line">tensor([<span class="number">-0.0385</span>, <span class="number">-0.0826</span>,  <span class="number">0.0033</span>,  <span class="number">0.4773</span>], requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>可以看到，咱们只是指定了输入和输出的维度，Linear Module就替我们把weight和bias全创建好了。</p>
<h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><p>PyTorch里头预先实现了一大票的激活函数，包括但不限于ReLU、Tanh和Sigmoid。因为他们都是模块，所以使用时需要先实例化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">activation_fn = nn.ReLU() <span class="comment"># we instantiate an instance of the ReLU module</span></span><br><span class="line">example_tensor = torch.tensor([<span class="number">-1.0</span>, <span class="number">1.0</span>, <span class="number">0.0</span>])</span><br><span class="line">activated = activation_fn(example_tensor)</span><br><span class="line">print(<span class="string">'example_tensor'</span>, example_tensor)</span><br><span class="line">print(<span class="string">'activated'</span>, activated)</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">example_tensor tensor([<span class="number">-1.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>])</span><br><span class="line">activated tensor([<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>])</span><br></pre></td></tr></table></figure>

<h2 id="Sequential"><a href="#Sequential" class="headerlink" title="Sequential"></a>Sequential</h2><p>Sequential提供给咱们一个绝佳的解决方案，用于将多个简单模块组合在一起。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">d_in = <span class="number">3</span></span><br><span class="line">d_hidden = <span class="number">4</span></span><br><span class="line">d_out = <span class="number">1</span></span><br><span class="line">model = torch.nn.Sequential(</span><br><span class="line">                            nn.Linear(d_in, d_hidden),</span><br><span class="line">                            nn.Tanh(),</span><br><span class="line">                            nn.Linear(d_hidden, d_out),</span><br><span class="line">                            nn.Sigmoid()</span><br><span class="line">                           )</span><br><span class="line"></span><br><span class="line">example_tensor = torch.tensor([[<span class="number">1.</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line">transformed = model(example_tensor)</span><br><span class="line">print(<span class="string">'transformed'</span>, transformed.shape)</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;</span><br><span class="line">transformed torch.Size([<span class="number">2</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<p>小技巧：咱们可以使用<code>parameters()</code>方法来得到得到任何<code>nn.Module( )</code>的参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">params = model.parameters()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">    print(param)</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Parameter containing:</span><br><span class="line">tensor([[<span class="number">-0.5554</span>,  <span class="number">0.0456</span>, <span class="number">-0.3115</span>],</span><br><span class="line">        [ <span class="number">0.0697</span>, <span class="number">-0.1629</span>,  <span class="number">0.3342</span>],</span><br><span class="line">        [ <span class="number">0.1340</span>, <span class="number">-0.1353</span>,  <span class="number">0.1261</span>],</span><br><span class="line">        [ <span class="number">0.0624</span>,  <span class="number">0.3285</span>, <span class="number">-0.4536</span>]], requires_grad=<span class="literal">True</span>)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([ <span class="number">0.3684</span>, <span class="number">-0.0760</span>, <span class="number">-0.2277</span>, <span class="number">-0.0276</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([[ <span class="number">0.0345</span>, <span class="number">-0.0294</span>, <span class="number">-0.1481</span>,  <span class="number">0.4977</span>]], requires_grad=<span class="literal">True</span>)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([<span class="number">0.1952</span>], requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>PyTorch中为我们预先实现好了很多loss函数，比方说<code>MSELoss</code>和<code>CrossEntropyLoss</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mse_loss_fn = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">input = torch.tensor([[<span class="number">0.</span>, <span class="number">0</span>, <span class="number">0</span>]])</span><br><span class="line">target = torch.tensor([[<span class="number">1.</span>, <span class="number">0</span>, <span class="number">-1</span>]])</span><br><span class="line"></span><br><span class="line">loss = mse_loss_fn(input, target)</span><br><span class="line"></span><br><span class="line">print(loss)</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(<span class="number">0.6667</span>)</span><br></pre></td></tr></table></figure>

<h2 id="torch-optim"><a href="#torch-optim" class="headerlink" title="torch.optim"></a>torch.optim</h2><p>PyTorch实现了很多优化方法。在使用时，你最少也要指定模型参数和学习率。</p>
<p>优化器虽好，但它并不会自动帮你计算梯度。so，你自己要记得调用一下<code>backward()</code>奥对了，在运行这个之前还要记得调用一下<code>optim.zero_grad()&#39;初始化一下.grad里头的变量。这相当于对所有.grad里头的变量做</code>detach_()<code>和</code>zero_()`</p>
<p><code>torch.optim</code><a href="https://pytorch.org/docs/stable/optim.html" target="_blank" rel="noopener">docs</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create a simple model</span></span><br><span class="line">model = nn.Linear(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create a simple dataset</span></span><br><span class="line">X_simple = torch.tensor([[<span class="number">1.</span>]])</span><br><span class="line">y_simple = torch.tensor([[<span class="number">2.</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># create our optimizer</span></span><br><span class="line">optim = torch.optim.SGD(model.parameters(), lr=<span class="number">1e-2</span>)</span><br><span class="line">mse_loss_fn = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">y_hat = model(X_simple)</span><br><span class="line">print(<span class="string">'model params before:'</span>, model.weight)</span><br><span class="line">loss = mse_loss_fn(y_hat, y_simple)</span><br><span class="line">optim.zero_grad()</span><br><span class="line">loss.backward()</span><br><span class="line">optim.step()</span><br><span class="line">print(<span class="string">'model params after:'</span>, model.weight)</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model params before: Parameter containing:</span><br><span class="line">tensor([[<span class="number">0.7107</span>]], requires_grad=<span class="literal">True</span>)</span><br><span class="line">model params after: Parameter containing:</span><br><span class="line">tensor([[<span class="number">0.7547</span>]], requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>可以看到，参数向正确的方向变化。</p>
<h2 id="用梯度下降、自动求导与PyTorch模块实现线性回归"><a href="#用梯度下降、自动求导与PyTorch模块实现线性回归" class="headerlink" title="用梯度下降、自动求导与PyTorch模块实现线性回归"></a>用梯度下降、自动求导与PyTorch模块实现线性回归</h2><p>现在，咱们把前面学的一堆东西揉在一起，用PyTorchic的方式实现线性回归。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">step_size = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line">linear_module = nn.Linear(d, <span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">loss_func = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">optim = torch.optim.SGD(linear_module.parameters(), lr=step_size)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'iter,\tloss,\tw'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">    y_hat = linear_module(X)</span><br><span class="line">    loss = loss_func(y_hat, y)</span><br><span class="line">    optim.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optim.step()</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">'&#123;&#125;,\t&#123;:.2f&#125;,\t&#123;&#125;'</span>.format(i, loss.item(), linear_module.weight.view(<span class="number">2</span>).detach().numpy()))</span><br><span class="line"></span><br><span class="line">print(<span class="string">'\ntrue w\t\t'</span>, true_w.view(<span class="number">2</span>).numpy())</span><br><span class="line">print(<span class="string">'estimated w\t'</span>, linear_module.weight.view(<span class="number">2</span>).detach().numpy())</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">iter,	loss,	w</span><br><span class="line"><span class="number">0</span>,	<span class="number">3.38</span>,	[<span class="number">-0.06555872</span>  <span class="number">0.9564365</span> ]</span><br><span class="line"><span class="number">1</span>,	<span class="number">2.09</span>,	[<span class="number">-0.25302264</span>  <span class="number">1.1884048</span> ]</span><br><span class="line"><span class="number">2</span>,	<span class="number">1.30</span>,	[<span class="number">-0.40178707</span>  <span class="number">1.3695917</span> ]</span><br><span class="line"><span class="number">3</span>,	<span class="number">0.81</span>,	[<span class="number">-0.5199211</span>  <span class="number">1.5110494</span>]</span><br><span class="line"><span class="number">4</span>,	<span class="number">0.50</span>,	[<span class="number">-0.61379874</span>  <span class="number">1.6214342</span> ]</span><br><span class="line"><span class="number">5</span>,	<span class="number">0.32</span>,	[<span class="number">-0.68845654</span>  <span class="number">1.7075248</span> ]</span><br><span class="line"><span class="number">6</span>,	<span class="number">0.20</span>,	[<span class="number">-0.74787635</span>  <span class="number">1.774628</span>  ]</span><br><span class="line"><span class="number">7</span>,	<span class="number">0.13</span>,	[<span class="number">-0.79520756</span>  <span class="number">1.8268977</span> ]</span><br><span class="line"><span class="number">8</span>,	<span class="number">0.08</span>,	[<span class="number">-0.83294225</span>  <span class="number">1.8675839</span> ]</span><br><span class="line"><span class="number">9</span>,	<span class="number">0.06</span>,	[<span class="number">-0.8630534</span>  <span class="number">1.8992288</span>]</span><br><span class="line"><span class="number">10</span>,	<span class="number">0.04</span>,	[<span class="number">-0.8871039</span>  <span class="number">1.9238206</span>]</span><br><span class="line"><span class="number">11</span>,	<span class="number">0.03</span>,	[<span class="number">-0.9063326</span>  <span class="number">1.9429132</span>]</span><br><span class="line"><span class="number">12</span>,	<span class="number">0.02</span>,	[<span class="number">-0.921722</span>   <span class="number">1.9577209</span>]</span><br><span class="line"><span class="number">13</span>,	<span class="number">0.02</span>,	[<span class="number">-0.9340517</span>  <span class="number">1.9691923</span>]</span><br><span class="line"><span class="number">14</span>,	<span class="number">0.02</span>,	[<span class="number">-0.9439409</span>  <span class="number">1.9780676</span>]</span><br><span class="line"><span class="number">15</span>,	<span class="number">0.01</span>,	[<span class="number">-0.95188165</span>  <span class="number">1.9849249</span> ]</span><br><span class="line"><span class="number">16</span>,	<span class="number">0.01</span>,	[<span class="number">-0.9582653</span>  <span class="number">1.9902146</span>]</span><br><span class="line"><span class="number">17</span>,	<span class="number">0.01</span>,	[<span class="number">-0.9634034</span>  <span class="number">1.9942878</span>]</span><br><span class="line"><span class="number">18</span>,	<span class="number">0.01</span>,	[<span class="number">-0.9675441</span>  <span class="number">1.9974183</span>]</span><br><span class="line"><span class="number">19</span>,	<span class="number">0.01</span>,	[<span class="number">-0.9708851</span>  <span class="number">1.9998189</span>]</span><br><span class="line"></span><br><span class="line">true w		 [<span class="number">-1.</span>  <span class="number">2.</span>]</span><br><span class="line">estimated w	 [<span class="number">-0.9708851</span>  <span class="number">1.9998189</span>]</span><br></pre></td></tr></table></figure>

<h2 id="使用SGD完成线性回归"><a href="#使用SGD完成线性回归" class="headerlink" title="使用SGD完成线性回归"></a>使用SGD完成线性回归</h2><p>在上一个例子中，咱们计算在整个数据集上的平均梯度(Gradient Decent)。我们可以通过简单修改实现随机梯度下降(Stochastic Gradient Descent)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">step_size = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line">linear_module = nn.Linear(d, <span class="number">1</span>)</span><br><span class="line">loss_func = nn.MSELoss()</span><br><span class="line">optim = torch.optim.SGD(linear_module.parameters(), lr=step_size)</span><br><span class="line">print(<span class="string">'iter,\tloss,\tw'</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">200</span>):</span><br><span class="line">    rand_idx = np.random.choice(n) <span class="comment"># take a random point from the dataset</span></span><br><span class="line">    x = X[rand_idx] </span><br><span class="line">    y_hat = linear_module(x)</span><br><span class="line">    loss = loss_func(y_hat, y[rand_idx]) <span class="comment"># only compute the loss on the single point</span></span><br><span class="line">    optim.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optim.step()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'&#123;&#125;,\t&#123;:.2f&#125;,\t&#123;&#125;'</span>.format(i, loss.item(), linear_module.weight.view(<span class="number">2</span>).detach().numpy()))</span><br><span class="line"></span><br><span class="line">print(<span class="string">'\ntrue w\t\t'</span>, true_w.view(<span class="number">2</span>).numpy())</span><br><span class="line">print(<span class="string">'estimated w\t'</span>, linear_module.weight.view(<span class="number">2</span>).detach().numpy())</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">iter,	loss,	w</span><br><span class="line"><span class="number">0</span>,	<span class="number">17.04</span>,	[ <span class="number">0.5124521</span>  <span class="number">-0.06253883</span>]</span><br><span class="line"><span class="number">20</span>,	<span class="number">0.52</span>,	[<span class="number">0.05047676</span> <span class="number">0.4122801</span> ]</span><br><span class="line"><span class="number">40</span>,	<span class="number">0.10</span>,	[<span class="number">-0.29989475</span>  <span class="number">0.9859146</span> ]</span><br><span class="line"><span class="number">60</span>,	<span class="number">2.08</span>,	[<span class="number">-0.65285033</span>  <span class="number">1.3815426</span> ]</span><br><span class="line"><span class="number">80</span>,	<span class="number">0.07</span>,	[<span class="number">-0.787713</span>   <span class="number">1.4951732</span>]</span><br><span class="line"><span class="number">100</span>,	<span class="number">0.28</span>,	[<span class="number">-0.90760225</span>  <span class="number">1.683282</span>  ]</span><br><span class="line"><span class="number">120</span>,	<span class="number">0.01</span>,	[<span class="number">-0.92447585</span>  <span class="number">1.8040558</span> ]</span><br><span class="line"><span class="number">140</span>,	<span class="number">0.00</span>,	[<span class="number">-0.9492291</span>  <span class="number">1.8816731</span>]</span><br><span class="line"><span class="number">160</span>,	<span class="number">0.01</span>,	[<span class="number">-0.9679263</span>  <span class="number">1.9195584</span>]</span><br><span class="line"><span class="number">180</span>,	<span class="number">0.01</span>,	[<span class="number">-0.9705014</span>  <span class="number">1.9513198</span>]</span><br><span class="line"></span><br><span class="line">true w		 [<span class="number">-1.</span>  <span class="number">2.</span>]</span><br><span class="line">estimated w	 [<span class="number">-0.9696742</span>  <span class="number">1.9616756</span>]</span><br></pre></td></tr></table></figure>

<p>其中，咱们通过<code>np.random.choice(n)</code>随机选取一个数据用以更新而不是基于整个数据集更新。（这边相当于是针对data的stachastic）。</p>
<h2 id="CrossEntropyLoss"><a href="#CrossEntropyLoss" class="headerlink" title="CrossEntropyLoss"></a>CrossEntropyLoss</h2><p>现在，咱们用CrossEntropy作为loss函数来进行一个分类任务。</p>
<p>PyTorch在<a href="https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss" target="_blank" rel="noopener">CrossEntropyLoss</a>模块中预先实现了一个版本的cross entropy，它的用法和MSE稍微有点不同，所以这儿详细瞎逼逼一下它的参数：</p>
<ul>
<li><p>input:第一个参数就是咱们的分类神经网络的原始输出，它应该是一个维度为(N, C)的张量。其中，N是minibatch的大小，而C是class的数量。其中，第二维的数据是当前输入被分到各类别的没有normalize过的原始打分。CrossEntropyLoss模块自动为我们计算softmax，因此我们不需要自己算。</p>
</li>
<li><p>output:第二个参数是数据对应的label，用的时候应该输入一个长度为N的张量。每个维度上是正确的类别。</p>
</li>
</ul>
<p>下方代码展示三个预测在CrossEntropyLoss上的打分。正确的label为$y=[1, 1, 0]$。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">input = torch.tensor([[<span class="number">-1.</span>, <span class="number">1</span>],[<span class="number">-1</span>, <span class="number">1</span>],[<span class="number">1</span>, <span class="number">-1</span>]]) <span class="comment"># raw scores correspond to the correct class</span></span><br><span class="line"><span class="comment"># input = torch.tensor([[-3., 3],[-3, 3],[3, -3]]) # raw scores correspond to the correct class with higher confidence</span></span><br><span class="line"><span class="comment"># input = torch.tensor([[1., -1],[1, -1],[-1, 1]]) # raw scores correspond to the incorrect class</span></span><br><span class="line"><span class="comment"># input = torch.tensor([[3., -3],[3, -3],[-3, 3]]) # raw scores correspond to the incorrect class with incorrectly placed confidence</span></span><br><span class="line"></span><br><span class="line">target = torch.tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>])</span><br><span class="line">output = loss(input, target)</span><br><span class="line">print(output)</span><br></pre></td></tr></table></figure>



<p>输出:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;</span><br><span class="line">tensor(<span class="number">0.1269</span>)</span><br></pre></td></tr></table></figure>

<h2 id="动态改变学习速率"><a href="#动态改变学习速率" class="headerlink" title="动态改变学习速率"></a>动态改变学习速率</h2><p>通常，我们不希望在训练过程中全程采用相同的学习速率。PyTorch提供相应组件支持根据训练进度自动调整学习速率。通常的策略包括每个epoch对学习速率lr乘以一个比率（比如0.9），并在训练loss下降地没那么快时将学习速率减半。</p>
<p>查看<a href="https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate" target="_blank" rel="noopener">learning rate scheduler docs</a>获取更多信息。</p>
<h2 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h2><p>当数据是图片时，我们通常希望使用卷积操作来怼它。PyTorh在<code>torch.nn.Conv2d</code>模块中实现了卷积操作。用户需要传入一个$(N,C_{in},H_{in},W_{in})$。其中，$N$是batch大小，$C_{in}$是通道数，$H_{in}$和$W_{in}$分别是输入图片的高和宽。</p>
<p>通过自定义以下参数，我们可以修改卷积操作：</p>
<ul>
<li>卷积核大小</li>
<li>步长</li>
<li>填充</li>
</ul>
<p>这些参数对输出张量的维度有影响，所以应该小心。</p>
<p>在<a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d" target="_blank" rel="noopener"><code>torch.nn.Conv2d</code> docs</a>中查看更多信息。</p>
<p>栗子：</p>
<p>在图片上怼一个gaussian blur kernel：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># a gaussian blur kernel</span></span><br><span class="line">gaussian_kernel = torch.tensor([[<span class="number">1.</span>, <span class="number">2</span>, <span class="number">1</span>],[<span class="number">2</span>, <span class="number">4</span>, <span class="number">2</span>],[<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>]]) / <span class="number">16.0</span></span><br><span class="line"></span><br><span class="line">conv = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># manually set the conv weight</span></span><br><span class="line">conv.weight.data[:] = gaussian_kernel</span><br><span class="line"></span><br><span class="line">convolved = conv(image_torch)</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">'original image'</span>)</span><br><span class="line">plt.imshow(image_torch.view(<span class="number">28</span>,<span class="number">28</span>).detach().numpy())</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">'blurred image'</span>)</span><br><span class="line">plt.imshow(convolved.view(<span class="number">26</span>,<span class="number">26</span>).detach().numpy())</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>读取一张RGB图片，输入通道数为3，输出通道数为16，在经过一个激活函数处理后，代码部分展示的输出结果又可以作为一个<code>Conv2d</code>的输入。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">im_channels = <span class="number">3</span> <span class="comment"># if we are working with RGB images, there are 3 input channels, with black and white, 1</span></span><br><span class="line">out_channels = <span class="number">16</span> <span class="comment"># this is a hyperparameter we can tune</span></span><br><span class="line">kernel_size = <span class="number">3</span> <span class="comment"># this is another hyperparameter we can tune</span></span><br><span class="line">batch_size = <span class="number">4</span></span><br><span class="line">image_width = <span class="number">32</span></span><br><span class="line">image_height = <span class="number">32</span></span><br><span class="line"></span><br><span class="line">im = torch.randn(batch_size, im_channels, image_width, image_height)</span><br><span class="line"></span><br><span class="line">m = nn.Conv2d(im_channels, out_channels, kernel_size)</span><br><span class="line">convolved = m(im) <span class="comment"># it is a module so we can call it</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'im shape'</span>, im.shape)</span><br><span class="line">print(<span class="string">'convolved im shape'</span>, convolved.shape)</span><br></pre></td></tr></table></figure>

<h2 id="一些蛮有用的链接"><a href="#一些蛮有用的链接" class="headerlink" title="一些蛮有用的链接"></a>一些蛮有用的链接</h2><ul>
<li><p><a href="https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html" target="_blank" rel="noopener">60 minute PyTorch Tutorial</a></p>
</li>
<li><p><a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">PyTorch Docs</a></p>
</li>
<li><p><a href="https://courses.cs.washington.edu/courses/cse446/19wi/notes/auto-diff.pdf" target="_blank" rel="noopener">Lecture notes on Auto-Diff</a></p>
</li>
</ul>
<h2 id="数据类"><a href="#数据类" class="headerlink" title="数据类"></a>数据类</h2><p><code>torch.utils.data.Dataset</code>是一个抽象类，咱们如果要弄一个自己的数据集的话就需要继承<code>Dataset</code>，然后覆盖以下的方法：</p>
<ul>
<li><code>__len__</code>，确保<code>len(dataset)</code>可以正确返回数据集的大小。</li>
<li><code>__getitem__</code>用来实现索引，确保dataset[i]可以正确拿到第i个数据。</li>
</ul>
<p>咱们现在来搞一个face landmarks数据集。咱们在<code>__init__</code>里头读取csv文件，但把读取图片的任务留给<code>__getitem__</code>。这样做的好处是节省内存，不需要将所有数据一下子全读到内存里去。</p>
<p>咱们数据集的sample是一个dict:<code>{&#39;image&#39;: image, &#39;landmarks&#39;: landmarks}</code>。咱们数据集还要再弄一个<code>transform</code>方法，这样任何需要的操作都可以被施加在图片上。咱们在下一节里头看看它的用处。</p>
<p>栗子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FaceLandmarksDataset</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    <span class="string">"""Face Landmarks dataset."""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, csv_file, root_dir, transform=None)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            csv_file (string): Path to the csv file with annotations.</span></span><br><span class="line"><span class="string">            root_dir (string): Directory with all the images.</span></span><br><span class="line"><span class="string">            transform (callable, optional): Optional transform to be applied</span></span><br><span class="line"><span class="string">                on a sample.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.landmarks_frame = pd.read_csv(csv_file)</span><br><span class="line">        self.root_dir = root_dir</span><br><span class="line">        self.transform = transform</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.landmarks_frame)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, idx)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> torch.is_tensor(idx):</span><br><span class="line">            idx = idx.tolist()</span><br><span class="line"></span><br><span class="line">        img_name = os.path.join(self.root_dir,</span><br><span class="line">                                self.landmarks_frame.iloc[idx, <span class="number">0</span>])</span><br><span class="line">        image = io.imread(img_name)</span><br><span class="line">        landmarks = self.landmarks_frame.iloc[idx, <span class="number">1</span>:]</span><br><span class="line">        landmarks = np.array([landmarks])</span><br><span class="line">        landmarks = landmarks.astype(<span class="string">'float'</span>).reshape(<span class="number">-1</span>, <span class="number">2</span>)</span><br><span class="line">        sample = &#123;<span class="string">'image'</span>: image, <span class="string">'landmarks'</span>: landmarks&#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.transform:</span><br><span class="line">            sample = self.transform(sample)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> sample</span><br></pre></td></tr></table></figure>

<p>然鹅，如果咱们直接用<code>for</code>把整个数据走一遍会少掉很多东西，比如:</p>
<ul>
<li>按batch读取data</li>
<li>随机读取data</li>
<li>多线程读取data</li>
</ul>
<p>不过施主先不要着急，这些功能<code>torch.utils.data.DataLoader</code>都有。通过修改<code>collate_fn</code>参数，你可以指定数据按什么样的batch方式被读取，不过默认的读取方法已经可以cover大部分的需求了。</p>
<p>🌰：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">dataloader = DataLoader(transformed_dataset, batch_size=<span class="number">4</span>,</span><br><span class="line">                        shuffle=<span class="literal">True</span>, num_workers=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i_batch, sample_batched <span class="keyword">in</span> enumerate(dataloader):</span><br><span class="line">    print(i_batch, sample_batched[<span class="string">'image'</span>].size(),</span><br><span class="line">          sample_batched[<span class="string">'landmarks'</span>].size())</span><br></pre></td></tr></table></figure>

<h2 id="混合精度训练"><a href="#混合精度训练" class="headerlink" title="混合精度训练"></a>混合精度训练</h2><p>作者: <code>Chi-Liang Liu</code> 引用: <a href="https://github.com/NVIDIA/apex。采用混合精度训练你的模型，训练出来的神经网络可以：" target="_blank" rel="noopener">https://github.com/NVIDIA/apex。采用混合精度训练你的模型，训练出来的神经网络可以：</a></p>
<ul>
<li>运行速度快2-4倍</li>
<li>几行代码就可以省下一笔买新内存的钱</li>
</ul>
<h2 id="Apex"><a href="#Apex" class="headerlink" title="Apex"></a>Apex</h2><p>nvidia维护的工具简化了Pytorch中的混合精度和分布式培训。这里的一些代码最终将包含在上游Pytorch中。Apex的目的是尽快为用户提供最新的实用工具。</p>
<h2 id="apex-amp"><a href="#apex-amp" class="headerlink" title="apex.amp"></a>apex.amp</h2><p>Amp允许用户轻松地尝试不同的纯模式和混合精度模式。通过选择“优化级”或opt_level选择常用的默认模式;每个opt_level建立一组属性来管理Amp实现的纯精度或混合精度训练。通过将特定属性的值直接传递给amp.initialize，可以实现对给定opt_level行为方式的细粒度控制。这些手动指定的值覆盖opt_level建立的默认值。</p>
<p>🌰:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Declare model and optimizer as usual, with default (FP32) precision</span></span><br><span class="line">model = torch.nn.Linear(D_in, D_out).cuda()</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">1e-3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Allow Amp to perform casts as required by the opt_level</span></span><br><span class="line">model, optimizer = amp.initialize(model, optimizer, opt_level=<span class="string">"O1"</span>)</span><br><span class="line">...</span><br><span class="line"><span class="comment"># loss.backward() becomes:</span></span><br><span class="line"><span class="keyword">with</span> amp.scale_loss(loss, optimizer) <span class="keyword">as</span> scaled_loss:</span><br><span class="line">    scaled_loss.backward()</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">butyuhao</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://butyuhao.xyz/2020/03/21/PyTorch%E6%95%99%E5%AD%A6/">http://butyuhao.xyz/2020/03/21/PyTorch%E6%95%99%E5%AD%A6/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://butyuhao.xyz" target="_blank">butyuhao</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9D%8E%E5%AE%8F%E6%AF%85/">李宏毅    </a></div><div class="post_share"><div class="social-share" data-image="[&quot;/cover_img/tree-eval.png&quot;]" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"/><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-button button--primary button--animated"> <i class="fa fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/wechat.jpg" alt="微信"/><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/alipay.jpg" alt="支付寶"/><div class="post-qr-code__desc">支付寶</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="next-post pull-full"><a href="/2020/03/20/Pandas%E5%85%A5%E9%97%A8/"><img class="next_cover lazyload" data-src="/img/post.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Pandas入门</div></div></a></div></nav></article></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2020 By butyuhao</div><div class="framework-info"><span>驱动 </span><a href="https://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换" target="_self">簡</a><i class="darkmode fa fa-moon-o" id="darkmode" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>Powered by</span> <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener" style="color:#49B1F5;">hexo-generator-search</a></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script><script src="/js/search/local-search.js"></script></body></html>