<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>CMake基础</title>
    <url>/2020/04/13/CMake%E5%9F%BA%E7%A1%80/</url>
    <content><![CDATA[<h2 id="Hello-World"><a href="#Hello-World" class="headerlink" title="Hello World"></a>Hello World</h2><h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><h3 id="CMakeLists-txt"><a href="#CMakeLists-txt" class="headerlink" title="CMakeLists.txt"></a>CMakeLists.txt</h3><p>该文件保存所有CMake命令，挡在shell中运行cmake时，它将寻找该txt文件，如果找不到，cmake将报错并退出。</p>
<h3 id="最低CMake版本"><a href="#最低CMake版本" class="headerlink" title="最低CMake版本"></a>最低CMake版本</h3><p>用户可以指定可运行CMakeLists.txt文件的最低CMake版本，通过以下语句指定：</p>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line">cmake_minimun_required(VERSION <span class="number">3.5</span>)</span><br></pre></td></tr></table></figure>

<h3 id="项目名称"><a href="#项目名称" class="headerlink" title="项目名称"></a>项目名称</h3><p>通过指定项目名称，使得指定变量变得更加容易。</p>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">project</span>(hello_cmake)</span><br></pre></td></tr></table></figure>

<p>PS:当采用该命令指定项目名称后，自动添加一个变量${PROJECT_NAME}，其值就是指定的项目名称hello_cmake。</p>
<h3 id="创建可执行文件"><a href="#创建可执行文件" class="headerlink" title="创建可执行文件"></a>创建可执行文件</h3><p><code>add_executable()</code>明确了可执行文件应该从哪些源文件(source file)中被构建(build)。该命令含有两个参数，第一个参数是要构建的可执行文件的名称，第二个参数是将要被编译的源文件的列表。</p>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">add_executable</span>(hello_cmake main.cpp)</span><br></pre></td></tr></table></figure>

<p>将上面的三步合起来，就是：</p>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">cmake_minimum_required</span>(VERSION <span class="number">3.5</span>)</span><br><span class="line"><span class="keyword">project</span> (hello_cmake)</span><br><span class="line"><span class="keyword">add_executable</span>(<span class="variable">$&#123;PROJECT_NAME&#125;</span> main.cpp)</span><br></pre></td></tr></table></figure>

<h3 id="Binary-Directory"><a href="#Binary-Directory" class="headerlink" title="Binary Directory"></a>Binary Directory</h3><p>运行cmake指令的文件夹就是Binary Directory，该文件夹用于存放cmake build出的各种文件，该文件夹的路径保存在<code>CMAKE_BINARY_DIR</code>中。通常CMake支持两种build方式：</p>
<ul>
<li><h4 id="In-Place-Build"><a href="#In-Place-Build" class="headerlink" title="In-Place Build"></a>In-Place Build</h4></li>
</ul>
<p>该方式直接在存放代码的文件夹运行cmake，由于生成的文件会和代码文件混在一起，通常不采用这种方式。</p>
<ul>
<li><h4 id="Out-of-Source-Build"><a href="#Out-of-Source-Build" class="headerlink" title="Out-of-Source Build"></a>Out-of-Source Build</h4></li>
</ul>
<p>该方式在代码所在目录中新建一个叫做build的文件夹，然后在里头进行cmake的build工作，这样可以很方便地管理build过程中产生的各种文件。通常这么做：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mkdir build</span><br><span class="line"><span class="built_in">cd</span> build</span><br><span class="line">cmake ..</span><br></pre></td></tr></table></figure>

]]></content>
      <tags>
        <tag>CMake</tag>
      </tags>
  </entry>
  <entry>
    <title>利用反向传播训练多层神经网络的原理</title>
    <url>/2020/04/01/%E5%88%A9%E7%94%A8%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%AD%E7%BB%83%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8E%9F%E7%90%86/</url>
    <content><![CDATA[<p>文本详细描述了使用<em>后向传播</em>算法训练神经网络的过程。如下图所示，我们使用一个三层神经网络来描述这个过程，它有两个输入，一个输出：</p>
<p><img src="/" alt="img01" class="lazyload" data-src="/2020/04/01/%E5%88%A9%E7%94%A8%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%AD%E7%BB%83%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8E%9F%E7%90%86/img01.gif"></p>
<p>每个神经元由两部分组成，第一部分将输入信号与权重分别相乘然后求和。第二部分使用一个非线性函数（激活函数）对第一部分产生的信号进行非线性变换。信号$e$是第一部分（加法器）的输出信号，$y=f(e)$是第二部分（非线性变换）的输出信号。信号$y$也是整个神经元的输出。</p>
<p><img src="/" alt="img01b" class="lazyload" data-src="/2020/04/01/%E5%88%A9%E7%94%A8%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%AD%E7%BB%83%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8E%9F%E7%90%86/img01b.gif"></p>
<p>想要训练神经网络，我们首先需要数据集。每个训练数据包含输入信号($x_1$和$x_2$)以及对应的标签(期望的输出$z$)。神经网络的训练需要迭代进行。在每次迭代中，结点的权重系数($w_1$和$w_2$)都根据新的训练数据被更新。更新的数值大小使用以下算法确定：每个训练步骤都是从两个输入信号开始的。在这之后，可以根据输入信号确定每一层结点的输出信号值。下方图片展示了信号是如何在神经网络中传播，符号$w_{(xm)n}$表示在输入层中，输入$x_m$与神经元$n$之间的权重连接。符号$y_n$表示神经元$n$的输出信号。</p>
<p><img src="/" alt="img02" class="lazyload" data-src="/2020/04/01/%E5%88%A9%E7%94%A8%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%AD%E7%BB%83%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8E%9F%E7%90%86/img02.gif"></p>
<p><img src="/" alt="img03" class="lazyload" data-src="/2020/04/01/%E5%88%A9%E7%94%A8%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%AD%E7%BB%83%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8E%9F%E7%90%86/img03.gif"></p>
<p><img src="/" alt="img04" class="lazyload" data-src="/2020/04/01/%E5%88%A9%E7%94%A8%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%AD%E7%BB%83%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8E%9F%E7%90%86/img04.gif"></p>
<p>下图展示信号在隐藏层中传播。符号$w_{mn}$代表神经元$m$的输出与下一层中的神经元$n$的输入间的连接权重。</p>
<p><img src="/" alt="img05" class="lazyload" data-src="/2020/04/01/%E5%88%A9%E7%94%A8%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%AD%E7%BB%83%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8E%9F%E7%90%86/img05.gif"></p>
<p><img src="/" alt="img05" class="lazyload" data-src="/2020/04/01/%E5%88%A9%E7%94%A8%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%AD%E7%BB%83%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8E%9F%E7%90%86/img06.gif"></p>
<p>下图展示信号在输出层中传播。</p>
<p><img src="/" alt="img07" class="lazyload" data-src="/2020/04/01/%E5%88%A9%E7%94%A8%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%AD%E7%BB%83%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8E%9F%E7%90%86/img07.gif"></p>
<p>在算法的下一个步骤中，网络的输出信号$y$被拿来与期望的输出值$z$进行比较，$z$就是前文中所述的，由数据集提供的标签。差值被称作是输出层神经元的误差信号$\delta$。</p>
<p><img src="/" alt="img08" class="lazyload" data-src="/2020/04/01/%E5%88%A9%E7%94%A8%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%AD%E7%BB%83%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8E%9F%E7%90%86/img08.gif"></p>
<p>我们并不可能直接计算内层神经元的误差信号，因为这些神经元的输出值是未知的。多年来，人们都不知道如何有效训练多层神经网络。直到上世纪80年代中期，后向传播算法才被人们发现。该算法的思想是将误差信号$\delta$(单步训练中得出)往后传播回所有神经元，将输出信号作为他们的输入。</p>
<p><img src="/" alt="img09" class="lazyload" data-src="/2020/04/01/%E5%88%A9%E7%94%A8%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%AD%E7%BB%83%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8E%9F%E7%90%86/img09.gif"></p>
<p><img src="/" alt="img09" class="lazyload" data-src="/2020/04/01/%E5%88%A9%E7%94%A8%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%AD%E7%BB%83%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8E%9F%E7%90%86/img10.gif"></p>
<p>用来计算误差的权重系数$w_{mn}$和训练时得出的那个一致。这边仅仅是数据流被改变(信号被一个接一个地从输出方向传播到输入方向)。这个方法在所有网络层上应用。当误差来自多个神经元时，总误差是这些误差的和。示意图如下：</p>
<p><img src="/" alt="img11" class="lazyload" data-src="/2020/04/01/%E5%88%A9%E7%94%A8%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%AD%E7%BB%83%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8E%9F%E7%90%86/img11.gif"></p>
<p><img src="/" alt="img12" class="lazyload" data-src="/2020/04/01/%E5%88%A9%E7%94%A8%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%AD%E7%BB%83%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8E%9F%E7%90%86/img12.gif"></p>
<p><img src="/" alt="img11" class="lazyload" data-src="/2020/04/01/%E5%88%A9%E7%94%A8%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%AD%E7%BB%83%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8E%9F%E7%90%86/img13.gif"></p>
<p>当误差信号计算完毕时，我们使用计算出的误差信号更新网络权重。在下方公式中，${df_1(e)\over{de}}$代表激活函数的导数，$w’$代表$w$更新后的值。</p>
<p><img src="/" alt="img14" class="lazyload" data-src="/2020/04/01/%E5%88%A9%E7%94%A8%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%AD%E7%BB%83%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8E%9F%E7%90%86/img14.gif"></p>
<p><img src="/" alt="img15" class="lazyload" data-src="/2020/04/01/%E5%88%A9%E7%94%A8%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%AD%E7%BB%83%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8E%9F%E7%90%86/img15.gif"></p>
<p><img src="/" alt="img16" class="lazyload" data-src="/2020/04/01/%E5%88%A9%E7%94%A8%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%AD%E7%BB%83%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8E%9F%E7%90%86/img16.gif"></p>
<p><img src="/" alt="img17" class="lazyload" data-src="/2020/04/01/%E5%88%A9%E7%94%A8%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%AD%E7%BB%83%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8E%9F%E7%90%86/img17.gif"></p>
<p><img src="/" alt="img18" class="lazyload" data-src="/2020/04/01/%E5%88%A9%E7%94%A8%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%AD%E7%BB%83%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8E%9F%E7%90%86/img18.gif"></p>
<p><img src="/" alt="img19" class="lazyload" data-src="/2020/04/01/%E5%88%A9%E7%94%A8%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%AD%E7%BB%83%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8E%9F%E7%90%86/img19.gif"></p>
<p>系数$\eta$影响网络的学习速率。对于选择合适的学习速率，有几种技巧可以使用。第一种技巧是一开始使用一个较大的学习速率，然后渐渐递减。第二种方法更为复杂，一开始选用一个小的系数，随着训练的进行逐渐升大，然后在最后阶段再次减小。以低速率开始有利于确定权重的符号。</p>
<p>原文链接:<a href="http://galaxy.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html" target="_blank" rel="noopener">http://galaxy.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html</a></p>
]]></content>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习之反向传播算法 Part3</title>
    <url>/2020/03/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%20Part3/</url>
    <content><![CDATA[<p>在看反向传播的过程中，我们先intuitice地看一遍过程，然后再看具体的公式。</p>
<h2 id="Intuition"><a href="#Intuition" class="headerlink" title="Intuition"></a>Intuition</h2><p>后向传播就是衡量前层对后层的影响使得梯度下降时，能够根据影响来调整权重大小。</p>
<p><img src="/" alt="1" class="lazyload" data-src="/2020/03/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%20Part3/1.png"></p>
<p>首先让我们看一下损失函数，通过Part2中的损失函数，我们可以计算出每个输出结点的误差。现在，让我们只看一个结点的误差情况。如下图所示，通过将该结点的loss和上一层的每个结点到该结点的权重相乘，可以得出每个权重需要修正的方向及大小。loss大小代表误差大小，更新前的weight大小代表当前情况下上一层每个结点对输出层中的这个结点的影响大小，而正负则决定了权重更新的方向。</p>
<p>![2](/Users/butyuhao/OneDrive - smail.shnu.edu.cn/blog/source/_posts/深度学习之反向传播算法 Part3/2.png)</p>
<p>然而，上面只是考虑到一个输出结点的情况。因为我们的输出层上有十个结点，因此我们需要考虑他们各自的更新需求，然后将十个结点的更新需求求和取平均就是当前输入的这张图片在这次更新中最终需要更新的数值大小。</p>
<p><img src="/" alt="3" class="lazyload" data-src="/2020/03/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%20Part3/3.png"></p>
<p>数据集中并不是只有当前输入的一张图片，还应该计算每一张图片的更新需求，如下图所示，将数据集中每张图片对权重的更新需求取平均作为整个数据集在这一轮更新中对权重的更新大小。</p>
<p><img src="/" alt="4" class="lazyload" data-src="/2020/03/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%20Part3/4.png"></p>
<h2 id="具体公式"><a href="#具体公式" class="headerlink" title="具体公式"></a>具体公式</h2><p>假设每一层都只有一个结点，那么让我们来看第L层（图中的L层是输出层）和第L-1层的情况。右上角中，是Cost、激活前的数值和激活后的数值，$z()$是激活函数。左下角的图片表示他们的关系。此时我们关心，$w(L)$变动会在多大程度上影响$C_0$。</p>
<p><img src="/" alt="7" class="lazyload" data-src="/2020/03/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%20Part3/7.png"></p>
<p>其实$C_0$对$w(L)$的偏导数表示的就是$w(L)$的变动会在多大程度上影响$C_0$。如下图所示，我们可以根据链式求导法则，将其进行变换。</p>
<p><img src="/" alt="6" class="lazyload" data-src="/2020/03/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%20Part3/6.png"></p>
<p>稍微变换上述公式，我们就可以得到bias对$C_0$的影响：</p>
<p><img src="/" alt="8" class="lazyload" data-src="/2020/03/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%20Part3/8.png"></p>
<p>最后让我们来看一下梯度公式：</p>
<p><img src="/" alt="9" class="lazyload" data-src="/2020/03/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%20Part3/9.png"></p>
<p>视频地址：<a href="https://www.bilibili.com/video/BV16x411V7Qg?p=2" target="_blank" rel="noopener">https://www.bilibili.com/video/BV16x411V7Qg?p=2</a></p>
<p>事实上，该视频较为适合在微观层面上理解后向传播机制，如果想要有更加全面而整体的把握，非常推荐阅读这篇文章：<a href="http://galaxy.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html" target="_blank" rel="noopener">http://galaxy.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html</a>  </p>
<p>当然，我也提供了翻译版本：利用反向传播训练多层神经网络的原理。</p>
]]></content>
      <tags>
        <tag>深度学习</tag>
        <tag>3Blue1Brown</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习之梯度下降法 Part2</title>
    <url>/2020/03/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%20Part2/</url>
    <content><![CDATA[<p>在Part1中，我们定义了一个用于识别手写数字的神经网络，而现在，我们需要看一看如何训练其中的weight，使得网络可以最好地对这些图片进行分类。</p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>首先我们需要一个指标来衡量当前网络中的weight能够多准确地完成数字图片的分类。损失函数做的就是这样一件事情：</p>
<p>$Cost(w) = $权重多烂</p>
<p>$w$是咱们网络中的所有权重数值，将$w$输入损失函数，它输出一个数值告诉我们，当前的权重到底有多烂。那么损失函数是怎样被implement的呢？让我们来康康：</p>
<p><img src="/" alt="1" class="lazyload" data-src="/2020/03/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%20Part2/1.png"></p>
<p>如上图所示，我们可以通过计算每个结点输出的结果与其应该有的正确值的平方和的总和来得知当前权重的糟糕程度。</p>
<h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>然鹅，知道了糟糕程度，我们如何据此调整权重，让结果不那么糟糕呢？这时候就要用到梯度下降。</p>
<p><img src="/" alt="2" class="lazyload" data-src="/2020/03/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%20Part2/2.png"></p>
<p>之所以叫梯度下降，是因为这个方法用到了梯度。我们知道，某个函数的某一点的梯度方向事实上是这个函数在这一点的数值上升得最快的方向。利用这一性质，我们计算损失函数的梯度，然后往反方向走，就是使得损失函数下降最快的方向。而梯度下降，事实上就是不停地重复以下过程：</p>
<ul>
<li>计算损失函数在某一点的梯度</li>
<li>向梯度相反的方向更新权重</li>
</ul>
<p><img src="/" alt="3" class="lazyload" data-src="/2020/03/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%20Part2/3.png"><br>事实上，梯度的正负，告诉我们往哪个方向走可以使得我们获得更小的cost，而梯度的大小则告诉我们每一项对于梯度大小变化的影响程度大小。比如某一项的梯度大，那么更新它相比于更新别人更能使得计算出的损失值发生变化。</p>
<h2 id="额外资料"><a href="#额外资料" class="headerlink" title="额外资料"></a>额外资料</h2><p>在关心机器怎么学习之前，咱们先来看看我们自己应该怎么学，这个网页中对训练一个数字图片分类器进行了详细的介绍：<a href="http://neuralnetworksanddeeplearning.com/chap1.html。" target="_blank" rel="noopener">http://neuralnetworksanddeeplearning.com/chap1.html。</a></p>
<p>视频地址：<a href="https://www.bilibili.com/video/BV1Ux411j7ri" target="_blank" rel="noopener">https://www.bilibili.com/video/BV1Ux411j7ri</a></p>
]]></content>
      <tags>
        <tag>深度学习</tag>
        <tag>3Blue1Brown</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习之神经网络的结构 Part1</title>
    <url>/2020/03/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%93%E6%9E%84%20Part1/</url>
    <content><![CDATA[<h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p>假设现在我们要训练一个神经网络来识别手写数字图片。如下图所示，我们使用最为简单的多层感知机(MLP)训练，定义一个输入层，两个隐藏层和一个输出层。我们将28x28的图片“拉直”，因此输入层有784个结点。而输出层的是个结点代表是十个数字中某个数字的可能性。</p>
<p><img src="/" alt="Screen Shot 2020-03-31 at 12.07.48 PM" class="lazyload" data-src="/2020/03/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%93%E6%9E%84%20Part1/1.png"></p>
<h2 id="网络输入"><a href="#网络输入" class="headerlink" title="网络输入"></a>网络输入</h2><p><img src="/" alt="Screen Shot 2020-03-31 at 12.08.41 PM" class="lazyload" data-src="/2020/03/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%93%E6%9E%84%20Part1/2.png"></p>
<p>输入值是图片中某个像素的灰度，取值范围在[0,1]，0代表全黑，1代表全白，输入某个结点的数值叫做激活值（Activation）。</p>
<h2 id="为什么我们需要隐藏层？"><a href="#为什么我们需要隐藏层？" class="headerlink" title="为什么我们需要隐藏层？"></a>为什么我们需要隐藏层？</h2><p>那么为什么我们需要搞很多层而不是就只有一个隐藏层呢？</p>
<p><img src="/" alt="Screen Shot 2020-03-31 at 12.10.56 PM" class="lazyload" data-src="/2020/03/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%93%E6%9E%84%20Part1/3.png"></p>
<p><img src="/" alt="Screen Shot 2020-03-31 at 12.11.20 PM" class="lazyload" data-src="/2020/03/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%93%E6%9E%84%20Part1/4.png">  </p>
<p>先来看一看这些数字，他们各自由一些笔画组成。</p>
<p><img src="/" alt="Screen Shot 2020-03-31 at 12.11.37 PM" class="lazyload" data-src="/2020/03/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%93%E6%9E%84%20Part1/5.png"></p>
<p>对于隐藏层中的结点来说，也许每个结点负责一种笔划的判断。而我们放置多个隐藏层，就可以组合判断多种笔划，达到识别数字的目的。</p>
<h2 id="权重是用来干什么的？"><a href="#权重是用来干什么的？" class="headerlink" title="权重是用来干什么的？"></a>权重是用来干什么的？</h2><p><img src="/" alt="Screen Shot 2020-03-31 at 12.13.10 PM" class="lazyload" data-src="/2020/03/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%93%E6%9E%84%20Part1/6.png"></p>
<p>权重其实决定了某个结点的值受上一层中各结点的影响程度。在这个例子中，就是某数字由各笔划组成的程度。</p>
<h2 id="激活函数的作用"><a href="#激活函数的作用" class="headerlink" title="激活函数的作用"></a>激活函数的作用</h2><p><img src="/" alt="Screen Shot 2020-03-31 at 12.15.27 PM" class="lazyload" data-src="/2020/03/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%93%E6%9E%84%20Part1/7.png"></p>
<p>这是一个Sigmoid函数，它是一个最为Basic的激活函数，将范围为负无穷到正无穷的输入映射到范围为(0,1)的输出。</p>
<h2 id="放在一起观察"><a href="#放在一起观察" class="headerlink" title="放在一起观察"></a>放在一起观察</h2><p><img src="/" alt="Screen Shot 2020-03-31 at 12.16.44 PM" class="lazyload" data-src="/2020/03/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%93%E6%9E%84%20Part1/8.png"></p>
<p>放在一起看，权重决定某神经元关注上一层的哪些神经元，而bias决定了加权和需要多大，才可以激活此神经元。而激活函数事实上衡量了加权和到底有多正。<img src="/" alt="Screen Shot 2020-03-31 at 12.18.37 PM" class="lazyload" data-src="/2020/03/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%93%E6%9E%84%20Part1/9.png"></p>
<p>需要注意的是，某层上某个结点的值是其上一层结点值的加权和，因此对于第二层的每个结点来说，其都有对应的784个权重，而每个结点又都有一个bias，因此对于第二层的所有结点来说，总共有784x16个weight以及16个bias。</p>
<h2 id="常用的激活函数"><a href="#常用的激活函数" class="headerlink" title="常用的激活函数"></a>常用的激活函数</h2><p><img src="/" alt="Screen Shot 2020-03-31 at 12.22.08 PM" class="lazyload" data-src="/2020/03/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%93%E6%9E%84%20Part1/10.png"></p>
<p>激活函数的任务事实上是模拟人脑中的神经元，在输入达到某种数值时，才向后输出。实验表明，Sigmoid函数的激活效果并不好。与其相比，Relu的激活</p>
<p>原视频链接：<a href="https://www.bilibili.com/video/BV1bx411M7Zx" target="_blank" rel="noopener">https://www.bilibili.com/video/BV1bx411M7Zx</a></p>
]]></content>
      <tags>
        <tag>深度学习</tag>
        <tag>3Blue1Brown</tag>
      </tags>
  </entry>
  <entry>
    <title>argparser简明教程</title>
    <url>/2020/03/25/argparser%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B/</url>
    <content><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>argparser是在使用Python编程时首选的命令行参数解析器。该库是Python语言自带的库，它自动决定如何从<code>sys.argv</code>中解析出各种命令行参数。根据用户指定的参数，argparser自动生成帮助信息，并能在用户传入无效参数时提示用户。</p>
<h2 id="创建解析器"><a href="#创建解析器" class="headerlink" title="创建解析器"></a>创建解析器</h2><p>在进一步指定解析器的各种参数前，我们需要首先实例化一个解析器,通过description选项，可以告诉用户这个程序是干什么的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser(description=<span class="string">'Process some integers.'</span>)</span><br></pre></td></tr></table></figure>



<h2 id="添加必选参数"><a href="#添加必选参数" class="headerlink" title="添加必选参数"></a>添加必选参数</h2><p>在我们编程时，有时需要在命令行中向Python程序传入一些参数，且这些参数是必选的，没有这些参数，程序将无法正常运行。</p>
<p>假设现在我们需要写一个程序，使其可以读入命令行中用户指定的一个数字，然后将该数字的平方打印出来。编写程序prog.py：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">"square"</span>, help=<span class="string">"display a square of a given number"</span>,</span><br><span class="line">                    type=int)</span><br><span class="line">args = parser.parse_args()</span><br><span class="line">print(args.square**<span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<p>在上方的程序中，我们通过<code>add_argument</code>向解析器中添加了一个必选参数<code>&quot;square&quot;</code>，且该参数的帮助信息为<code>help=&quot;display a square of a given number&quot;</code>，该参数的类型为<code>int</code>（如果不指定，则读取到的参数将被认为是字符串类型）。通过<code>args = parser.parse_args()</code>对传入的参数进行解析，通过<code>args.square</code>读取传入的<code>square</code>数值。</p>
<p>现在，让咱们运行一下上面写的那个程序：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ python3 prog.py 4 <span class="comment">#传入正确参数</span></span><br><span class="line">16</span><br><span class="line">$ python3 prog.py four <span class="comment">#传入错误参数，解析器报错</span></span><br><span class="line">usage: prog.py [-h] square</span><br><span class="line">prog.py: error: argument square: invalid int value: <span class="string">'four'</span></span><br></pre></td></tr></table></figure>

<p>可以看到，通过命令行运行该程序，并指定<code>square</code>为4，程序打印出了正确的结果16。</p>
<h2 id="添加可选参数"><a href="#添加可选参数" class="headerlink" title="添加可选参数"></a>添加可选参数</h2><p>在上一节中，我们已经学会如何向解析器添加必选参数，然鹅，有时候我们的参数并不是必选的，或者，它只是某种开关，那咱们咋办呢？。此时，我们可以通过添加可选参数来解决问题。</p>
<p>我们编写测试文件parser_test.py:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">"--瞎逼逼"</span>, help=<span class="string">"输出信息更加详细"</span>)</span><br><span class="line">args = parser.parse_args()</span><br><span class="line"><span class="keyword">if</span> args.瞎逼逼:</span><br><span class="line">    print(<span class="string">"瞎逼逼模式已被打开"</span>)</span><br></pre></td></tr></table></figure>

<p>运行该文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ python parse_test.py --瞎逼逼 1 <span class="comment">#传入参数，正常</span></span><br><span class="line">瞎逼逼模式已被打开</span><br><span class="line">$ python parser_test --瞎逼逼 <span class="comment">#没传入参数，报错</span></span><br><span class="line">python: can<span class="string">'t open file '</span>parser_test<span class="string">': [Errno 2] No such file or directory</span></span><br></pre></td></tr></table></figure>

<h3 id="作为“开关”的可选参数"><a href="#作为“开关”的可选参数" class="headerlink" title="作为“开关”的可选参数"></a>作为“开关”的可选参数</h3><p>在上方的例子中，需要传入一个没有用的参数才能使其正常运行，但是咱们其实是想把<code>--瞎逼逼</code>作为一个开关，如果指定该选项，程序就以瞎逼逼模式运行，如果没指定就以正常模式运行，<code>瞎逼逼</code>这个参数其实是充当一个bool变量，不是<code>True</code>就是<code>False</code>。咱们可以把上面的程序改改，写成下面这样：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">"--瞎逼逼"</span>, help=<span class="string">"输出信息更加详细"</span>, action=<span class="string">"store_true"</span>)</span><br><span class="line">args = parser.parse_args()</span><br><span class="line"><span class="keyword">if</span> args.瞎逼逼:</span><br><span class="line">    print(<span class="string">"瞎逼逼模式已被打开"</span>)</span><br></pre></td></tr></table></figure>

<p>上方程序通过添加<code>action=&quot;store_true&quot;</code>将<code>瞎逼逼</code>这个参数指定成一个bool参数，运行该文件，我们得到一下结果：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ python parse_test.py --瞎逼逼</span><br><span class="line">瞎逼逼模式已被打开</span><br><span class="line">$ python parse_test.py --瞎逼逼 1</span><br><span class="line">usage: parse_test.py [-h] [--瞎逼逼]</span><br><span class="line">parse_test.py: error: unrecognized arguments: 1</span><br></pre></td></tr></table></figure>

<p>这样以来，<code>瞎逼逼</code>参数就变成了一个开关，而用户如果多传入无关参数就会报错。事实上，在使用<code>action=&quot;store_true&quot;</code>选项后，当用户传入<code>--瞎逼逼</code>选项时，<code>瞎逼逼</code>这个参数就为<code>True</code>，而如果没有传入这个选项，参数值为<code>False</code>。</p>
<h3 id="添加可选参数的缩写"><a href="#添加可选参数的缩写" class="headerlink" title="添加可选参数的缩写"></a>添加可选参数的缩写</h3><p>在上方🌰中，我们将<code>--瞎逼逼</code>作为可选参数，但有时候我们比较懒，不想要写“瞎逼逼”这三个字。此时，我们可以选择添加可选参数的缩写：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">"-瞎"</span>, <span class="string">"--瞎逼逼"</span>, help=<span class="string">"输出信息更加详细"</span>, action=<span class="string">"store_true"</span>)</span><br><span class="line">args = parser.parse_args()</span><br><span class="line"><span class="keyword">if</span> args.瞎逼逼:</span><br><span class="line">    print(<span class="string">"瞎逼逼模式已被打开"</span>)</span><br></pre></td></tr></table></figure>

<p>运行结果:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ python parse_test.py -瞎</span><br><span class="line">瞎逼逼模式已被打开</span><br></pre></td></tr></table></figure>

<p>从结果可以看出，通过添加”-瞎”，我们只写一个字就能打开瞎逼逼开关。</p>
<h2 id="结合可选参数与必选参数"><a href="#结合可选参数与必选参数" class="headerlink" title="结合可选参数与必选参数"></a>结合可选参数与必选参数</h2><p>现在咱们把“添加必选参数”部分的程序与“添加可选参数”部分的代码放到一起：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">"square"</span>, type=int,</span><br><span class="line">                    help=<span class="string">"display a square of a given number"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"-瞎"</span>, <span class="string">"--瞎逼逼"</span>, help=<span class="string">"输出信息更加详细"</span>, action=<span class="string">"store_true"</span>)</span><br><span class="line">args = parser.parse_args()</span><br><span class="line">answer = args.square**<span class="number">2</span></span><br><span class="line"><span class="keyword">if</span> args.瞎逼逼:</span><br><span class="line">    print(<span class="string">"the square of &#123;&#125; equals &#123;&#125;"</span>.format(args.square, answer))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(answer)</span><br></pre></td></tr></table></figure>

<p>运行结果:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ python parse_test.py</span><br><span class="line">usage: parse_test.py [-h] [-瞎] square</span><br><span class="line">parse_test.py: error: the following arguments are required: square</span><br><span class="line">$ python parse_test.py -瞎</span><br><span class="line">usage: parse_test.py [-h] [-瞎] square</span><br><span class="line">parse_test.py: error: the following arguments are required: square</span><br><span class="line">$ python parse_test.py -瞎 4</span><br><span class="line">the square of 4 equals 16</span><br></pre></td></tr></table></figure>

<h3 id="添加“副”开关"><a href="#添加“副”开关" class="headerlink" title="添加“副”开关"></a>添加“副”开关</h3><p>虽然咱们在上面添加了<code>--瞎逼逼</code>开关来决定程序输出信息的详细程度，但有时，我们添加的开关也需要参数。举个🌰，现在我们不仅需要指定程序是否要瞎逼逼，我们还要指定其瞎逼逼的程度，我们希望程序的瞎逼逼度为0、1或2，瞎逼逼程度随数字大小而增加。此时，我们可以这样写：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">"square"</span>, type=int,</span><br><span class="line">                    help=<span class="string">"display a square of a given number"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"-瞎"</span>, <span class="string">"--瞎逼逼"</span>, type=int, choices=[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">                    help=<span class="string">"increase output verbosity"</span>)</span><br><span class="line">args = parser.parse_args()</span><br><span class="line">answer = args.square**<span class="number">2</span></span><br><span class="line"><span class="keyword">if</span> args.瞎逼逼 == <span class="number">2</span>:</span><br><span class="line">    print(<span class="string">"the square of &#123;&#125; equals &#123;&#125;"</span>.format(args.square, answer))</span><br><span class="line"><span class="keyword">elif</span> args.瞎逼逼 == <span class="number">1</span>:</span><br><span class="line">    print(<span class="string">"&#123;&#125;^2 == &#123;&#125;"</span>.format(args.square, answer))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(answer)</span><br></pre></td></tr></table></figure>

<p>通过在添加参数时加入<code>choices=[0, 1, 2]</code>将瞎逼逼程度的传入参数限制在这三个数字以防用户传进来一些奇奇怪怪的东西，在下方通过if else来区分用户传入的不同瞎逼逼程度，并以相应程度输出结果。</p>
<p>结果:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ python parse_test.py 4 -瞎 3 <span class="comment">#不在choices中，因此报错。</span></span><br><span class="line">usage: parse_test.py [-h] [-瞎 &#123;0,1,2&#125;] square</span><br><span class="line">parse_test.py: error: argument -瞎/--瞎逼逼: invalid choice: 3 (choose from 0, 1, 2)</span><br><span class="line">$ python parse_test.py 4 -h</span><br><span class="line">usage: parse_test.py [-h] [-瞎 &#123;0,1,2&#125;] square</span><br><span class="line"></span><br><span class="line">positional arguments:</span><br><span class="line">  square                display a square of a given number</span><br><span class="line"></span><br><span class="line">optional arguments:</span><br><span class="line">  -h, --<span class="built_in">help</span>            show this <span class="built_in">help</span> message and <span class="built_in">exit</span></span><br><span class="line">  -瞎 &#123;0,1,2&#125;, --瞎逼逼 &#123;0,1,2&#125;</span><br><span class="line">                        increase output verbosity</span><br><span class="line">$ python parse_test.py 4 -瞎 1<span class="comment">#以一级瞎逼逼程度输出</span></span><br><span class="line">4^2 == 16</span><br><span class="line">(base) butyuhao@Yuhaos-MBP <span class="built_in">test</span> %</span><br></pre></td></tr></table></figure>

<p>试试上还有另一种指定瞎逼逼程度的方式，那就是用重复参数次数的方式：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">"square"</span>, type=int,</span><br><span class="line">                    help=<span class="string">"display a square of a given number"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"-瞎"</span>, <span class="string">"--瞎逼逼"</span>,action=<span class="string">"count"</span>,</span><br><span class="line">                    help=<span class="string">"increase output verbosity"</span>)</span><br><span class="line">args = parser.parse_args()</span><br><span class="line">answer = args.square**<span class="number">2</span></span><br><span class="line"><span class="keyword">if</span> args.瞎逼逼 &gt;= <span class="number">2</span>:</span><br><span class="line">    print(<span class="string">"the square of &#123;&#125; equals &#123;&#125;"</span>.format(args.square, answer))</span><br><span class="line"><span class="keyword">elif</span> args.瞎逼逼 == <span class="number">1</span>:</span><br><span class="line">    print(<span class="string">"&#123;&#125;^2 == &#123;&#125;"</span>.format(args.square, answer))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(answer)</span><br></pre></td></tr></table></figure>

<p>通过添加<code>action=&quot;count&quot;</code>选项，解析器将存储该参数在传入时的重复次数，这样一来就可以通过重复次数来表示瞎逼逼程度。</p>
<p>输出：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ python parse_test.py -瞎 4</span><br><span class="line">4^2 == 16</span><br><span class="line">$ python parse_test.py -瞎瞎 4</span><br><span class="line">the square of 4 equals 16</span><br><span class="line">$ python parse_test.py -瞎瞎瞎 4</span><br><span class="line">the square of 4 equals 16</span><br></pre></td></tr></table></figure>

<p>在结果中可以看到，随着参数重复次数的改变，输出也作相应改变。</p>
<p>不过，你真的觉得这样就行了么？你还是太naive了，如果这时候用户不指定这个参数就会报错：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ python parse_test.py 4</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">"parse_test.py"</span>, line 9, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    <span class="keyword">if</span> args.瞎逼逼 &gt;= 2:</span><br><span class="line">TypeError: <span class="string">'&gt;='</span> not supported between instances of <span class="string">'NoneType'</span> and <span class="string">'int'</span></span><br></pre></td></tr></table></figure>

<p>如果用户不传入这个参数，则该参数取值将为None，而&gt;=操作符不可以比较一个None和一个数字，因此报错。我们可以通过为该参数指定默认值解决该问题，这样在用户不指定该参数时，该参数为默认值，通过添加default参数指定：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">"square"</span>, type=int,</span><br><span class="line">                    help=<span class="string">"display a square of a given number"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"-瞎"</span>, <span class="string">"--瞎逼逼"</span>,action=<span class="string">"count"</span>,</span><br><span class="line">                    default=<span class="number">0</span>, help=<span class="string">"increase output verbosity"</span>)</span><br><span class="line">args = parser.parse_args()</span><br><span class="line">answer = args.square**<span class="number">2</span></span><br><span class="line"><span class="keyword">if</span> args.瞎逼逼 &gt;= <span class="number">2</span>:</span><br><span class="line">    print(<span class="string">"the square of &#123;&#125; equals &#123;&#125;"</span>.format(args.square, answer))</span><br><span class="line"><span class="keyword">elif</span> args.瞎逼逼 == <span class="number">1</span>:</span><br><span class="line">    print(<span class="string">"&#123;&#125;^2 == &#123;&#125;"</span>.format(args.square, answer))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(answer)</span><br></pre></td></tr></table></figure>

<h2 id="互斥的参数"><a href="#互斥的参数" class="headerlink" title="互斥的参数"></a>互斥的参数</h2><p>有时，咱们需要让用户知道，某些参数间只能选一个。举个🌰，此时我们不仅有<code>--瞎逼逼</code>选项，还有<code>--安静</code>选项。很明显，你并不能既让程序瞎逼逼，又让程序安静，这很明显会让你的程序精神分裂。这时候我们需要将它们设置为互斥，这样用户只能从中选一个指定。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"></span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">group = parser.add_mutually_exclusive_group()</span><br><span class="line">group.add_argument(<span class="string">"-瞎"</span>, <span class="string">"--瞎逼逼"</span>, action=<span class="string">"store_true"</span>)</span><br><span class="line">group.add_argument(<span class="string">"-安"</span>, <span class="string">"--安静"</span>, action=<span class="string">"store_true"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"x"</span>, type=int, help=<span class="string">"the base"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"y"</span>, type=int, help=<span class="string">"the exponent"</span>)</span><br><span class="line">args = parser.parse_args()</span><br><span class="line">answer = args.x**args.y</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> args.安静:</span><br><span class="line">    print(answer)</span><br><span class="line"><span class="keyword">elif</span> args.瞎逼逼:</span><br><span class="line">    print(<span class="string">"&#123;&#125; to the power &#123;&#125; equals &#123;&#125;"</span>.format(args.x, args.y, answer))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">"&#123;&#125;^&#123;&#125; == &#123;&#125;"</span>.format(args.x, args.y, answer))</span><br></pre></td></tr></table></figure>

<p>我们通过<code>group = parser.add_mutually_exclusive_group()</code>创建了一个互斥组，然后将两个参数添加进去，这样，用户就只能选择其一指定。</p>
<p>结果：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ python parse_test.py</span><br><span class="line">usage: parse_test.py [-h] [-v | -q] x y</span><br><span class="line">parse_test.py: error: the following arguments are required: x, y</span><br><span class="line">$ python parse_test.py 4 2</span><br><span class="line">4^2 == 16</span><br><span class="line">$ python parse_test.py 4 2 -安</span><br><span class="line">16</span><br><span class="line">$ python parse_test.py 4 2 -瞎</span><br><span class="line">4 to the power 2 equals 16</span><br></pre></td></tr></table></figure>

<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>事实上，argparser模块有更多复杂有趣的功能，可以在<a href="https://docs.python.org/3/library/argparse.html#module-argparse" target="_blank" rel="noopener"><code>官方文档</code></a>中详细了解。本文是在<a href="https://docs.python.org/3/howto/argparse.html#id1" target="_blank" rel="noopener"><code>官方教程</code></a>的基础上改编而成，如果觉得写的不好，也请不要在网络上逼逼赖赖，有本事现实碰一碰，你看我扎不扎你就完了。</p>
]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch教学</title>
    <url>/2020/03/21/PyTorch%E6%95%99%E5%AD%A6/</url>
    <content><![CDATA[<h2 id="PyTorch-介绍"><a href="#PyTorch-介绍" class="headerlink" title="PyTorch 介绍"></a>PyTorch 介绍</h2><p>PyTorch是一款强有力的深度学习框架，不管是科研还是生产部署的需求它都能满足。<br>本教程仅提供粗略的介绍，如果有任何疑问可以上网查查或是去咨询朋友。（小声bb：多google少百度）<br>一些用得上的point：</p>
<ul>
<li><p>自动differentiation工具非常强大</p>
</li>
<li><p>PyTorch为你实现好了深度学习中的常用功能</p>
</li>
<li><p>使用PyTorch DataSet来处理数据</p>
</li>
</ul>
<h2 id="Tensor与numpy的关系"><a href="#Tensor与numpy的关系" class="headerlink" title="Tensor与numpy的关系"></a>Tensor与numpy的关系</h2><p>PyTorch的基本组成部分，block与numpy的ndarray相似。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># we create tensors in a similar way to numpy nd arrays</span></span><br><span class="line">x_numpy = np.array([<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>])</span><br><span class="line">x_torch = torch.tensor([<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>])</span><br><span class="line">print(<span class="string">'x_numpy, x_torch'</span>)</span><br><span class="line">print(x_numpy, x_torch)</span><br><span class="line">print()</span><br><span class="line"></span><br><span class="line"><span class="comment"># to and from numpy, pytorch</span></span><br><span class="line">print(<span class="string">'to and from numpy and pytorch'</span>)</span><br><span class="line">print(torch.from_numpy(x_numpy), x_torch.numpy())</span><br><span class="line">print()</span><br><span class="line"></span><br><span class="line"><span class="comment"># we can do basic operations like +-*/</span></span><br><span class="line">y_numpy = np.array([<span class="number">3</span>,<span class="number">4</span>,<span class="number">5.</span>])</span><br><span class="line">y_torch = torch.tensor([<span class="number">3</span>,<span class="number">4</span>,<span class="number">5.</span>])</span><br><span class="line">print(<span class="string">"x+y"</span>)</span><br><span class="line">print(x_numpy + y_numpy, x_torch + y_torch)</span><br><span class="line">print()</span><br><span class="line"></span><br><span class="line"><span class="comment"># many functions that are in numpy are also in pytorch</span></span><br><span class="line">print(<span class="string">"norm"</span>)</span><br><span class="line">print(np.linalg.norm(x_numpy), torch.norm(x_torch))</span><br><span class="line">print()</span><br><span class="line"></span><br><span class="line"><span class="comment"># to apply an operation along a dimension,</span></span><br><span class="line"><span class="comment"># we use the dim keyword argument instead of axis</span></span><br><span class="line">print(<span class="string">"mean along the 0th dimension"</span>)</span><br><span class="line">x_numpy = np.array([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4.</span>]])</span><br><span class="line">x_torch = torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4.</span>]])</span><br><span class="line">print(np.mean(x_numpy, axis=<span class="number">0</span>), torch.mean(x_torch, dim=<span class="number">0</span>))</span><br></pre></td></tr></table></figure>
<h2 id="Tensor-view"><a href="#Tensor-view" class="headerlink" title="Tensor.view"></a>Tensor.view</h2><p>和numpy.reshape()一样，我们可以使用tensor.view()来改变tensor的形状。当使用-1时，其自动计算剩下维度的值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># "MNIST"</span></span><br><span class="line">N, C, W, H = <span class="number">10000</span>, <span class="number">3</span>, <span class="number">28</span>, <span class="number">28</span></span><br><span class="line">X = torch.randn((N, C, W, H))</span><br><span class="line"></span><br><span class="line">print(X.shape)</span><br><span class="line">print(X.view(N, C, <span class="number">784</span>).shape)</span><br><span class="line">print(X.view(<span class="number">-1</span>, C, <span class="number">784</span>).shape) <span class="comment"># automatically choose the 0th dimension</span></span><br></pre></td></tr></table></figure>
<h2 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h2><p>PyTorch的tensor有一个特别的地方，当tensor对象时，Pytorch自动在后台创建“计算图”。计算图是使用图的方式来表达一个数学表达式。其带有一种算法来计算一个计算图中所有变量的梯度，计算顺序和函数本身的顺序一致。<br>假设表达式为$e=(a+b)*(b+1)$,其中，$a=2, b=1$我们可以画出如下的计算图：</p>
<p><img src="/" alt class="lazyload" data-src="/2020/03/21/PyTorch%E6%95%99%E5%AD%A6/tree-eval.png"></p>
<p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.tensor(<span class="number">2.0</span>, requires_grad=<span class="literal">True</span>) <span class="comment"># we set requires_grad=True to let PyTorch know to keep the graph</span></span><br><span class="line">b = torch.tensor(<span class="number">1.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">c = a + b</span><br><span class="line">d = b + <span class="number">1</span></span><br><span class="line">e = c * d</span><br><span class="line">print(<span class="string">'c'</span>, c)</span><br><span class="line">print(<span class="string">'d'</span>, d)</span><br><span class="line">print(<span class="string">'e'</span>, e)</span><br></pre></td></tr></table></figure>

<p>执行结果为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt;</span><br><span class="line">c tensor(<span class="number">3.</span>, grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">d tensor(<span class="number">2.</span>, grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">e tensor(<span class="number">6.</span>, grad_fn=&lt;MulBackward0&gt;)</span><br></pre></td></tr></table></figure>

<p>可以看到，PyTorch为我们记录了计算图。</p>
<h2 id="使用Pytorch来计算梯度"><a href="#使用Pytorch来计算梯度" class="headerlink" title="使用Pytorch来计算梯度"></a>使用Pytorch来计算梯度</h2><p>我们已经发现PyTorch会自动为我们记录计算图，现在，让我们用它来求一求梯度。</p>
<p>考虑这样一个方程：$f(x)=(x-2)^2$。</p>
<p>问题：计算${df(x)}\over{dx}$，求$f’(1)$。</p>
<p>我们对leaf variable <code>y</code>调用<code>backward()</code>方法，一次性计算<code>y</code>的所有梯度。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (x<span class="number">-2</span>)**<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fp</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span>*(x<span class="number">-2</span>)</span><br><span class="line"></span><br><span class="line">x = torch.tensor([<span class="number">1.0</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">y = f(x)</span><br><span class="line">y.backward()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Analytical f\'(x):'</span>, fp(x))</span><br><span class="line">print(<span class="string">'PyTorch\'s f\'(x):'</span>, x.grad)</span><br></pre></td></tr></table></figure>

<p>输出为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt;</span><br><span class="line">Analytical <span class="string">f'(x): tensor([-2.], grad_fn=&lt;MulBackward0&gt;) #带有grad_fn</span></span><br><span class="line"><span class="string">PyTorch'</span>s <span class="string">f'(x): tensor([-2.])</span></span><br></pre></td></tr></table></figure>

<p><code>backward()</code>也可以计算带有数学函数的表达式的梯度：</p>
<p>现有$w=[w_1,w_2]^T$，</p>
<p>$g(w)=2w_1w_2+w_2cos(w_1)$</p>
<p>问题：计算&nabla;$_w g(w)$然后验证&nabla;$_w g([\pi,1]) = [2,\pi-1]^T$。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">g</span><span class="params">(w)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span>*w[<span class="number">0</span>]*w[<span class="number">1</span>] + w[<span class="number">1</span>]*torch.cos(w[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grad_g</span><span class="params">(w)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> torch.tensor([<span class="number">2</span>*w[<span class="number">1</span>] - w[<span class="number">1</span>]*torch.sin(w[<span class="number">0</span>]), <span class="number">2</span>*w[<span class="number">0</span>] + torch.cos(w[<span class="number">0</span>])])</span><br><span class="line"></span><br><span class="line">w = torch.tensor([np.pi, <span class="number">1</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">z = g(w)</span><br><span class="line">z.backward()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Analytical grad g(w)'</span>, grad_g(w))</span><br><span class="line">print(<span class="string">'PyTorch\'s grad g(w)'</span>, w.grad)</span><br></pre></td></tr></table></figure>

<p>输出如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Analytical grad g(w) tensor([<span class="number">2.0000</span>, <span class="number">5.2832</span>])</span><br><span class="line">PyTorch<span class="string">'s grad g(w) tensor([2.0000, 5.2832])</span></span><br></pre></td></tr></table></figure>

<h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>有了梯度，咱们就可以试试看梯度下降：</p>
<p>假设$f$就是上部分定义的函数</p>
<p>问题：找出使得$f$最小的$x$。</p>
<p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">5.0</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">step_size = <span class="number">0.25</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'iter,\tx,\tf(x),\tf\'(x),\tf\'(x) pytorch'</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">15</span>):</span><br><span class="line">    y = f(x)</span><br><span class="line">    y.backward() <span class="comment"># compute the gradient</span></span><br><span class="line">    </span><br><span class="line">    print(<span class="string">'&#123;&#125;,\t&#123;:.3f&#125;,\t&#123;:.3f&#125;,\t&#123;:.3f&#125;,\t&#123;:.3f&#125;'</span>.format(i, x.item(), f(x).item(), fp(x).item(), x.grad.item()))</span><br><span class="line">    </span><br><span class="line">    x.data = x.data - step_size * x.grad <span class="comment"># perform a GD update step</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># We need to zero the grad variable since the backward()</span></span><br><span class="line">    <span class="comment"># call accumulates the gradients in .grad instead of overwriting.</span></span><br><span class="line">    <span class="comment"># The detach_() is for efficiency. You do not need to worry too much about it.</span></span><br><span class="line">    x.grad.detach_()</span><br><span class="line">    x.grad.zero_()</span><br></pre></td></tr></table></figure>

<p>结果为：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">iter,	x,	f(x),	f<span class="string">'(x),	f'</span>(x) pytorch</span><br><span class="line">0,	5.000,	9.000,	6.000,	6.000</span><br><span class="line">1,	3.500,	2.250,	3.000,	3.000</span><br><span class="line">2,	2.750,	0.562,	1.500,	1.500</span><br><span class="line">3,	2.375,	0.141,	0.750,	0.750</span><br><span class="line">4,	2.188,	0.035,	0.375,	0.375</span><br><span class="line">5,	2.094,	0.009,	0.188,	0.188</span><br><span class="line">6,	2.047,	0.002,	0.094,	0.094</span><br><span class="line">7,	2.023,	0.001,	0.047,	0.047</span><br><span class="line">8,	2.012,	0.000,	0.023,	0.023</span><br><span class="line">9,	2.006,	0.000,	0.012,	0.012</span><br><span class="line">10,	2.003,	0.000,	0.006,	0.006</span><br><span class="line">11,	2.001,	0.000,	0.003,	0.003</span><br><span class="line">12,	2.001,	0.000,	0.001,	0.001</span><br><span class="line">13,	2.000,	0.000,	0.001,	0.001</span><br><span class="line">14,	2.000,	0.000,	0.000,	0.000</span><br></pre></td></tr></table></figure>

<h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><p>刚刚咱们最小化了一个瞎掰出来的函数，现在让我们最小化一个loss函数，并放入一些瞎掰的data。</p>
<p>现在咱用梯度下降解决一下线性回归问题：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># make a simple linear dataset with some noise</span></span><br><span class="line"></span><br><span class="line">d = <span class="number">2</span></span><br><span class="line">n = <span class="number">50</span></span><br><span class="line">X = torch.randn(n,d)</span><br><span class="line">true_w = torch.tensor([[<span class="number">-1.0</span>], [<span class="number">2.0</span>]])</span><br><span class="line">y = X @ true_w + torch.randn(n,<span class="number">1</span>) * <span class="number">0.1</span> <span class="comment"># @是矩阵乘法，*是元素逐个相乘。</span></span><br><span class="line">print(<span class="string">'X shape'</span>, X.shape)</span><br><span class="line">print(<span class="string">'y shape'</span>, y.shape)</span><br><span class="line">print(<span class="string">'w shape'</span>, true_w.shape)</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X shape torch.Size([<span class="number">50</span>, <span class="number">2</span>])</span><br><span class="line">y shape torch.Size([<span class="number">50</span>, <span class="number">1</span>])</span><br><span class="line">w shape torch.Size([<span class="number">2</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<h2 id="验证梯度计算的正确性"><a href="#验证梯度计算的正确性" class="headerlink" title="验证梯度计算的正确性"></a>验证梯度计算的正确性</h2><p>本部分我们验证PyTorch梯度计算的正确性。</p>
<p>Loss的梯度公式如下：</p>
<p>$\nabla _w \frac{1}{n}||y-Xw||_2^2=-\frac{2}{n}X^T(y-Xw)$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># define a linear model with no bias</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X, w)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> X @ w</span><br><span class="line"></span><br><span class="line"><span class="comment"># the residual sum of squares loss function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rss</span><span class="params">(y, y_hat)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> torch.norm(y - y_hat)**<span class="number">2</span> / n <span class="comment">#torch.norm是范数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># analytical expression for the gradient</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grad_rss</span><span class="params">(X, y, w)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">-2</span>*X.t() @ (y - X @ w) / n</span><br><span class="line"></span><br><span class="line">w = torch.tensor([[<span class="number">1.</span>], [<span class="number">0</span>]], requires_grad=<span class="literal">True</span>)</span><br><span class="line">y_hat = model(X, w)</span><br><span class="line"></span><br><span class="line">loss = rss(y, y_hat)</span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Analytical gradient'</span>, grad_rss(X, y, w).detach().view(<span class="number">2</span>).numpy())</span><br><span class="line">print(<span class="string">'PyTorch\'s gradient'</span>, w.grad.view(<span class="number">2</span>).numpy())</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Analytical gradient [ <span class="number">3.974099</span> <span class="number">-4.494481</span>]</span><br><span class="line">PyTorch<span class="string">'s gradient [ 3.9740987 -4.4944816]</span></span><br></pre></td></tr></table></figure>

<p>从结果中，我们可以看出PyTorch计算出了正确的梯度，现在让我们把梯度用起来吧。</p>
<h2 id="使用梯度下降算法，基于自动求导功能的线性回归"><a href="#使用梯度下降算法，基于自动求导功能的线性回归" class="headerlink" title="使用梯度下降算法，基于自动求导功能的线性回归"></a>使用梯度下降算法，基于自动求导功能的线性回归</h2><p>现在咱们用得出的梯度来实现梯度下降算法。</p>
<p>Note:这个例子只是单纯展示如何用PyTorch将之前的想法实现出来，后面还会介绍如何用PyTorchic的方式来做同样的事情。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">step_size = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'iter,\tloss,\tw'</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">    y_hat = model(X, w)</span><br><span class="line">    loss = rss(y, y_hat)</span><br><span class="line">    </span><br><span class="line">    loss.backward() <span class="comment"># compute the gradient of the loss</span></span><br><span class="line">    </span><br><span class="line">    w.data = w.data - step_size * w.grad <span class="comment"># do a gradient descent step</span></span><br><span class="line">    </span><br><span class="line">    print(<span class="string">'&#123;&#125;,\t&#123;:.2f&#125;,\t&#123;&#125;'</span>.format(i, loss.item(), w.view(<span class="number">2</span>).detach().numpy()))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># We need to zero the grad variable since the backward()</span></span><br><span class="line">    <span class="comment"># call accumulates the gradients in .grad instead of overwriting.</span></span><br><span class="line">    <span class="comment"># The detach_() is for efficiency. You do not need to worry too much about it.</span></span><br><span class="line">    w.grad.detach()</span><br><span class="line">    w.grad.zero_()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'\ntrue w\t\t'</span>, true_w.view(<span class="number">2</span>).numpy())</span><br><span class="line">print(<span class="string">'estimated w\t'</span>, w.view(<span class="number">2</span>).detach().numpy())</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">iter,	loss,	w</span><br><span class="line"><span class="number">0</span>,	<span class="number">8.47</span>,	[<span class="number">0.20518023</span> <span class="number">0.89889634</span>]</span><br><span class="line"><span class="number">1</span>,	<span class="number">2.80</span>,	[<span class="number">-0.03049211</span>  <span class="number">1.1496693</span> ]</span><br><span class="line"><span class="number">2</span>,	<span class="number">1.75</span>,	[<span class="number">-0.21864393</span>  <span class="number">1.3446302</span> ]</span><br><span class="line"><span class="number">3</span>,	<span class="number">1.09</span>,	[<span class="number">-0.3690024</span>  <span class="number">1.4960643</span>]</span><br><span class="line"><span class="number">4</span>,	<span class="number">0.68</span>,	[<span class="number">-0.4892798</span>  <span class="number">1.6135726</span>]</span><br><span class="line"><span class="number">5</span>,	<span class="number">0.43</span>,	[<span class="number">-0.5855947</span>  <span class="number">1.7046558</span>]</span><br><span class="line"><span class="number">6</span>,	<span class="number">0.27</span>,	[<span class="number">-0.66280454</span>  <span class="number">1.7751708</span> ]</span><br><span class="line"><span class="number">7</span>,	<span class="number">0.18</span>,	[<span class="number">-0.7247683</span>  <span class="number">1.8296891</span>]</span><br><span class="line"><span class="number">8</span>,	<span class="number">0.11</span>,	[<span class="number">-0.7745538</span>  <span class="number">1.8717768</span>]</span><br><span class="line"><span class="number">9</span>,	<span class="number">0.08</span>,	[<span class="number">-0.8146021</span>  <span class="number">1.9042141</span>]</span><br><span class="line"><span class="number">10</span>,	<span class="number">0.05</span>,	[<span class="number">-0.84685683</span>  <span class="number">1.9291675</span> ]</span><br><span class="line"><span class="number">11</span>,	<span class="number">0.04</span>,	[<span class="number">-0.87286717</span>  <span class="number">1.9483235</span> ]</span><br><span class="line"><span class="number">12</span>,	<span class="number">0.03</span>,	[<span class="number">-0.89386874</span>  <span class="number">1.9629945</span> ]</span><br><span class="line"><span class="number">13</span>,	<span class="number">0.02</span>,	[<span class="number">-0.91084814</span>  <span class="number">1.9742005</span> ]</span><br><span class="line"><span class="number">14</span>,	<span class="number">0.02</span>,	[<span class="number">-0.9245938</span>  <span class="number">1.982734</span> ]</span><br><span class="line"><span class="number">15</span>,	<span class="number">0.02</span>,	[<span class="number">-0.93573654</span>  <span class="number">1.9892098</span> ]</span><br><span class="line"><span class="number">16</span>,	<span class="number">0.01</span>,	[<span class="number">-0.9447814</span>  <span class="number">1.9941043</span>]</span><br><span class="line"><span class="number">17</span>,	<span class="number">0.01</span>,	[<span class="number">-0.9521335</span>  <span class="number">1.9977864</span>]</span><br><span class="line"><span class="number">18</span>,	<span class="number">0.01</span>,	[<span class="number">-0.9581177</span>  <span class="number">2.0005412</span>]</span><br><span class="line"><span class="number">19</span>,	<span class="number">0.01</span>,	[<span class="number">-0.96299535</span>  <span class="number">2.002589</span>  ]</span><br><span class="line"></span><br><span class="line">true w		 [<span class="number">-1.</span>  <span class="number">2.</span>]</span><br><span class="line">estimated w	 [<span class="number">-0.96299535</span>  <span class="number">2.002589</span>  ]</span><br></pre></td></tr></table></figure>

<h2 id="torch-nn-Module"><a href="#torch-nn-Module" class="headerlink" title="torch.nn.Module"></a>torch.nn.Module</h2><p><code>Module</code>是PyTorch对tensor施加操作的一种方式，各模块被作为torch.nn.Module的子类实现。所有模块都可以被调用，并可以被组合起来形成更加复杂的功能。</p>
<p><code>torch.nn</code><a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">docs</a></p>
<p>Note：多数为module实现的功能也可以通过<code>torch.nn.functional</code>来访问，但需要用户自己维护权重tensor。</p>
<p><code>torch.nn.functional</code><a href="https://pytorch.org/docs/stable/nn.html#torch-nn-functional" target="_blank" rel="noopener">docs</a></p>
<h2 id="Linear-Module"><a href="#Linear-Module" class="headerlink" title="Linear Module"></a>Linear Module</h2><p>Linear Module是一个常用模块，它很方便地实现带有一个bias的线性变换模型。你只需要指定输入结点的个数与输出节点的个数，它将自动为你生成中间权重与bias，创建出相应的线性模型。</p>
<p>和我们手动初始化$w$不同的是，Linear Module自动为我们初始化权重。对于最小化一个非凸的loss函数来说，权重的初始化是非常重要的。如果训练出来的模型没有想象中的好，可以尝试手动初始化权重，使其与默认的权重不同。<code>torch.nn.init</code>中有一些常用的参数初始化方式。</p>
<p><code>torch.nn.init</code><a href="https://pytorch.org/docs/stable/nn.html#torch-nn-init" target="_blank" rel="noopener">docs</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d_in = <span class="number">3</span></span><br><span class="line">d_out = <span class="number">4</span></span><br><span class="line">linear_module = nn.Linear(d_in, d_out)</span><br><span class="line"></span><br><span class="line">example_tensor = torch.tensor([[<span class="number">1.</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line"><span class="comment"># applys a linear transformation to the data</span></span><br><span class="line">transformed = linear_module(example_tensor)</span><br><span class="line">print(<span class="string">'example_tensor'</span>, example_tensor.shape)</span><br><span class="line">print(<span class="string">'transormed'</span>, transformed.shape)</span><br><span class="line">print()</span><br><span class="line">print(<span class="string">'We can see that the weights exist in the background\n'</span>)</span><br><span class="line">print(<span class="string">'W:'</span>, linear_module.weight)</span><br><span class="line">print(<span class="string">'b:'</span>, linear_module.bias)</span><br></pre></td></tr></table></figure>



<p>结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt;</span><br><span class="line">print(<span class="string">'We can see that the weights exist in the background\n'</span>)</span><br><span class="line">print(<span class="string">'W:'</span>, linear_module.weight)</span><br><span class="line">print(<span class="string">'b:'</span>, linear_module.bias)</span><br><span class="line">example_tensor torch.Size([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">transormed torch.Size([<span class="number">2</span>, <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">We can see that the weights exist <span class="keyword">in</span> the background</span><br><span class="line"></span><br><span class="line">W: Parameter containing:</span><br><span class="line">tensor([[ <span class="number">0.2383</span>, <span class="number">-0.0450</span>,  <span class="number">0.2986</span>],</span><br><span class="line">        [<span class="number">-0.0828</span>, <span class="number">-0.0900</span>,  <span class="number">0.2475</span>],</span><br><span class="line">        [<span class="number">-0.4174</span>,  <span class="number">0.3788</span>,  <span class="number">0.5005</span>],</span><br><span class="line">        [<span class="number">-0.3601</span>, <span class="number">-0.4104</span>, <span class="number">-0.0584</span>]], requires_grad=<span class="literal">True</span>)</span><br><span class="line">b: Parameter containing:</span><br><span class="line">tensor([<span class="number">-0.0385</span>, <span class="number">-0.0826</span>,  <span class="number">0.0033</span>,  <span class="number">0.4773</span>], requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>可以看到，咱们只是指定了输入和输出的维度，Linear Module就替我们把weight和bias全创建好了。</p>
<h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><p>PyTorch里头预先实现了一大票的激活函数，包括但不限于ReLU、Tanh和Sigmoid。因为他们都是模块，所以使用时需要先实例化。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">activation_fn = nn.ReLU() <span class="comment"># we instantiate an instance of the ReLU module</span></span><br><span class="line">example_tensor = torch.tensor([<span class="number">-1.0</span>, <span class="number">1.0</span>, <span class="number">0.0</span>])</span><br><span class="line">activated = activation_fn(example_tensor)</span><br><span class="line">print(<span class="string">'example_tensor'</span>, example_tensor)</span><br><span class="line">print(<span class="string">'activated'</span>, activated)</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">example_tensor tensor([<span class="number">-1.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>])</span><br><span class="line">activated tensor([<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>])</span><br></pre></td></tr></table></figure>

<h2 id="Sequential"><a href="#Sequential" class="headerlink" title="Sequential"></a>Sequential</h2><p>Sequential提供给咱们一个绝佳的解决方案，用于将多个简单模块组合在一起。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d_in = <span class="number">3</span></span><br><span class="line">d_hidden = <span class="number">4</span></span><br><span class="line">d_out = <span class="number">1</span></span><br><span class="line">model = torch.nn.Sequential(</span><br><span class="line">                            nn.Linear(d_in, d_hidden),</span><br><span class="line">                            nn.Tanh(),</span><br><span class="line">                            nn.Linear(d_hidden, d_out),</span><br><span class="line">                            nn.Sigmoid()</span><br><span class="line">                           )</span><br><span class="line"></span><br><span class="line">example_tensor = torch.tensor([[<span class="number">1.</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line">transformed = model(example_tensor)</span><br><span class="line">print(<span class="string">'transformed'</span>, transformed.shape)</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt;</span><br><span class="line">transformed torch.Size([<span class="number">2</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<p>小技巧：咱们可以使用<code>parameters()</code>方法来得到得到任何<code>nn.Module( )</code>的参数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">params = model.parameters()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">    print(param)</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Parameter containing:</span><br><span class="line">tensor([[<span class="number">-0.5554</span>,  <span class="number">0.0456</span>, <span class="number">-0.3115</span>],</span><br><span class="line">        [ <span class="number">0.0697</span>, <span class="number">-0.1629</span>,  <span class="number">0.3342</span>],</span><br><span class="line">        [ <span class="number">0.1340</span>, <span class="number">-0.1353</span>,  <span class="number">0.1261</span>],</span><br><span class="line">        [ <span class="number">0.0624</span>,  <span class="number">0.3285</span>, <span class="number">-0.4536</span>]], requires_grad=<span class="literal">True</span>)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([ <span class="number">0.3684</span>, <span class="number">-0.0760</span>, <span class="number">-0.2277</span>, <span class="number">-0.0276</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([[ <span class="number">0.0345</span>, <span class="number">-0.0294</span>, <span class="number">-0.1481</span>,  <span class="number">0.4977</span>]], requires_grad=<span class="literal">True</span>)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([<span class="number">0.1952</span>], requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>PyTorch中为我们预先实现好了很多loss函数，比方说<code>MSELoss</code>和<code>CrossEntropyLoss</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mse_loss_fn = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">input = torch.tensor([[<span class="number">0.</span>, <span class="number">0</span>, <span class="number">0</span>]])</span><br><span class="line">target = torch.tensor([[<span class="number">1.</span>, <span class="number">0</span>, <span class="number">-1</span>]])</span><br><span class="line"></span><br><span class="line">loss = mse_loss_fn(input, target)</span><br><span class="line"></span><br><span class="line">print(loss)</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor(<span class="number">0.6667</span>)</span><br></pre></td></tr></table></figure>

<h2 id="torch-optim"><a href="#torch-optim" class="headerlink" title="torch.optim"></a>torch.optim</h2><p>PyTorch实现了很多优化方法。在使用时，你最少也要指定模型参数和学习率。</p>
<p>优化器虽好，但它并不会自动帮你计算梯度。so，你自己要记得调用一下<code>backward()</code>奥对了，在运行这个之前还要记得调用一下<code>optim.zero_grad()&#39;初始化一下.grad里头的变量。这相当于对所有.grad里头的变量做</code>detach_()<code>和</code>zero_()`</p>
<p><code>torch.optim</code><a href="https://pytorch.org/docs/stable/optim.html" target="_blank" rel="noopener">docs</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># create a simple model</span></span><br><span class="line">model = nn.Linear(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create a simple dataset</span></span><br><span class="line">X_simple = torch.tensor([[<span class="number">1.</span>]])</span><br><span class="line">y_simple = torch.tensor([[<span class="number">2.</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># create our optimizer</span></span><br><span class="line">optim = torch.optim.SGD(model.parameters(), lr=<span class="number">1e-2</span>)</span><br><span class="line">mse_loss_fn = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">y_hat = model(X_simple)</span><br><span class="line">print(<span class="string">'model params before:'</span>, model.weight)</span><br><span class="line">loss = mse_loss_fn(y_hat, y_simple)</span><br><span class="line">optim.zero_grad()</span><br><span class="line">loss.backward()</span><br><span class="line">optim.step()</span><br><span class="line">print(<span class="string">'model params after:'</span>, model.weight)</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model params before: Parameter containing:</span><br><span class="line">tensor([[<span class="number">0.7107</span>]], requires_grad=<span class="literal">True</span>)</span><br><span class="line">model params after: Parameter containing:</span><br><span class="line">tensor([[<span class="number">0.7547</span>]], requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>可以看到，参数向正确的方向变化。</p>
<h2 id="用梯度下降、自动求导与PyTorch模块实现线性回归"><a href="#用梯度下降、自动求导与PyTorch模块实现线性回归" class="headerlink" title="用梯度下降、自动求导与PyTorch模块实现线性回归"></a>用梯度下降、自动求导与PyTorch模块实现线性回归</h2><p>现在，咱们把前面学的一堆东西揉在一起，用PyTorchic的方式实现线性回归。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">step_size = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line">linear_module = nn.Linear(d, <span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">loss_func = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">optim = torch.optim.SGD(linear_module.parameters(), lr=step_size)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'iter,\tloss,\tw'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">    y_hat = linear_module(X)</span><br><span class="line">    loss = loss_func(y_hat, y)</span><br><span class="line">    optim.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optim.step()</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">'&#123;&#125;,\t&#123;:.2f&#125;,\t&#123;&#125;'</span>.format(i, loss.item(), linear_module.weight.view(<span class="number">2</span>).detach().numpy()))</span><br><span class="line"></span><br><span class="line">print(<span class="string">'\ntrue w\t\t'</span>, true_w.view(<span class="number">2</span>).numpy())</span><br><span class="line">print(<span class="string">'estimated w\t'</span>, linear_module.weight.view(<span class="number">2</span>).detach().numpy())</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">iter,	loss,	w</span><br><span class="line"><span class="number">0</span>,	<span class="number">3.38</span>,	[<span class="number">-0.06555872</span>  <span class="number">0.9564365</span> ]</span><br><span class="line"><span class="number">1</span>,	<span class="number">2.09</span>,	[<span class="number">-0.25302264</span>  <span class="number">1.1884048</span> ]</span><br><span class="line"><span class="number">2</span>,	<span class="number">1.30</span>,	[<span class="number">-0.40178707</span>  <span class="number">1.3695917</span> ]</span><br><span class="line"><span class="number">3</span>,	<span class="number">0.81</span>,	[<span class="number">-0.5199211</span>  <span class="number">1.5110494</span>]</span><br><span class="line"><span class="number">4</span>,	<span class="number">0.50</span>,	[<span class="number">-0.61379874</span>  <span class="number">1.6214342</span> ]</span><br><span class="line"><span class="number">5</span>,	<span class="number">0.32</span>,	[<span class="number">-0.68845654</span>  <span class="number">1.7075248</span> ]</span><br><span class="line"><span class="number">6</span>,	<span class="number">0.20</span>,	[<span class="number">-0.74787635</span>  <span class="number">1.774628</span>  ]</span><br><span class="line"><span class="number">7</span>,	<span class="number">0.13</span>,	[<span class="number">-0.79520756</span>  <span class="number">1.8268977</span> ]</span><br><span class="line"><span class="number">8</span>,	<span class="number">0.08</span>,	[<span class="number">-0.83294225</span>  <span class="number">1.8675839</span> ]</span><br><span class="line"><span class="number">9</span>,	<span class="number">0.06</span>,	[<span class="number">-0.8630534</span>  <span class="number">1.8992288</span>]</span><br><span class="line"><span class="number">10</span>,	<span class="number">0.04</span>,	[<span class="number">-0.8871039</span>  <span class="number">1.9238206</span>]</span><br><span class="line"><span class="number">11</span>,	<span class="number">0.03</span>,	[<span class="number">-0.9063326</span>  <span class="number">1.9429132</span>]</span><br><span class="line"><span class="number">12</span>,	<span class="number">0.02</span>,	[<span class="number">-0.921722</span>   <span class="number">1.9577209</span>]</span><br><span class="line"><span class="number">13</span>,	<span class="number">0.02</span>,	[<span class="number">-0.9340517</span>  <span class="number">1.9691923</span>]</span><br><span class="line"><span class="number">14</span>,	<span class="number">0.02</span>,	[<span class="number">-0.9439409</span>  <span class="number">1.9780676</span>]</span><br><span class="line"><span class="number">15</span>,	<span class="number">0.01</span>,	[<span class="number">-0.95188165</span>  <span class="number">1.9849249</span> ]</span><br><span class="line"><span class="number">16</span>,	<span class="number">0.01</span>,	[<span class="number">-0.9582653</span>  <span class="number">1.9902146</span>]</span><br><span class="line"><span class="number">17</span>,	<span class="number">0.01</span>,	[<span class="number">-0.9634034</span>  <span class="number">1.9942878</span>]</span><br><span class="line"><span class="number">18</span>,	<span class="number">0.01</span>,	[<span class="number">-0.9675441</span>  <span class="number">1.9974183</span>]</span><br><span class="line"><span class="number">19</span>,	<span class="number">0.01</span>,	[<span class="number">-0.9708851</span>  <span class="number">1.9998189</span>]</span><br><span class="line"></span><br><span class="line">true w		 [<span class="number">-1.</span>  <span class="number">2.</span>]</span><br><span class="line">estimated w	 [<span class="number">-0.9708851</span>  <span class="number">1.9998189</span>]</span><br></pre></td></tr></table></figure>

<h2 id="使用SGD完成线性回归"><a href="#使用SGD完成线性回归" class="headerlink" title="使用SGD完成线性回归"></a>使用SGD完成线性回归</h2><p>在上一个例子中，咱们计算在整个数据集上的平均梯度(Gradient Decent)。我们可以通过简单修改实现随机梯度下降(Stochastic Gradient Descent)。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">step_size = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line">linear_module = nn.Linear(d, <span class="number">1</span>)</span><br><span class="line">loss_func = nn.MSELoss()</span><br><span class="line">optim = torch.optim.SGD(linear_module.parameters(), lr=step_size)</span><br><span class="line">print(<span class="string">'iter,\tloss,\tw'</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">200</span>):</span><br><span class="line">    rand_idx = np.random.choice(n) <span class="comment"># take a random point from the dataset</span></span><br><span class="line">    x = X[rand_idx] </span><br><span class="line">    y_hat = linear_module(x)</span><br><span class="line">    loss = loss_func(y_hat, y[rand_idx]) <span class="comment"># only compute the loss on the single point</span></span><br><span class="line">    optim.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optim.step()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'&#123;&#125;,\t&#123;:.2f&#125;,\t&#123;&#125;'</span>.format(i, loss.item(), linear_module.weight.view(<span class="number">2</span>).detach().numpy()))</span><br><span class="line"></span><br><span class="line">print(<span class="string">'\ntrue w\t\t'</span>, true_w.view(<span class="number">2</span>).numpy())</span><br><span class="line">print(<span class="string">'estimated w\t'</span>, linear_module.weight.view(<span class="number">2</span>).detach().numpy())</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">iter,	loss,	w</span><br><span class="line"><span class="number">0</span>,	<span class="number">17.04</span>,	[ <span class="number">0.5124521</span>  <span class="number">-0.06253883</span>]</span><br><span class="line"><span class="number">20</span>,	<span class="number">0.52</span>,	[<span class="number">0.05047676</span> <span class="number">0.4122801</span> ]</span><br><span class="line"><span class="number">40</span>,	<span class="number">0.10</span>,	[<span class="number">-0.29989475</span>  <span class="number">0.9859146</span> ]</span><br><span class="line"><span class="number">60</span>,	<span class="number">2.08</span>,	[<span class="number">-0.65285033</span>  <span class="number">1.3815426</span> ]</span><br><span class="line"><span class="number">80</span>,	<span class="number">0.07</span>,	[<span class="number">-0.787713</span>   <span class="number">1.4951732</span>]</span><br><span class="line"><span class="number">100</span>,	<span class="number">0.28</span>,	[<span class="number">-0.90760225</span>  <span class="number">1.683282</span>  ]</span><br><span class="line"><span class="number">120</span>,	<span class="number">0.01</span>,	[<span class="number">-0.92447585</span>  <span class="number">1.8040558</span> ]</span><br><span class="line"><span class="number">140</span>,	<span class="number">0.00</span>,	[<span class="number">-0.9492291</span>  <span class="number">1.8816731</span>]</span><br><span class="line"><span class="number">160</span>,	<span class="number">0.01</span>,	[<span class="number">-0.9679263</span>  <span class="number">1.9195584</span>]</span><br><span class="line"><span class="number">180</span>,	<span class="number">0.01</span>,	[<span class="number">-0.9705014</span>  <span class="number">1.9513198</span>]</span><br><span class="line"></span><br><span class="line">true w		 [<span class="number">-1.</span>  <span class="number">2.</span>]</span><br><span class="line">estimated w	 [<span class="number">-0.9696742</span>  <span class="number">1.9616756</span>]</span><br></pre></td></tr></table></figure>

<p>其中，咱们通过<code>np.random.choice(n)</code>随机选取一个数据用以更新而不是基于整个数据集更新。（这边相当于是针对data的stachastic）。</p>
<h2 id="CrossEntropyLoss"><a href="#CrossEntropyLoss" class="headerlink" title="CrossEntropyLoss"></a>CrossEntropyLoss</h2><p>现在，咱们用CrossEntropy作为loss函数来进行一个分类任务。</p>
<p>PyTorch在<a href="https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss" target="_blank" rel="noopener">CrossEntropyLoss</a>模块中预先实现了一个版本的cross entropy，它的用法和MSE稍微有点不同，所以这儿详细瞎逼逼一下它的参数：</p>
<ul>
<li><p>input:第一个参数就是咱们的分类神经网络的原始输出，它应该是一个维度为(N, C)的张量。其中，N是minibatch的大小，而C是class的数量。其中，第二维的数据是当前输入被分到各类别的没有normalize过的原始打分。CrossEntropyLoss模块自动为我们计算softmax，因此我们不需要自己算。</p>
</li>
<li><p>output:第二个参数是数据对应的label，用的时候应该输入一个长度为N的张量。每个维度上是正确的类别。</p>
</li>
</ul>
<p>下方代码展示三个预测在CrossEntropyLoss上的打分。正确的label为$y=[1, 1, 0]$。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loss = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">input = torch.tensor([[<span class="number">-1.</span>, <span class="number">1</span>],[<span class="number">-1</span>, <span class="number">1</span>],[<span class="number">1</span>, <span class="number">-1</span>]]) <span class="comment"># raw scores correspond to the correct class</span></span><br><span class="line"><span class="comment"># input = torch.tensor([[-3., 3],[-3, 3],[3, -3]]) # raw scores correspond to the correct class with higher confidence</span></span><br><span class="line"><span class="comment"># input = torch.tensor([[1., -1],[1, -1],[-1, 1]]) # raw scores correspond to the incorrect class</span></span><br><span class="line"><span class="comment"># input = torch.tensor([[3., -3],[3, -3],[-3, 3]]) # raw scores correspond to the incorrect class with incorrectly placed confidence</span></span><br><span class="line"></span><br><span class="line">target = torch.tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>])</span><br><span class="line">output = loss(input, target)</span><br><span class="line">print(output)</span><br></pre></td></tr></table></figure>



<p>输出:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt;</span><br><span class="line">tensor(<span class="number">0.1269</span>)</span><br></pre></td></tr></table></figure>

<h2 id="动态改变学习速率"><a href="#动态改变学习速率" class="headerlink" title="动态改变学习速率"></a>动态改变学习速率</h2><p>通常，我们不希望在训练过程中全程采用相同的学习速率。PyTorch提供相应组件支持根据训练进度自动调整学习速率。通常的策略包括每个epoch对学习速率lr乘以一个比率（比如0.9），并在训练loss下降地没那么快时将学习速率减半。</p>
<p>查看<a href="https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate" target="_blank" rel="noopener">learning rate scheduler docs</a>获取更多信息。</p>
<h2 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h2><p>当数据是图片时，我们通常希望使用卷积操作来怼它。PyTorh在<code>torch.nn.Conv2d</code>模块中实现了卷积操作。用户需要传入一个$(N,C_{in},H_{in},W_{in})$。其中，$N$是batch大小，$C_{in}$是通道数，$H_{in}$和$W_{in}$分别是输入图片的高和宽。</p>
<p>通过自定义以下参数，我们可以修改卷积操作：</p>
<ul>
<li>卷积核大小</li>
<li>步长</li>
<li>填充</li>
</ul>
<p>这些参数对输出张量的维度有影响，所以应该小心。</p>
<p>在<a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d" target="_blank" rel="noopener"><code>torch.nn.Conv2d</code> docs</a>中查看更多信息。</p>
<p>栗子：</p>
<p>在图片上怼一个gaussian blur kernel：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># a gaussian blur kernel</span></span><br><span class="line">gaussian_kernel = torch.tensor([[<span class="number">1.</span>, <span class="number">2</span>, <span class="number">1</span>],[<span class="number">2</span>, <span class="number">4</span>, <span class="number">2</span>],[<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>]]) / <span class="number">16.0</span></span><br><span class="line"></span><br><span class="line">conv = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># manually set the conv weight</span></span><br><span class="line">conv.weight.data[:] = gaussian_kernel</span><br><span class="line"></span><br><span class="line">convolved = conv(image_torch)</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">'original image'</span>)</span><br><span class="line">plt.imshow(image_torch.view(<span class="number">28</span>,<span class="number">28</span>).detach().numpy())</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">'blurred image'</span>)</span><br><span class="line">plt.imshow(convolved.view(<span class="number">26</span>,<span class="number">26</span>).detach().numpy())</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>读取一张RGB图片，输入通道数为3，输出通道数为16，在经过一个激活函数处理后，代码部分展示的输出结果又可以作为一个<code>Conv2d</code>的输入。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">im_channels = <span class="number">3</span> <span class="comment"># if we are working with RGB images, there are 3 input channels, with black and white, 1</span></span><br><span class="line">out_channels = <span class="number">16</span> <span class="comment"># this is a hyperparameter we can tune</span></span><br><span class="line">kernel_size = <span class="number">3</span> <span class="comment"># this is another hyperparameter we can tune</span></span><br><span class="line">batch_size = <span class="number">4</span></span><br><span class="line">image_width = <span class="number">32</span></span><br><span class="line">image_height = <span class="number">32</span></span><br><span class="line"></span><br><span class="line">im = torch.randn(batch_size, im_channels, image_width, image_height)</span><br><span class="line"></span><br><span class="line">m = nn.Conv2d(im_channels, out_channels, kernel_size)</span><br><span class="line">convolved = m(im) <span class="comment"># it is a module so we can call it</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'im shape'</span>, im.shape)</span><br><span class="line">print(<span class="string">'convolved im shape'</span>, convolved.shape)</span><br></pre></td></tr></table></figure>

<h2 id="一些蛮有用的链接"><a href="#一些蛮有用的链接" class="headerlink" title="一些蛮有用的链接"></a>一些蛮有用的链接</h2><ul>
<li><p><a href="https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html" target="_blank" rel="noopener">60 minute PyTorch Tutorial</a></p>
</li>
<li><p><a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">PyTorch Docs</a></p>
</li>
<li><p><a href="https://courses.cs.washington.edu/courses/cse446/19wi/notes/auto-diff.pdf" target="_blank" rel="noopener">Lecture notes on Auto-Diff</a></p>
</li>
</ul>
<h2 id="数据类"><a href="#数据类" class="headerlink" title="数据类"></a>数据类</h2><p><code>torch.utils.data.Dataset</code>是一个抽象类，咱们如果要弄一个自己的数据集的话就需要继承<code>Dataset</code>，然后覆盖以下的方法：</p>
<ul>
<li><code>__len__</code>，确保<code>len(dataset)</code>可以正确返回数据集的大小。</li>
<li><code>__getitem__</code>用来实现索引，确保dataset[i]可以正确拿到第i个数据。</li>
</ul>
<p>咱们现在来搞一个face landmarks数据集。咱们在<code>__init__</code>里头读取csv文件，但把读取图片的任务留给<code>__getitem__</code>。这样做的好处是节省内存，不需要将所有数据一下子全读到内存里去。</p>
<p>咱们数据集的sample是一个dict:<code>{&#39;image&#39;: image, &#39;landmarks&#39;: landmarks}</code>。咱们数据集还要再弄一个<code>transform</code>方法，这样任何需要的操作都可以被施加在图片上。咱们在下一节里头看看它的用处。</p>
<p>栗子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FaceLandmarksDataset</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    <span class="string">"""Face Landmarks dataset."""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, csv_file, root_dir, transform=None)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            csv_file (string): Path to the csv file with annotations.</span></span><br><span class="line"><span class="string">            root_dir (string): Directory with all the images.</span></span><br><span class="line"><span class="string">            transform (callable, optional): Optional transform to be applied</span></span><br><span class="line"><span class="string">                on a sample.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.landmarks_frame = pd.read_csv(csv_file)</span><br><span class="line">        self.root_dir = root_dir</span><br><span class="line">        self.transform = transform</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.landmarks_frame)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, idx)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> torch.is_tensor(idx):</span><br><span class="line">            idx = idx.tolist()</span><br><span class="line"></span><br><span class="line">        img_name = os.path.join(self.root_dir,</span><br><span class="line">                                self.landmarks_frame.iloc[idx, <span class="number">0</span>])</span><br><span class="line">        image = io.imread(img_name)</span><br><span class="line">        landmarks = self.landmarks_frame.iloc[idx, <span class="number">1</span>:]</span><br><span class="line">        landmarks = np.array([landmarks])</span><br><span class="line">        landmarks = landmarks.astype(<span class="string">'float'</span>).reshape(<span class="number">-1</span>, <span class="number">2</span>)</span><br><span class="line">        sample = &#123;<span class="string">'image'</span>: image, <span class="string">'landmarks'</span>: landmarks&#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.transform:</span><br><span class="line">            sample = self.transform(sample)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> sample</span><br></pre></td></tr></table></figure>

<p>然鹅，如果咱们直接用<code>for</code>把整个数据走一遍会少掉很多东西，比如:</p>
<ul>
<li>按batch读取data</li>
<li>随机读取data</li>
<li>多线程读取data</li>
</ul>
<p>不过施主先不要着急，这些功能<code>torch.utils.data.DataLoader</code>都有。通过修改<code>collate_fn</code>参数，你可以指定数据按什么样的batch方式被读取，不过默认的读取方法已经可以cover大部分的需求了。</p>
<p>🌰：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataloader = DataLoader(transformed_dataset, batch_size=<span class="number">4</span>,</span><br><span class="line">                        shuffle=<span class="literal">True</span>, num_workers=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i_batch, sample_batched <span class="keyword">in</span> enumerate(dataloader):</span><br><span class="line">    print(i_batch, sample_batched[<span class="string">'image'</span>].size(),</span><br><span class="line">          sample_batched[<span class="string">'landmarks'</span>].size())</span><br></pre></td></tr></table></figure>

<h2 id="混合精度训练"><a href="#混合精度训练" class="headerlink" title="混合精度训练"></a>混合精度训练</h2><p>作者: <code>Chi-Liang Liu</code> 引用: <a href="https://github.com/NVIDIA/apex。采用混合精度训练你的模型，训练出来的神经网络可以：" target="_blank" rel="noopener">https://github.com/NVIDIA/apex。采用混合精度训练你的模型，训练出来的神经网络可以：</a></p>
<ul>
<li>运行速度快2-4倍</li>
<li>几行代码就可以省下一笔买新内存的钱</li>
</ul>
<h2 id="Apex"><a href="#Apex" class="headerlink" title="Apex"></a>Apex</h2><p>nvidia维护的工具简化了Pytorch中的混合精度和分布式培训。这里的一些代码最终将包含在上游Pytorch中。Apex的目的是尽快为用户提供最新的实用工具。</p>
<h2 id="apex-amp"><a href="#apex-amp" class="headerlink" title="apex.amp"></a>apex.amp</h2><p>Amp允许用户轻松地尝试不同的纯模式和混合精度模式。通过选择“优化级”或opt_level选择常用的默认模式;每个opt_level建立一组属性来管理Amp实现的纯精度或混合精度训练。通过将特定属性的值直接传递给amp.initialize，可以实现对给定opt_level行为方式的细粒度控制。这些手动指定的值覆盖opt_level建立的默认值。</p>
<p>🌰:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Declare model and optimizer as usual, with default (FP32) precision</span></span><br><span class="line">model = torch.nn.Linear(D_in, D_out).cuda()</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">1e-3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Allow Amp to perform casts as required by the opt_level</span></span><br><span class="line">model, optimizer = amp.initialize(model, optimizer, opt_level=<span class="string">"O1"</span>)</span><br><span class="line">...</span><br><span class="line"><span class="comment"># loss.backward() becomes:</span></span><br><span class="line"><span class="keyword">with</span> amp.scale_loss(loss, optimizer) <span class="keyword">as</span> scaled_loss:</span><br><span class="line">    scaled_loss.backward()</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

]]></content>
      <tags>
        <tag>李宏毅</tag>
      </tags>
  </entry>
  <entry>
    <title>Pandas入门</title>
    <url>/2020/03/20/Pandas%E5%85%A5%E9%97%A8/</url>
    <content><![CDATA[<h1 id="Pandas"><a href="#Pandas" class="headerlink" title="Pandas"></a>Pandas</h1><h2 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h2><p>Pandas中的基础数据类型有Series和DataFrame，其中，Series具有一维索引（行索引），DataFrame具有二维索引（行索引和列索引）。</p>
<h3 id="Series"><a href="#Series" class="headerlink" title="Series"></a>Series</h3><p>既然是一维索引，就要有key,value对，可以直接传入一个dict。通过一个key，就可以确定一个value。<br><code>s = pd.Series({&#39;a&#39;: 10, &#39;b&#39;: 20, &#39;c&#39;: 30})</code></p>
<h3 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h3><p>需要行和列才可以确定一个元素。<br>可以按列传入</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">12df &#x3D; pd.DataFrame(&#123;&#39;one&#39;: pd.Series([1, 2, 3]),</span><br><span class="line">                   &#39;two&#39;: pd.Series([4, 5, 6])&#125;)</span><br></pre></td></tr></table></figure>

<p>也可以按行传入</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">123df &#x3D; pd.DataFrame([&#123;&#39;one&#39;: 1, &#39;two&#39;: 4&#125;,</span><br><span class="line">                   &#123;&#39;one&#39;: 2, &#39;two&#39;: 5&#125;,</span><br><span class="line">                   &#123;&#39;one&#39;: 3, &#39;two&#39;: 6&#125;])</span><br></pre></td></tr></table></figure>

<h2 id="数据读取"><a href="#数据读取" class="headerlink" title="数据读取"></a>数据读取</h2><p><a href="https://pandas.pydata.org/pandas-docs/stable/reference/io.html" target="_blank" rel="noopener">Input/output — pandas 1.0.1 documentation</a></p>
<h3 id="读取csv"><a href="#读取csv" class="headerlink" title="读取csv"></a>读取csv</h3><p><code>f = pd.read_csv()</code><br><img src="/" alt class="lazyload" data-src="/2020/03/20/Pandas%E5%85%A5%E9%97%A8/1.png"></p>
<h2 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h2><ul>
<li><code>df.head()</code>显示文件头部</li>
<li><code>df.tail()</code>显示文件尾部</li>
<li><code>df.describe()</code>显示数据概览</li>
<li><code>df.values</code>将DataFrame转换为numpy数组</li>
<li><code>df.index</code>行索引</li>
<li><code>df.columns</code>列索引</li>
<li><code>df.shape</code>DataFrame形状<br>所有属性：<a href="https://pandas.pydata.org/pandas-docs/stable/reference/frame.html#attributes-and-underlying-data" target="_blank" rel="noopener">DataFrame — pandas 1.0.1 documentation</a></li>
</ul>
<h2 id="数据选择"><a href="#数据选择" class="headerlink" title="数据选择"></a>数据选择</h2><p>数据选择就是将原始数据中的一部分拿出来，用于后续处理步骤</p>
<h3 id="基于索引数字选择"><a href="#基于索引数字选择" class="headerlink" title="基于索引数字选择"></a>基于索引数字选择</h3><ul>
<li><code>df.iloc[:3]</code> 选择前三行</li>
<li><code>df.iloc[5]</code>选择某一行</li>
<li><code>df.iloc[[行], [列]]</code> 其中，行和列都可以是list</li>
<li><code>df.iloc[:，1:4]</code>选择2到4列（切片时左闭右开）</li>
</ul>
<h3 id="基于标签的选择-切片时左右都闭"><a href="#基于标签的选择-切片时左右都闭" class="headerlink" title="基于标签的选择(切片时左右都闭)"></a>基于标签的选择(切片时左右都闭)</h3><ul>
<li><code>df.loc[0:2]</code>选择前三行</li>
<li><code>df.loc[[0,2,4]]</code>选择1、3、5行</li>
<li><code>df.loc[:,&#39;Total Population&#39;:&#39;Total Males&#39;]</code></li>
<li><code>df.loc[[0, 2], &#39;Median Age&#39;:]</code>选择1、3行的’Median Age’后面的列</li>
</ul>
<h3 id="数据删减"><a href="#数据删减" class="headerlink" title="数据删减"></a>数据删减</h3><ul>
<li><p><code>df.drop(labels=[&#39;Median Age&#39;, &#39;Total Males&#39;], axis = 1)</code>删除两个指定列</p>
</li>
<li><p><code>df.drop(labels=[0, 1], axis = 0)</code>删除0、1行</p>
</li>
<li><p><code>df.drop_dupicates()</code>数据去重</p>
</li>
<li><p><code>df.dropna()</code>数据去空值</p>
</li>
<li><p><code>df.insert(value = 要插入的值, loc=列号，column = &#39;列名&#39;)</code>在DataFrame中插入名字叫’列名’，值为value的一列。</p>
</li>
<li><p><code>df.isna()</code>返回bool</p>
</li>
<li><p><code>df.notna()</code>返回bool列表<br>填充缺失值</p>
</li>
<li><p><code>df.fillna()</code></p>
</li>
<li><p><code>df.fillna(method=&#39;pad&#39;)</code>使用前面的值填充空值</p>
</li>
<li><p>‘df.fillna(method=‘bfill’)’使用后面的值填充</p>
</li>
<li><p><code>df.fillna(df.mean()[&#39;C&#39;:&#39;E&#39;])</code>使用平均值填充<br>插值填充</p>
</li>
<li><pre><code>df.interpolate(method=)</code></pre><ul>
<li><code>method=&#39;quadratic&#39;</code>数据增长速率越来越快</li>
<li><code>method=&#39;pchip&#39;</code>累计分布</li>
<li><code>method=&#39;akima&#39;</code>以平滑绘图为目的<br>绘图</li>
</ul>
</li>
<li><p>df.plot(kind=‘bar’…)直接绘图<br>其它</p>
</li>
<li><p>数据计算 <a href="https://pandas.pydata.org/pandas-docs/stable/reference/frame.html#binary-operator-functions" target="_blank" rel="noopener">DataFrame — pandas 1.0.1 documentation</a></p>
</li>
<li><p>数据聚合 <a href="https://pandas.pydata.org/pandas-docs/stable/reference/frame.html#function-application-groupby-window" target="_blank" rel="noopener">DataFrame — pandas 1.0.1 documentation</a></p>
</li>
<li><p>统计分析 <a href="https://pandas.pydata.org/pandas-docs/stable/reference/frame.html#computations-descriptive-stats" target="_blank" rel="noopener">DataFrame — pandas 1.0.1 documentation</a></p>
</li>
<li><p>时间序列 <a href="https://pandas.pydata.org/pandas-docs/stable/reference/frame.html#time-series-related" target="_blank" rel="noopener">DataFrame — pandas 1.0.1 documentation</a></p>
</li>
</ul>
]]></content>
      <tags>
        <tag>Pandas</tag>
      </tags>
  </entry>
</search>
