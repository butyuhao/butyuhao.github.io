<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>爬虫入门总结</title>
    <url>/2020/06/06/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h4 id="浏览器请求"><a href="#浏览器请求" class="headerlink" title="浏览器请求"></a>浏览器请求</h4><ul>
<li>url<ul>
<li>url=请求协议+网站域名+资源路径+参数</li>
</ul>
</li>
<li>浏览器请求url地址<ul>
<li>当前url对应响应+js+css+图片—&gt;elements中内容</li>
<li>elements中内容可能与爬虫拿到的数据不同</li>
</ul>
</li>
<li>当前url所对应的响应在哪里<ul>
<li>从network中找到相应项目，点击response</li>
</ul>
</li>
</ul>
<h4 id="HTTP与HTTPS"><a href="#HTTP与HTTPS" class="headerlink" title="HTTP与HTTPS"></a>HTTP与HTTPS</h4><ul>
<li>HTTP:超文本传输协议<ul>
<li>明文形式传输</li>
<li>效率高，不安全</li>
</ul>
</li>
<li>HTTPS:HTTP+SSL<ul>
<li>传输前加密，读取前解密 </li>
<li>效率低，安全</li>
</ul>
</li>
<li>HTTP协议之请求<ul>
<li>HTTP请求行 GET XXX HTTP/ 1.1</li>
<li>请求头：<ul>
<li>User-agent：包含浏览器信息，有些网页不好爬的话可以考虑改成手机的UA</li>
<li>Cookie：用于携带用户信息，每次请求会被带上发给对方浏览器，服务器可能根据该字段判断是否为爬虫</li>
</ul>
</li>
<li>请求体:<ul>
<li>携带数据</li>
<li>get请求没有请求体</li>
<li>post请求有请求体</li>
</ul>
</li>
</ul>
</li>
<li>HTTP协议之相应<ul>
<li>响应头<ul>
<li>Set-Cookie:服务通过该字段设置Cookie到本地</li>
</ul>
</li>
<li>响应体<ul>
<li>Url地址所对应的响应</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>C++_Primer_笔记</title>
    <url>/2020/05/18/C-Primer-%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h2 id="第九章-内存模型和名称空间"><a href="#第九章-内存模型和名称空间" class="headerlink" title="第九章 内存模型和名称空间"></a>第九章 内存模型和名称空间</h2><h3 id="9-1-单独编译"><a href="#9-1-单独编译" class="headerlink" title="9.1 单独编译"></a>9.1 单独编译</h3><p>为了方便管理，一般我们将声明及定义放在不同的文件里。</p>
<ul>
<li>头文件：存放声明</li>
<li>源代码文件：存放定义</li>
</ul>
<p>不要将<code>函数定义</code>及<code>变量声明</code>放到头文件中，这可能引起重复定义，一般将下列内容放在头文件中：</p>
<ul>
<li>函数声明</li>
<li>使用#define或是const定义的常量</li>
<li>struct声明</li>
<li>类声明</li>
<li>模版声明</li>
<li>内联函数</li>
</ul>
<p>将struct放在头文件中是可以的，因为系统不为其创建变量。模版声明也是可以的，因为系统读到头文件里的模版声明时不编译其代码，而是用其来生成用户指定类型的函数定义。</p>
<p>对于文件的引用，有两种方法：</p>
<ul>
<li>include&lt;&gt; 引用系统目录中的文件</li>
<li>include”” 引用当前目录中的文件（用户自己创建的）</li>
</ul>
<p>注意：用ide时，不要去compile .h文件，应该compile .cpp文件，因为所有的h文件都被cpp文件include了，并且，不要在一个.cpp文件里include另一个.cpp文件。</p>
<p><strong>编译源代码的过程：</strong></p>
<p><img src="/" alt="IMG_0131" class="lazyload" data-src="/2020/05/18/C-Primer-%E7%AC%94%E8%AE%B0/IMG_0131.jpg"></p>
<ul>
<li>首先将各种预处理指令，比如include替换成相应的文本，产生temp.cpp文件</li>
<li>然后为每个translation unit创建一个.o的目标文件</li>
<li>最后将.o文件、库代码以及启动码链接起来，生成可运行的a.out。</li>
</ul>
<p>注意：在写头文件时记得使用防卫式编程，以避免重复引用。</p>
<p><strong>Trick：</strong></p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> a;</span><br><span class="line"><span class="keyword">while</span>(<span class="built_in">cin</span> &gt;&gt; a) &#123;</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>采用以上方法编程时，当cin读取到不为<code>q</code>的取值时，while循环都为true。</p>
<h3 id="9-2-存储持续性，作用域和链接性"><a href="#9-2-存储持续性，作用域和链接性" class="headerlink" title="9.2 存储持续性，作用域和链接性"></a>9.2 存储持续性，作用域和链接性</h3><p><strong>持续性</strong>：何时存在于内存中？</p>
<p><strong>作用域</strong>：在哪儿能访问到？</p>
<p><strong>链接性</strong>：别的文件能不能访问到？</p>
<p>不同的存储方案是通过存储的持续性，作用域和链接性描述的。</p>
<p><strong>C++提供三种不同的存储方案：</strong></p>
<ol>
<li><strong>自动存储持续性</strong></li>
</ol>
<p>离开代码块时，内存自动被释放。</p>
<ol start="2">
<li><strong>静态存储持续性</strong></li>
</ol>
<p>以下两项的存储持续性都为静态(在程序的整个生命周期都存在)：</p>
<ul>
<li>函数外定义的变量</li>
<li>static定义的变量</li>
</ul>
<ol start="3">
<li><strong>动态存储持续性</strong></li>
</ol>
<p>使用new运算符分配，直到使用delete运算符释放内存。</p>
<h4 id="9-2-1-作用域和链接"><a href="#9-2-1-作用域和链接" class="headerlink" title="9.2.1 作用域和链接"></a>9.2.1 作用域和链接</h4><p>作用域(scope)描述一个名称在translation unit中的多大范围内可见。</p>
<ul>
<li>作用域为局部的变量只在定义其的代码块中可用</li>
<li>自动变量的作用域为局部</li>
<li>函数原型作用域中使用的名称只在包含参数列表的括号中可用（所以函数原型中的各个参数的名称或者是否出现都不是很重要。）</li>
</ul>
<h4 id="9-2-2-自动存储持续性"><a href="#9-2-2-自动存储持续性" class="headerlink" title="9.2.2 自动存储持续性"></a>9.2.2 自动存储持续性</h4><p>函数中声明的参数和变量：</p>
<ul>
<li>存储持续性–&gt;自动</li>
<li>链接性–&gt;没有</li>
</ul>
<p>如果一个局部变量的名称和一个全局的一样，则进入局部变量的代码块后，在该代码块内，全局的那个将被暂时“隐藏”。在出代码块后，全局的那个被恢复。</p>
<p><img src="/" alt="IMG_0132" class="lazyload" data-src="/2020/05/18/C-Primer-%E7%AC%94%E8%AE%B0/IMG_0132.jpg"></p>
<p>C++11中的auto用于自动类型推断。</p>
<ol>
<li><strong>自动变量的初始化</strong></li>
</ol>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> w;<span class="comment">//不初始化的自动变量</span></span><br><span class="line"><span class="keyword">int</span> x = <span class="number">5</span>;<span class="comment">//使用常数初始化的自动变量</span></span><br></pre></td></tr></table></figure>

<ol start="2">
<li><strong>自动变量和栈</strong></li>
</ol>
<p><img src="/" alt="IMG_0133" class="lazyload" data-src="/2020/05/18/C-Primer-%E7%AC%94%E8%AE%B0/IMG_0133.jpg"></p>
<p>函数中的自动变量在使用时被押入栈，当函数运行完毕时，直接将栈顶指针指向未进入函数前的位置，相当于将进入函数后创建的自动变量都直接扔掉。</p>
<ol start="3">
<li><strong>寄存器变量</strong></li>
</ol>
<p>C++11前表明该变量被经常使用，建议编译器用cpu寄存器存储它，现在obsolete。</p>
<h4 id="9-2-3-静态持续变量"><a href="#9-2-3-静态持续变量" class="headerlink" title="9.2.3 静态持续变量"></a>9.2.3 静态持续变量</h4><p>所有静态持续变量在整个程序执行期间都存在。</p>
<p>C++为静态存储持续性变量提供了3种链接性：</p>
<ol>
<li>外部链接性（可在其它文件中访问）</li>
</ol>
<p>在代码块外部声明</p>
<ol start="2">
<li><p>内部链接性（只能在当前文件访问）</p>
</li>
<li><p>无链接性（只能在当前函数或代码块中访问）</p>
</li>
</ol>
<p>在代码块内部声明</p>
<p>编译器分配固定大小内存来存储所有静态变量。如果没有显式初始化静态变量，编译器自动将其置零。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> global = <span class="number">1000</span>;</span><br><span class="line"><span class="keyword">static</span> <span class="keyword">int</span> one_file = <span class="number">50</span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span> <span class="params">()</span> </span>&#123;</span><br><span class="line">	</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">func</span> <span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">static</span> in count = <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>global可在别的文件中用extern访问，one_file因为加上了static，只能在当前文件被访问，count只能在func中被访问。另外，无论能不能被访问，global、one_file、count在整个程序执行期间都是存在的。</p>
<p>以下是5种变量存储方式：</p>
<p><img src="/" alt="IMG_0134" class="lazyload" data-src="/2020/05/18/C-Primer-%E7%AC%94%E8%AE%B0/IMG_0134.jpg"></p>
<p><strong>静态变量的初始化</strong></p>
<p>静态初始化（在编译器处理translation unit的时候初始化）：</p>
<ul>
<li>零初始化</li>
<li>常量表达式初始化</li>
</ul>
<p>动态初始化（在编译后初始化）</p>
<h4 id="9-2-4-静态持续性、外部链接性"><a href="#9-2-4-静态持续性、外部链接性" class="headerlink" title="9.2.4 静态持续性、外部链接性"></a>9.2.4 静态持续性、外部链接性</h4><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">double</span> up;<span class="comment">//定义</span></span><br><span class="line"><span class="keyword">extern</span> <span class="keyword">int</span> blem;<span class="comment">//在别处定义（声明一个外部变量）</span></span><br><span class="line"><span class="keyword">extern</span> <span class="keyword">char</span> gr = <span class="string">'Z'</span>;<span class="comment">//定义，因为其被初始化，导致分配空间。（定义一个全局变量并给初值）</span></span><br></pre></td></tr></table></figure>

<p>当出现两个同名的变量时，假如都叫做warming,则在范围更小的代码块内使用::warming可以使得全局的warming不被局部的warming遮蔽。</p>
<p>一般来说，我们尽量使用局部变量，这样可以防止不小心用到同名的全局变量，不过在写一些常量数据的时候，最好使用全局变量+const的组合。</p>
<h4 id="9-2-5-静态持续性、内部链接性"><a href="#9-2-5-静态持续性、内部链接性" class="headerlink" title="9.2.5 静态持续性、内部链接性"></a>9.2.5 静态持续性、内部链接性</h4><ol>
<li>直接在函数外声明变量，该变量链接性为外部</li>
<li>使用关键字extern来使用别的源文件中，链接性为外部的变量</li>
<li>使用static将变量的链接性限定在代码块或本文件内。</li>
</ol>
<h4 id="9-2-6-静态存储持续性、无链接性"><a href="#9-2-6-静态存储持续性、无链接性" class="headerlink" title="9.2.6 静态存储持续性、无链接性"></a>9.2.6 静态存储持续性、无链接性</h4><p>在代码块中使用static创建变量，则该变量作用域在代码块中，持续性是程序的生命周期，且两次函数调用之间，该变量的值是不变的。</p>
<p><strong>Trick:</strong></p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cin</span>.<span class="built_in">get</span>(数组， 数组大小)</span><br></pre></td></tr></table></figure>

<p>采用以上方法，cin接收用户的输入，并将其存入数组中。并且，当用户输入超出数组大小时，其余字母留在缓冲区内，当字母数量不超过数组大小时，使用cin.get()得到的是’\n’。</p>
<h4 id="9-2-7说明符和限定符"><a href="#9-2-7说明符和限定符" class="headerlink" title="9.2.7说明符和限定符"></a>9.2.7说明符和限定符</h4><p><strong>说明符</strong></p>
<ul>
<li>auto</li>
<li>register</li>
<li>static</li>
<li>extern</li>
<li>thread_local</li>
</ul>
<p>变量的生命周期与线程相同。</p>
<ul>
<li>mutable</li>
</ul>
<p>消除const的限制，使得某const中的变量能被修改。</p>
<p><strong>限定符</strong></p>
<ul>
<li>const</li>
</ul>
<p>使用了const，不仅限定参数不能被修改，还将变量的链接性限制在了文件内。而这也是为什么const的变量可以被放在头文件中被一堆源文件引用的原因。虽然语句被插入每个源文件，但是链接性都在文件内，因此互不影响。而如果想要定义链接性为外部的const变量，应该加上extern。</p>
<ul>
<li>volatile</li>
</ul>
<p>当一个值被用到多次时，编译器可能自动优化，将该值放到cpu寄存器中而不是去寻找该值。但有时某个值其实是程序与外部硬件通讯的信号，那个值也可能被外部硬件改变。因此将其声明为volatile，防止编译器自动移动其位置。</p>
<h4 id="9-2-8-函数和链接性"><a href="#9-2-8-函数和链接性" class="headerlink" title="9.2.8 函数和链接性"></a>9.2.8 函数和链接性</h4><ul>
<li>在声明函数的时候使用extern来表明该函数是外部的</li>
<li>使用static来修饰函数的声明及定义来表明函数的链接性是内部的。</li>
</ul>
<p>除了内联函数外的其它函数都只能被定义一遍。</p>
<h4 id="9-1-9-语言链接性"><a href="#9-1-9-语言链接性" class="headerlink" title="9.1.9 语言链接性"></a>9.1.9 语言链接性</h4><p>C++的链接程序为不同函数提供不同的符号名，而C语言里没有。因此提供以下方法让用户自己决定要按哪种语言的方式来处理定义的函数。</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line"><span class="comment">//使用C的方式查找函数</span></span><br><span class="line"><span class="keyword">extern</span> <span class="string">"C"</span> <span class="function"><span class="keyword">void</span> <span class="title">spiff</span><span class="params">(<span class="keyword">int</span>)</span></span></span><br><span class="line"><span class="function"><span class="comment">//使用C++的方式查找函数</span></span></span><br><span class="line">extern "C++" void spiff(int)</span><br></pre></td></tr></table></figure>

<h4 id="9-2-10-存储方案和动态分配"><a href="#9-2-10-存储方案和动态分配" class="headerlink" title="9.2.10 存储方案和动态分配"></a>9.2.10 存储方案和动态分配</h4><p>编译器通常使用三块内存：</p>
<ul>
<li>存放静态变量 ：存放全局变量或者静态变量</li>
<li>存放自动变量：存放局部变量</li>
<li>实现动态存储：存放new出来或者malloc出来的东西</li>
</ul>
<ol>
<li>使用new运算符初始化</li>
</ol>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> *pi = <span class="keyword">new</span> <span class="keyword">int</span>(<span class="number">6</span>) <span class="comment">//声明指针，自动分配内存，并将值初始化为6</span></span><br><span class="line"><span class="keyword">int</span> * ar = <span class="keyword">new</span> <span class="keyword">int</span>[<span class="number">4</span>] &#123;<span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">5</span>&#125;<span class="comment">//使用大括号来初始化数组，大括号也可用于单值的初始化</span></span><br></pre></td></tr></table></figure>

<ol start="2">
<li>new失败</li>
</ol>
<p>最初10年中，new失败返回null指针，现在抛出std::bad_alloc异常。</p>
<ol start="3">
<li>new：运算符、函数和替换函数</li>
</ol>
<p>new和new[]都属于运算符。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> * pi = <span class="keyword">new</span> <span class="keyword">int</span>;</span><br><span class="line">被转换为</span><br><span class="line"><span class="keyword">int</span> * pi = <span class="keyword">new</span>(<span class="keyword">sizeof</span>(<span class="keyword">int</span>))</span><br><span class="line">  </span><br><span class="line"><span class="keyword">int</span> * pa - <span class="keyword">new</span> <span class="keyword">int</span>[<span class="number">40</span>]</span><br><span class="line">被转换为</span><br><span class="line"><span class="keyword">int</span> * pa = <span class="keyword">new</span>(<span class="number">40</span> * <span class="keyword">sizeof</span>(<span class="keyword">int</span>))</span><br><span class="line">  </span><br><span class="line"><span class="keyword">delete</span> pi;</span><br><span class="line">被转换为</span><br><span class="line"><span class="keyword">delete</span>(pi);</span><br></pre></td></tr></table></figure>

<p>如果用户具有足够知识，可以自己写new及delete。</p>
<p>4.定位new运算符</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">chaff</span> &#123;</span></span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">char</span> buffer1[<span class="number">50</span>];</span><br><span class="line"><span class="keyword">char</span> buffer2[<span class="number">100</span>];</span><br><span class="line">p1 = <span class="keyword">new</span> (buffer1) <span class="keyword">int</span>[<span class="number">20</span>] <span class="comment">//将int[20]放到buffer1中</span></span><br><span class="line">p2 = <span class="keyword">new</span> (buffer2) chaff <span class="comment">//将chaff放到buffer2里头去</span></span><br></pre></td></tr></table></figure>

<p>常规new与定位new的差别：</p>
<ul>
<li>常规new出来的空间可以用delete释放，但是不能用delete释放定位new的空间，因为其所用的空间本身就不是动态的。</li>
</ul>
<p>定位new运算符可与初始化结合使用，将信息放在特定的硬件位置。</p>
<p>定位new运算符的原理是将传递给他的地址强制转换为void *类型。</p>
<h3 id="9-3-名称空间"><a href="#9-3-名称空间" class="headerlink" title="9.3 名称空间"></a>9.3 名称空间</h3><p>术语：</p>
<ul>
<li>声明区域：可在其中进行声明的区域</li>
<li>潜在作用域：从声明位置开始到声明区域结束</li>
<li>作用域：变量对程序而言的可见范围。</li>
</ul>
<p><strong>与名称空间有关的几个使用方法：</strong></p>
<ul>
<li>创建名称空间</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">namespace</span> <span class="built_in">std</span> &#123;</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>using声明</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">using</span> <span class="built_in">std</span>::<span class="built_in">cout</span>;</span><br><span class="line"><span class="keyword">using</span> <span class="built_in">std</span>::<span class="built_in">cin</span>;</span><br></pre></td></tr></table></figure>

<ul>
<li>using编译</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">在代码开头或者函数中用</span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br></pre></td></tr></table></figure>

<ul>
<li>解析运算符</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"fuck c++"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br></pre></td></tr></table></figure>



<p><strong>using声明与using编译的不同之处：</strong></p>
<p>当名称空间和声明区域中定义了相同的名称，使用using声明导致出错，使用using编译，则局部的那个名称隐藏全局的那个名称。</p>
<p>注意：using 一个namespace，则也会顺便using这个namespace中包含的所有namespace。</p>
<p>此外，可以使用以下方法来给已经有的名称空间取一个别名:</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">namespace</span> WTF = <span class="built_in">std</span>;</span><br></pre></td></tr></table></figure>

<p>我们还能创建一个未命名的名称空间：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">namespace</span>  &#123;</span><br><span class="line">  <span class="comment">//未命名的名称空间不能用using来使用，且在其中定义的变量相当于被定义为static的。</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>注意：</p>
<ul>
<li>定义和声明必须放在同一个名称空间中。</li>
<li>using namespace XXX是个不太好的做法， 直接用解析运算符是更好的做法。</li>
<li>如果函数被重载，则using该函数将导入所有的重载版本。</li>
</ul>
]]></content>
      <tags>
        <tag>C++</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>CMake基础</title>
    <url>/2020/04/13/CMake%E5%9F%BA%E7%A1%80/</url>
    <content><![CDATA[<h2 id="Hello-World"><a href="#Hello-World" class="headerlink" title="Hello World"></a>Hello World</h2><h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><h3 id="CMakeLists-txt"><a href="#CMakeLists-txt" class="headerlink" title="CMakeLists.txt"></a>CMakeLists.txt</h3><p>该文件保存所有CMake命令，挡在shell中运行cmake时，它将寻找该txt文件，如果找不到，cmake将报错并退出。</p>
<h3 id="最低CMake版本"><a href="#最低CMake版本" class="headerlink" title="最低CMake版本"></a>最低CMake版本</h3><p>用户可以指定可运行CMakeLists.txt文件的最低CMake版本，通过以下语句指定：</p>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line">cmake_minimun_required(VERSION <span class="number">3.5</span>)</span><br></pre></td></tr></table></figure>

<h3 id="项目名称"><a href="#项目名称" class="headerlink" title="项目名称"></a>项目名称</h3><p>通过指定项目名称，使得指定变量变得更加容易。</p>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">project</span>(hello_cmake)</span><br></pre></td></tr></table></figure>

<p>PS:当采用该命令指定项目名称后，自动添加一个变量${PROJECT_NAME}，其值就是指定的项目名称hello_cmake。</p>
<h3 id="创建可执行文件"><a href="#创建可执行文件" class="headerlink" title="创建可执行文件"></a>创建可执行文件</h3><p><code>add_executable()</code>明确了可执行文件应该从哪些源文件(source file)中被构建(build)。该命令含有两个参数，第一个参数是要构建的可执行文件的名称，第二个参数是将要被编译的源文件的列表。</p>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">add_executable</span>(hello_cmake main.cpp)</span><br></pre></td></tr></table></figure>

<p>将上面的三步合起来，就是：</p>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">cmake_minimum_required</span>(VERSION <span class="number">3.5</span>)</span><br><span class="line"><span class="keyword">project</span> (hello_cmake)</span><br><span class="line"><span class="keyword">add_executable</span>(<span class="variable">$&#123;PROJECT_NAME&#125;</span> main.cpp)</span><br></pre></td></tr></table></figure>

<h3 id="Binary-Directory"><a href="#Binary-Directory" class="headerlink" title="Binary Directory"></a>Binary Directory</h3><p>运行cmake指令的文件夹就是Binary Directory，该文件夹用于存放cmake build出的各种文件，该文件夹的路径保存在<code>CMAKE_BINARY_DIR</code>中。通常CMake支持两种build方式：</p>
<ul>
<li><h4 id="In-Place-Build"><a href="#In-Place-Build" class="headerlink" title="In-Place Build"></a>In-Place Build</h4></li>
</ul>
<p>该方式直接在存放代码的文件夹运行cmake，由于生成的文件会和代码文件混在一起，通常不采用这种方式。</p>
<ul>
<li><h4 id="Out-of-Source-Build"><a href="#Out-of-Source-Build" class="headerlink" title="Out-of-Source Build"></a>Out-of-Source Build</h4></li>
</ul>
<p>该方式在代码所在目录中新建一个叫做build的文件夹，然后在里头进行cmake的build工作，这样可以很方便地管理build过程中产生的各种文件。通常这么做：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mkdir build</span><br><span class="line"><span class="built_in">cd</span> build</span><br><span class="line">cmake ..</span><br></pre></td></tr></table></figure>

<h2 id="头文件"><a href="#头文件" class="headerlink" title="头文件"></a>头文件</h2><h3 id="目录路径"><a href="#目录路径" class="headerlink" title="目录路径"></a>目录路径</h3><p>CMake中有一系列实用的目录路径参数，常用的有：</p>
<p>更多参数，请看：<a href="https://gitlab.kitware.com/cmake/community/-/wikis/doc/cmake/Useful-Variables" target="_blank" rel="noopener">https://gitlab.kitware.com/cmake/community/-/wikis/doc/cmake/Useful-Variables</a></p>
<table>
<thead>
<tr>
<th>参数</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>CMAKE_SOURCE_DIR</td>
<td>源文件目录</td>
</tr>
<tr>
<td>CMAKE_CURRENT_SOURCE_DIR</td>
<td>当前project的源文件目录</td>
</tr>
<tr>
<td>PROJECT_SOURCE_DIR</td>
<td>当前项目的根目录</td>
</tr>
<tr>
<td>CMAKE_BINARY_DIR</td>
<td>运行cmake的目录（pwd）</td>
</tr>
<tr>
<td>CMAKE_CURRENT_BINARY_DIR</td>
<td>现在所在的build路径</td>
</tr>
<tr>
<td>PROJECT_BINARY_DIR</td>
<td>当前项目的build文件夹</td>
</tr>
</tbody></table>
<h3 id="保存源文件的变量"><a href="#保存源文件的变量" class="headerlink" title="保存源文件的变量"></a>保存源文件的变量</h3><p>创建一个含有当前所有源文件的变量让你可以更轻松地将这些源文件加入到一些命令中，例如<code>add_executable()</code>命令。</p>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Create a sources variable with a link to all cpp files to compile</span></span><br><span class="line"><span class="keyword">set</span>(SOURCES</span><br><span class="line">    src/Hello.cpp</span><br><span class="line">    src/main.cpp</span><br><span class="line">)</span><br><span class="line"><span class="keyword">add_executable</span>(<span class="variable">$&#123;PROJECT_NAME&#125;</span> <span class="variable">$&#123;SOURCES&#125;</span>)</span><br></pre></td></tr></table></figure>

<p>ps：不过，现在不太流行使用SOURCES，比较推荐直接将文件写在add_xxx函数里头。</p>
<h3 id="Include-目录"><a href="#Include-目录" class="headerlink" title="Include 目录"></a>Include 目录</h3><p>当你有不同的需要include目录的时候，你可以使用<code>target_include_directories()</code>函数来将目录include进来，这相当于使用命令行时的<code>-I /directory/path</code>。</p>
<p>当编译<code>target</code>的时候，如果其需要include一些目录，使用以下语句，CMake将自动include指定的目录。</p>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">target_include_directories</span>(<span class="keyword">target</span></span><br><span class="line">    PRIVATE</span><br><span class="line">        <span class="variable">$&#123;PROJECT_SOURCE_DIR&#125;</span>/<span class="keyword">include</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h3 id="编译时获得更加详细的输出"><a href="#编译时获得更加详细的输出" class="headerlink" title="编译时获得更加详细的输出"></a>编译时获得更加详细的输出</h3><p>在编译时如果想要获得更加详尽的数据，可以使用：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">make clean</span><br><span class="line">make VERBOSE=1</span><br></pre></td></tr></table></figure>

<h2 id="静态库"><a href="#静态库" class="headerlink" title="静态库"></a>静态库</h2><h3 id="添加一个静态库"><a href="#添加一个静态库" class="headerlink" title="添加一个静态库"></a>添加一个静态库</h3><p>使用<code>add_library()</code>来将一些源文件组合为库，使用以下方法调用：</p>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">add_library</span>(hello_library STATIC</span><br><span class="line">    src/Hello.cpp</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>上方命令将会创建一个叫做libhello_library.a的静态库，其包含命令中提到的源文件。</p>
<h3 id="Include目录Plus"><a href="#Include目录Plus" class="headerlink" title="Include目录Plus"></a>Include目录Plus</h3><p>在这里，我们也可以使用<code>target_include_directories</code>来将一些目录include到当前库中：</p>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">target_include_directories</span>(hello_library</span><br><span class="line">    PUBLIC</span><br><span class="line">        <span class="variable">$&#123;PROJECT_SOURCE_DIR&#125;</span>/<span class="keyword">include</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>使用以上语句，则在下列情况发生的时候，CMake自动include你指定的目录：</p>
<ul>
<li>当编译该library的时候</li>
<li>当编译任何链接到此library的target的时候</li>
</ul>
<p>以下是几个scope的含义：</p>
<ul>
<li>PRIVATE 当编译“指定的target”时，include指定目录。</li>
<li>INTERFACE 当编译“用到该库的任何target”的时候，include指定目录。</li>
<li>PUBLIC 包含以上两种情况</li>
</ul>
<p>TIP:</p>
<ul>
<li>对于公共头文件，将包含文件夹设置为带有子目录的“命名空间”通常是一个好主意。</li>
<li>传递到target_include_folders的目录将是包含目录树的根目录，c++文件应该包含从那里到头文件的路径。</li>
</ul>
<p>例如：</p>
<p><code>#include &quot;static/Hello.h&quot;</code></p>
<h3 id="链接库"><a href="#链接库" class="headerlink" title="链接库"></a>链接库</h3><p>当创建的可执行文件需要用到某个库的时候，你必须使用以下语句来告诉编译器：</p>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">add_executable</span>(hello_binary</span><br><span class="line">    src/main.cpp</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">target_link_libraries</span>( hello_binary</span><br><span class="line">    PRIVATE</span><br><span class="line">        hello_library</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h2 id="动态库"><a href="#动态库" class="headerlink" title="动态库"></a>动态库</h2><h3 id="添加动态库"><a href="#添加动态库" class="headerlink" title="添加动态库"></a>添加动态库</h3><p>与之前静态库的例子类似，<code>add_library()</code>函数可以用来将一些源文件创建为动态库，像是这样调用：</p>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">add_library</span>(hello_library SHARED</span><br><span class="line">    src/Hello.cpp</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>上方语句创建一个名为<code>libhello_library.so</code>的动态库，其中包含了指定的源文件。</p>
<h3 id="目标别名"><a href="#目标别名" class="headerlink" title="目标别名"></a>目标别名</h3><p>见名知意，目标别名就是可以给目标起一个新名字，如下方语句就添加叫做hello_library的库，且给其起了一个别名叫做hello::library。</p>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">add_library</span>(hello::library ALIAS hello_library)</span><br></pre></td></tr></table></figure>

<p>如下所示，当将目标与其它目标链接时，可以用别名指代它。</p>
<h3 id="链接动态库"><a href="#链接动态库" class="headerlink" title="链接动态库"></a>链接动态库</h3><p>链接动态库与链接静态库的方式相同，在创建可执行文件后，将库链接到它。</p>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">add_executable</span>(hello_binary</span><br><span class="line">    src/main.cpp</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">target_link_libraries</span>(hello_binary</span><br><span class="line">    PRIVATE</span><br><span class="line">        hello::library</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>上方语句让CMake将hello_library链接到hello_binary，且使用hello_library的别名hello::library来指代它。</p>
<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>CMake允许用户添加一个<code>make install</code>目标来安装二进制文件、库和其它文件。默认的安装位置由<code>CMAKE_INSTALL_PREFIX</code>指定。可由语句<code>cmake .. -DCMAKE_INSTALL_PREFIX=/install/location</code>指定。  </p>
<p>要安装的文件由<code>install()</code>函数指定：</p>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">install</span> (TARGETS cmake_examples_inst_bin</span><br><span class="line">    DESTINATION bin)</span><br></pre></td></tr></table></figure>

<p>该语句将目标<code>cmake_examples_inst_bin</code>产生的二进制文件安装到<code>${CMAKE_INSTALL_PREFIX}/bin</code>。</p>
<p>PS：在有DLL目标的平台上，比如windows，你需要运行下方的语句：</p>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">install</span> (TARGETS cmake_examples_inst</span><br><span class="line">    LIBRARY DESTINATION lib</span><br><span class="line">    RUNTIME DESTINATION bin)</span><br></pre></td></tr></table></figure>

<p>使用以下语句来安装与<code>cmake_examples_inst library</code>相对应的头文件到<code>${CMAKE_INSTALL_PREFIX}/include directory</code>。</p>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">install</span> (FILES cmake-examples.conf</span><br><span class="line">    DESTINATION etc)</span><br></pre></td></tr></table></figure>

<p>使用以下语句将配置文件安装到<code>${CMAKE_INSTALL_PREFIX}/etc</code>。</p>
<p>在运行<code>make install</code>后，CMake产生一个install_manifest.txt文件，其中包含了安装的细节。</p>
<p>PS：在运行文件前，如果<code>/usr/local/lib</code>如果不在你的library path里头，应该先将其添加到path。</p>
<p>如果你不想将库安装到默认的<code>/usr/local/</code>,你可以使用以下语句将文件安装到你的build目录。</p>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span>( CMAKE_INSTALL_PREFIX_INITIALIZED_TO_DEFAULT )</span><br><span class="line">  <span class="keyword">message</span>(STATUS <span class="string">"Setting default CMAKE_INSTALL_PREFIX path to $&#123;CMAKE_BINARY_DIR&#125;/install"</span>)</span><br><span class="line">  <span class="keyword">set</span>(CMAKE_INSTALL_PREFIX <span class="string">"$&#123;CMAKE_BINARY_DIR&#125;/install"</span> CACHE <span class="keyword">STRING</span> <span class="string">"The path to use for make install"</span> FORCE)</span><br><span class="line"><span class="keyword">endif</span>()</span><br></pre></td></tr></table></figure>

<h3 id="DESTDIR"><a href="#DESTDIR" class="headerlink" title="DESTDIR"></a>DESTDIR</h3><p>如果你想在<code>make install</code>之前确认一下是否所有的文件都包含进去了，可以使用<code>DESTDIR</code>命令：</p>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line">make <span class="keyword">install</span> DESTDIR=/tmp/stage</span><br></pre></td></tr></table></figure>

<p>它将会将所有文件安装到你指定的这个文件夹底下，这样你可以在正式安装之前检查一下是否缺少什么东西。</p>
<h2 id="Build-Type"><a href="#Build-Type" class="headerlink" title="Build Type"></a>Build Type</h2><p>CMake提供了很多build时的配置，用户指定的优化级别以及debug信息将被包含在二进制文件中。</p>
<p>CMake提供以下级别：</p>
<ul>
<li>Release - Adds the <code>-O3 -DNDEBUG</code> flags to the compiler</li>
<li>Debug - Adds the <code>-g</code> flag</li>
<li>MinSizeRel - Adds <code>-Os -DNDEBUG</code></li>
<li>RelWithDebInfo - Adds <code>-O2 -g -DNDEBUG</code> flags</li>
</ul>
<h3 id="设置Build-Type"><a href="#设置Build-Type" class="headerlink" title="设置Build Type"></a>设置Build Type</h3><p>你可以使用一个gui工具，例如ccmake或是cmake-gui来改变build type，或者使用以下的语句：</p>
<p><code>cmake .. -DCMAKE_BUILD_TYPE=Release</code></p>
<h3 id="设置默认的Build-Type"><a href="#设置默认的Build-Type" class="headerlink" title="设置默认的Build Type"></a>设置默认的Build Type</h3><p>你可以在CMakeLists的顶部添加以下配置来设置默认的Build Type：</p>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span>(<span class="keyword">NOT</span> CMAKE_BUILD_TYPE <span class="keyword">AND</span> <span class="keyword">NOT</span> CMAKE_CONFIGURATION_TYPES)</span><br><span class="line">  <span class="keyword">message</span>(<span class="string">"Setting build type to 'RelWithDebInfo' as none was specified."</span>)</span><br><span class="line">  <span class="keyword">set</span>(CMAKE_BUILD_TYPE RelWithDebInfo CACHE <span class="keyword">STRING</span> <span class="string">"Choose the type of build."</span> FORCE)</span><br><span class="line">  <span class="comment"># Set the possible values of build type for cmake-gui</span></span><br><span class="line">  <span class="keyword">set_property</span>(CACHE CMAKE_BUILD_TYPE PROPERTY STRINGS <span class="string">"Debug"</span> <span class="string">"Release"</span></span><br><span class="line">    <span class="string">"MinSizeRel"</span> <span class="string">"RelWithDebInfo"</span>)</span><br><span class="line"><span class="keyword">endif</span>()</span><br></pre></td></tr></table></figure>

<h2 id="Compile-Flags"><a href="#Compile-Flags" class="headerlink" title="Compile Flags"></a>Compile Flags</h2><p>CMake支持使用多种方式来指定编译时的flag。</p>
<h3 id="为不同目标设置特定Flag"><a href="#为不同目标设置特定Flag" class="headerlink" title="为不同目标设置特定Flag"></a>为不同目标设置特定Flag</h3><p>使用以下语句来使得编译器编译目标时添加-DEX3选项。</p>
<h3 id="设置默认的C-Flag"><a href="#设置默认的C-Flag" class="headerlink" title="设置默认的C++Flag"></a>设置默认的C++Flag</h3><p>将以下语句添加到CMakeLists来设置默认的C++ Flag。</p>
<p><code>set (CMAKE_CXX_FLAGS &quot;${CMAKE_CXX_FLAGS} -DEX2&quot; CACHE STRING &quot;Set C++ Compiler Flags&quot; FORCE)</code></p>
<p>与<code>CMAKE_CXX_FLAGS</code>相似，可以设置以下选项：</p>
<ul>
<li><p>Setting C compiler flags using CMAKE_C_FLAGS</p>
</li>
<li><p>Setting linker flags using CMAKE_LINKER_FLAGS</p>
</li>
</ul>
<p>PS:上方的<code>CACHE STRING &quot;Set C++ Compiler Flags&quot; FORCE</code>用来强制将参数写到CMakeCache.txt文件中。</p>
<p>一旦设置了<code>CMAKE_CXX_FLAGS</code>或是<code>CMAKE_C_FLAGS</code>，定义将广播到该文件夹下的所有目标中（包含子文件夹）。现在更推荐使用<code>target_compile_definitions</code>。</p>
<h3 id="设置CMake-Flag"><a href="#设置CMake-Flag" class="headerlink" title="设置CMake Flag"></a>设置CMake Flag</h3><p>采用<code>cmake .. -DCMAKE_CXX_FLAGS=&quot;-DEX3&quot;</code>添加CMake的flag。</p>
<p>###</p>
]]></content>
      <tags>
        <tag>CMake</tag>
      </tags>
  </entry>
  <entry>
    <title>利用反向传播训练多层神经网络的原理</title>
    <url>/2020/04/01/%E5%88%A9%E7%94%A8%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%AD%E7%BB%83%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8E%9F%E7%90%86/</url>
    <content><![CDATA[<p>文本详细描述了使用<em>后向传播</em>算法训练神经网络的过程。如下图所示，我们使用一个三层神经网络来描述这个过程，它有两个输入，一个输出：</p>
<p><img src="/" alt="img01" class="lazyload" data-src="/2020/04/01/%E5%88%A9%E7%94%A8%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%AD%E7%BB%83%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8E%9F%E7%90%86/img01.gif"></p>
<p>每个神经元由两部分组成，第一部分将输入信号与权重分别相乘然后求和。第二部分使用一个非线性函数（激活函数）对第一部分产生的信号进行非线性变换。信号$e$是第一部分（加法器）的输出信号，$y=f(e)$是第二部分（非线性变换）的输出信号。信号$y$也是整个神经元的输出。</p>
<p><img src="/" alt="img01b" class="lazyload" data-src="/2020/04/01/%E5%88%A9%E7%94%A8%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%AD%E7%BB%83%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8E%9F%E7%90%86/img01b.gif"></p>
<p>想要训练神经网络，我们首先需要数据集。每个训练数据包含输入信号($x_1$和$x_2$)以及对应的标签(期望的输出$z$)。神经网络的训练需要迭代进行。在每次迭代中，结点的权重系数($w_1$和$w_2$)都根据新的训练数据被更新。更新的数值大小使用以下算法确定：每个训练步骤都是从两个输入信号开始的。在这之后，可以根据输入信号确定每一层结点的输出信号值。下方图片展示了信号是如何在神经网络中传播，符号$w_{(xm)n}$表示在输入层中，输入$x_m$与神经元$n$之间的权重连接。符号$y_n$表示神经元$n$的输出信号。</p>
<p><img src="/" alt="img02" class="lazyload" data-src="/2020/04/01/%E5%88%A9%E7%94%A8%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%AD%E7%BB%83%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8E%9F%E7%90%86/img02.gif"></p>
<p><img src="/" alt="img03" class="lazyload" data-src="/2020/04/01/%E5%88%A9%E7%94%A8%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%AD%E7%BB%83%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8E%9F%E7%90%86/img03.gif"></p>
<p><img src="/" alt="img04" class="lazyload" data-src="/2020/04/01/%E5%88%A9%E7%94%A8%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%AD%E7%BB%83%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8E%9F%E7%90%86/img04.gif"></p>
<p>下图展示信号在隐藏层中传播。符号$w_{mn}$代表神经元$m$的输出与下一层中的神经元$n$的输入间的连接权重。</p>
<p><img src="/" alt="img05" class="lazyload" data-src="/2020/04/01/%E5%88%A9%E7%94%A8%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%AD%E7%BB%83%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8E%9F%E7%90%86/img05.gif"></p>
<p><img src="/" alt="img05" class="lazyload" data-src="/2020/04/01/%E5%88%A9%E7%94%A8%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%AD%E7%BB%83%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8E%9F%E7%90%86/img06.gif"></p>
<p>下图展示信号在输出层中传播。</p>
<p><img src="/" alt="img07" class="lazyload" data-src="/2020/04/01/%E5%88%A9%E7%94%A8%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%AD%E7%BB%83%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8E%9F%E7%90%86/img07.gif"></p>
<p>在算法的下一个步骤中，网络的输出信号$y$被拿来与期望的输出值$z$进行比较，$z$就是前文中所述的，由数据集提供的标签。差值被称作是输出层神经元的误差信号$\delta$。</p>
<p><img src="/" alt="img08" class="lazyload" data-src="/2020/04/01/%E5%88%A9%E7%94%A8%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%AD%E7%BB%83%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8E%9F%E7%90%86/img08.gif"></p>
<p>我们并不可能直接计算内层神经元的误差信号，因为这些神经元的输出值是未知的。多年来，人们都不知道如何有效训练多层神经网络。直到上世纪80年代中期，后向传播算法才被人们发现。该算法的思想是将误差信号$\delta$(单步训练中得出)往后传播回所有神经元，将输出信号作为他们的输入。</p>
<p><img src="/" alt="img09" class="lazyload" data-src="/2020/04/01/%E5%88%A9%E7%94%A8%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%AD%E7%BB%83%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8E%9F%E7%90%86/img09.gif"></p>
<p><img src="/" alt="img09" class="lazyload" data-src="/2020/04/01/%E5%88%A9%E7%94%A8%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%AD%E7%BB%83%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8E%9F%E7%90%86/img10.gif"></p>
<p>用来计算误差的权重系数$w_{mn}$和训练时得出的那个一致。这边仅仅是数据流被改变(信号被一个接一个地从输出方向传播到输入方向)。这个方法在所有网络层上应用。当误差来自多个神经元时，总误差是这些误差的和。示意图如下：</p>
<p><img src="/" alt="img11" class="lazyload" data-src="/2020/04/01/%E5%88%A9%E7%94%A8%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%AD%E7%BB%83%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8E%9F%E7%90%86/img11.gif"></p>
<p><img src="/" alt="img12" class="lazyload" data-src="/2020/04/01/%E5%88%A9%E7%94%A8%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%AD%E7%BB%83%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8E%9F%E7%90%86/img12.gif"></p>
<p><img src="/" alt="img11" class="lazyload" data-src="/2020/04/01/%E5%88%A9%E7%94%A8%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%AD%E7%BB%83%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8E%9F%E7%90%86/img13.gif"></p>
<p>当误差信号计算完毕时，我们使用计算出的误差信号更新网络权重。在下方公式中，${df_1(e)\over{de}}$代表激活函数的导数，$w’$代表$w$更新后的值。</p>
<p><img src="/" alt="img14" class="lazyload" data-src="/2020/04/01/%E5%88%A9%E7%94%A8%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%AD%E7%BB%83%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8E%9F%E7%90%86/img14.gif"></p>
<p><img src="/" alt="img15" class="lazyload" data-src="/2020/04/01/%E5%88%A9%E7%94%A8%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%AD%E7%BB%83%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8E%9F%E7%90%86/img15.gif"></p>
<p><img src="/" alt="img16" class="lazyload" data-src="/2020/04/01/%E5%88%A9%E7%94%A8%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%AD%E7%BB%83%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8E%9F%E7%90%86/img16.gif"></p>
<p><img src="/" alt="img17" class="lazyload" data-src="/2020/04/01/%E5%88%A9%E7%94%A8%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%AD%E7%BB%83%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8E%9F%E7%90%86/img17.gif"></p>
<p><img src="/" alt="img18" class="lazyload" data-src="/2020/04/01/%E5%88%A9%E7%94%A8%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%AD%E7%BB%83%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8E%9F%E7%90%86/img18.gif"></p>
<p><img src="/" alt="img19" class="lazyload" data-src="/2020/04/01/%E5%88%A9%E7%94%A8%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%AD%E7%BB%83%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8E%9F%E7%90%86/img19.gif"></p>
<p>系数$\eta$影响网络的学习速率。对于选择合适的学习速率，有几种技巧可以使用。第一种技巧是一开始使用一个较大的学习速率，然后渐渐递减。第二种方法更为复杂，一开始选用一个小的系数，随着训练的进行逐渐升大，然后在最后阶段再次减小。以低速率开始有利于确定权重的符号。</p>
<p>原文链接:<a href="http://galaxy.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html" target="_blank" rel="noopener">http://galaxy.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html</a></p>
]]></content>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习之反向传播算法 Part3</title>
    <url>/2020/03/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%20Part3/</url>
    <content><![CDATA[<p>在看反向传播的过程中，我们先intuitice地看一遍过程，然后再看具体的公式。</p>
<h2 id="Intuition"><a href="#Intuition" class="headerlink" title="Intuition"></a>Intuition</h2><p>后向传播就是衡量前层对后层的影响使得梯度下降时，能够根据影响来调整权重大小。</p>
<p><img src="/" alt="1" class="lazyload" data-src="/2020/03/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%20Part3/1.png"></p>
<p>首先让我们看一下损失函数，通过Part2中的损失函数，我们可以计算出每个输出结点的误差。现在，让我们只看一个结点的误差情况。如下图所示，通过将该结点的loss和上一层的每个结点到该结点的权重相乘，可以得出每个权重需要修正的方向及大小。loss大小代表误差大小，更新前的weight大小代表当前情况下上一层每个结点对输出层中的这个结点的影响大小，而正负则决定了权重更新的方向。</p>
<p>![2](/Users/butyuhao/OneDrive - smail.shnu.edu.cn/blog/source/_posts/深度学习之反向传播算法 Part3/2.png)</p>
<p>然而，上面只是考虑到一个输出结点的情况。因为我们的输出层上有十个结点，因此我们需要考虑他们各自的更新需求，然后将十个结点的更新需求求和取平均就是当前输入的这张图片在这次更新中最终需要更新的数值大小。</p>
<p><img src="/" alt="3" class="lazyload" data-src="/2020/03/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%20Part3/3.png"></p>
<p>数据集中并不是只有当前输入的一张图片，还应该计算每一张图片的更新需求，如下图所示，将数据集中每张图片对权重的更新需求取平均作为整个数据集在这一轮更新中对权重的更新大小。</p>
<p><img src="/" alt="4" class="lazyload" data-src="/2020/03/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%20Part3/4.png"></p>
<h2 id="具体公式"><a href="#具体公式" class="headerlink" title="具体公式"></a>具体公式</h2><p>假设每一层都只有一个结点，那么让我们来看第L层（图中的L层是输出层）和第L-1层的情况。右上角中，是Cost、激活前的数值和激活后的数值，$z()$是激活函数。左下角的图片表示他们的关系。此时我们关心，$w(L)$变动会在多大程度上影响$C_0$。</p>
<p><img src="/" alt="7" class="lazyload" data-src="/2020/03/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%20Part3/7.png"></p>
<p>其实$C_0$对$w(L)$的偏导数表示的就是$w(L)$的变动会在多大程度上影响$C_0$。如下图所示，我们可以根据链式求导法则，将其进行变换。</p>
<p><img src="/" alt="6" class="lazyload" data-src="/2020/03/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%20Part3/6.png"></p>
<p>稍微变换上述公式，我们就可以得到bias对$C_0$的影响：</p>
<p><img src="/" alt="8" class="lazyload" data-src="/2020/03/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%20Part3/8.png"></p>
<p>最后让我们来看一下梯度公式：</p>
<p><img src="/" alt="9" class="lazyload" data-src="/2020/03/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%20Part3/9.png"></p>
<p>视频地址：<a href="https://www.bilibili.com/video/BV16x411V7Qg?p=2" target="_blank" rel="noopener">https://www.bilibili.com/video/BV16x411V7Qg?p=2</a></p>
<p>事实上，该视频较为适合在微观层面上理解后向传播机制，如果想要有更加全面而整体的把握，非常推荐阅读这篇文章：<a href="http://galaxy.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html" target="_blank" rel="noopener">http://galaxy.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html</a>  </p>
<p>当然，我也提供了翻译版本：利用反向传播训练多层神经网络的原理。</p>
]]></content>
      <tags>
        <tag>深度学习</tag>
        <tag>3Blue1Brown</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习之梯度下降法 Part2</title>
    <url>/2020/03/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%20Part2/</url>
    <content><![CDATA[<p>在Part1中，我们定义了一个用于识别手写数字的神经网络，而现在，我们需要看一看如何训练其中的weight，使得网络可以最好地对这些图片进行分类。</p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>首先我们需要一个指标来衡量当前网络中的weight能够多准确地完成数字图片的分类。损失函数做的就是这样一件事情：</p>
<p>$Cost(w) = $权重多烂</p>
<p>$w$是咱们网络中的所有权重数值，将$w$输入损失函数，它输出一个数值告诉我们，当前的权重到底有多烂。那么损失函数是怎样被implement的呢？让我们来康康：</p>
<p><img src="/" alt="1" class="lazyload" data-src="/2020/03/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%20Part2/1.png"></p>
<p>如上图所示，我们可以通过计算每个结点输出的结果与其应该有的正确值的平方和的总和来得知当前权重的糟糕程度。</p>
<h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>然鹅，知道了糟糕程度，我们如何据此调整权重，让结果不那么糟糕呢？这时候就要用到梯度下降。</p>
<p><img src="/" alt="2" class="lazyload" data-src="/2020/03/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%20Part2/2.png"></p>
<p>之所以叫梯度下降，是因为这个方法用到了梯度。我们知道，某个函数的某一点的梯度方向事实上是这个函数在这一点的数值上升得最快的方向。利用这一性质，我们计算损失函数的梯度，然后往反方向走，就是使得损失函数下降最快的方向。而梯度下降，事实上就是不停地重复以下过程：</p>
<ul>
<li>计算损失函数在某一点的梯度</li>
<li>向梯度相反的方向更新权重</li>
</ul>
<p><img src="/" alt="3" class="lazyload" data-src="/2020/03/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%20Part2/3.png"><br>事实上，梯度的正负，告诉我们往哪个方向走可以使得我们获得更小的cost，而梯度的大小则告诉我们每一项对于梯度大小变化的影响程度大小。比如某一项的梯度大，那么更新它相比于更新别人更能使得计算出的损失值发生变化。</p>
<h2 id="额外资料"><a href="#额外资料" class="headerlink" title="额外资料"></a>额外资料</h2><p>在关心机器怎么学习之前，咱们先来看看我们自己应该怎么学，这个网页中对训练一个数字图片分类器进行了详细的介绍：<a href="http://neuralnetworksanddeeplearning.com/chap1.html。" target="_blank" rel="noopener">http://neuralnetworksanddeeplearning.com/chap1.html。</a></p>
<p>视频地址：<a href="https://www.bilibili.com/video/BV1Ux411j7ri" target="_blank" rel="noopener">https://www.bilibili.com/video/BV1Ux411j7ri</a></p>
]]></content>
      <tags>
        <tag>深度学习</tag>
        <tag>3Blue1Brown</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习之神经网络的结构 Part1</title>
    <url>/2020/03/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%93%E6%9E%84%20Part1/</url>
    <content><![CDATA[<h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p>假设现在我们要训练一个神经网络来识别手写数字图片。如下图所示，我们使用最为简单的多层感知机(MLP)训练，定义一个输入层，两个隐藏层和一个输出层。我们将28x28的图片“拉直”，因此输入层有784个结点。而输出层的是个结点代表是十个数字中某个数字的可能性。</p>
<p><img src="/" alt="Screen Shot 2020-03-31 at 12.07.48 PM" class="lazyload" data-src="/2020/03/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%93%E6%9E%84%20Part1/1.png"></p>
<h2 id="网络输入"><a href="#网络输入" class="headerlink" title="网络输入"></a>网络输入</h2><p><img src="/" alt="Screen Shot 2020-03-31 at 12.08.41 PM" class="lazyload" data-src="/2020/03/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%93%E6%9E%84%20Part1/2.png"></p>
<p>输入值是图片中某个像素的灰度，取值范围在[0,1]，0代表全黑，1代表全白，输入某个结点的数值叫做激活值（Activation）。</p>
<h2 id="为什么我们需要隐藏层？"><a href="#为什么我们需要隐藏层？" class="headerlink" title="为什么我们需要隐藏层？"></a>为什么我们需要隐藏层？</h2><p>那么为什么我们需要搞很多层而不是就只有一个隐藏层呢？</p>
<p><img src="/" alt="Screen Shot 2020-03-31 at 12.10.56 PM" class="lazyload" data-src="/2020/03/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%93%E6%9E%84%20Part1/3.png"></p>
<p><img src="/" alt="Screen Shot 2020-03-31 at 12.11.20 PM" class="lazyload" data-src="/2020/03/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%93%E6%9E%84%20Part1/4.png">  </p>
<p>先来看一看这些数字，他们各自由一些笔画组成。</p>
<p><img src="/" alt="Screen Shot 2020-03-31 at 12.11.37 PM" class="lazyload" data-src="/2020/03/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%93%E6%9E%84%20Part1/5.png"></p>
<p>对于隐藏层中的结点来说，也许每个结点负责一种笔划的判断。而我们放置多个隐藏层，就可以组合判断多种笔划，达到识别数字的目的。</p>
<h2 id="权重是用来干什么的？"><a href="#权重是用来干什么的？" class="headerlink" title="权重是用来干什么的？"></a>权重是用来干什么的？</h2><p><img src="/" alt="Screen Shot 2020-03-31 at 12.13.10 PM" class="lazyload" data-src="/2020/03/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%93%E6%9E%84%20Part1/6.png"></p>
<p>权重其实决定了某个结点的值受上一层中各结点的影响程度。在这个例子中，就是某数字由各笔划组成的程度。</p>
<h2 id="激活函数的作用"><a href="#激活函数的作用" class="headerlink" title="激活函数的作用"></a>激活函数的作用</h2><p><img src="/" alt="Screen Shot 2020-03-31 at 12.15.27 PM" class="lazyload" data-src="/2020/03/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%93%E6%9E%84%20Part1/7.png"></p>
<p>这是一个Sigmoid函数，它是一个最为Basic的激活函数，将范围为负无穷到正无穷的输入映射到范围为(0,1)的输出。</p>
<h2 id="放在一起观察"><a href="#放在一起观察" class="headerlink" title="放在一起观察"></a>放在一起观察</h2><p><img src="/" alt="Screen Shot 2020-03-31 at 12.16.44 PM" class="lazyload" data-src="/2020/03/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%93%E6%9E%84%20Part1/8.png"></p>
<p>放在一起看，权重决定某神经元关注上一层的哪些神经元，而bias决定了加权和需要多大，才可以激活此神经元。而激活函数事实上衡量了加权和到底有多正。<img src="/" alt="Screen Shot 2020-03-31 at 12.18.37 PM" class="lazyload" data-src="/2020/03/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%93%E6%9E%84%20Part1/9.png"></p>
<p>需要注意的是，某层上某个结点的值是其上一层结点值的加权和，因此对于第二层的每个结点来说，其都有对应的784个权重，而每个结点又都有一个bias，因此对于第二层的所有结点来说，总共有784x16个weight以及16个bias。</p>
<h2 id="常用的激活函数"><a href="#常用的激活函数" class="headerlink" title="常用的激活函数"></a>常用的激活函数</h2><p><img src="/" alt="Screen Shot 2020-03-31 at 12.22.08 PM" class="lazyload" data-src="/2020/03/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%93%E6%9E%84%20Part1/10.png"></p>
<p>激活函数的任务事实上是模拟人脑中的神经元，在输入达到某种数值时，才向后输出。实验表明，Sigmoid函数的激活效果并不好。与其相比，Relu的激活</p>
<p>原视频链接：<a href="https://www.bilibili.com/video/BV1bx411M7Zx" target="_blank" rel="noopener">https://www.bilibili.com/video/BV1bx411M7Zx</a></p>
]]></content>
      <tags>
        <tag>深度学习</tag>
        <tag>3Blue1Brown</tag>
      </tags>
  </entry>
  <entry>
    <title>argparser简明教程</title>
    <url>/2020/03/25/argparser%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B/</url>
    <content><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>argparser是在使用Python编程时首选的命令行参数解析器。该库是Python语言自带的库，它自动决定如何从<code>sys.argv</code>中解析出各种命令行参数。根据用户指定的参数，argparser自动生成帮助信息，并能在用户传入无效参数时提示用户。</p>
<h2 id="创建解析器"><a href="#创建解析器" class="headerlink" title="创建解析器"></a>创建解析器</h2><p>在进一步指定解析器的各种参数前，我们需要首先实例化一个解析器,通过description选项，可以告诉用户这个程序是干什么的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser(description=<span class="string">'Process some integers.'</span>)</span><br></pre></td></tr></table></figure>



<h2 id="添加必选参数"><a href="#添加必选参数" class="headerlink" title="添加必选参数"></a>添加必选参数</h2><p>在我们编程时，有时需要在命令行中向Python程序传入一些参数，且这些参数是必选的，没有这些参数，程序将无法正常运行。</p>
<p>假设现在我们需要写一个程序，使其可以读入命令行中用户指定的一个数字，然后将该数字的平方打印出来。编写程序prog.py：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">"square"</span>, help=<span class="string">"display a square of a given number"</span>,</span><br><span class="line">                    type=int)</span><br><span class="line">args = parser.parse_args()</span><br><span class="line">print(args.square**<span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<p>在上方的程序中，我们通过<code>add_argument</code>向解析器中添加了一个必选参数<code>&quot;square&quot;</code>，且该参数的帮助信息为<code>help=&quot;display a square of a given number&quot;</code>，该参数的类型为<code>int</code>（如果不指定，则读取到的参数将被认为是字符串类型）。通过<code>args = parser.parse_args()</code>对传入的参数进行解析，通过<code>args.square</code>读取传入的<code>square</code>数值。</p>
<p>现在，让咱们运行一下上面写的那个程序：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ python3 prog.py 4 <span class="comment">#传入正确参数</span></span><br><span class="line">16</span><br><span class="line">$ python3 prog.py four <span class="comment">#传入错误参数，解析器报错</span></span><br><span class="line">usage: prog.py [-h] square</span><br><span class="line">prog.py: error: argument square: invalid int value: <span class="string">'four'</span></span><br></pre></td></tr></table></figure>

<p>可以看到，通过命令行运行该程序，并指定<code>square</code>为4，程序打印出了正确的结果16。</p>
<h2 id="添加可选参数"><a href="#添加可选参数" class="headerlink" title="添加可选参数"></a>添加可选参数</h2><p>在上一节中，我们已经学会如何向解析器添加必选参数，然鹅，有时候我们的参数并不是必选的，或者，它只是某种开关，那咱们咋办呢？。此时，我们可以通过添加可选参数来解决问题。</p>
<p>我们编写测试文件parser_test.py:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">"--瞎逼逼"</span>, help=<span class="string">"输出信息更加详细"</span>)</span><br><span class="line">args = parser.parse_args()</span><br><span class="line"><span class="keyword">if</span> args.瞎逼逼:</span><br><span class="line">    print(<span class="string">"瞎逼逼模式已被打开"</span>)</span><br></pre></td></tr></table></figure>

<p>运行该文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ python parse_test.py --瞎逼逼 1 <span class="comment">#传入参数，正常</span></span><br><span class="line">瞎逼逼模式已被打开</span><br><span class="line">$ python parser_test --瞎逼逼 <span class="comment">#没传入参数，报错</span></span><br><span class="line">python: can<span class="string">'t open file '</span>parser_test<span class="string">': [Errno 2] No such file or directory</span></span><br></pre></td></tr></table></figure>

<h3 id="作为“开关”的可选参数"><a href="#作为“开关”的可选参数" class="headerlink" title="作为“开关”的可选参数"></a>作为“开关”的可选参数</h3><p>在上方的例子中，需要传入一个没有用的参数才能使其正常运行，但是咱们其实是想把<code>--瞎逼逼</code>作为一个开关，如果指定该选项，程序就以瞎逼逼模式运行，如果没指定就以正常模式运行，<code>瞎逼逼</code>这个参数其实是充当一个bool变量，不是<code>True</code>就是<code>False</code>。咱们可以把上面的程序改改，写成下面这样：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">"--瞎逼逼"</span>, help=<span class="string">"输出信息更加详细"</span>, action=<span class="string">"store_true"</span>)</span><br><span class="line">args = parser.parse_args()</span><br><span class="line"><span class="keyword">if</span> args.瞎逼逼:</span><br><span class="line">    print(<span class="string">"瞎逼逼模式已被打开"</span>)</span><br></pre></td></tr></table></figure>

<p>上方程序通过添加<code>action=&quot;store_true&quot;</code>将<code>瞎逼逼</code>这个参数指定成一个bool参数，运行该文件，我们得到一下结果：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ python parse_test.py --瞎逼逼</span><br><span class="line">瞎逼逼模式已被打开</span><br><span class="line">$ python parse_test.py --瞎逼逼 1</span><br><span class="line">usage: parse_test.py [-h] [--瞎逼逼]</span><br><span class="line">parse_test.py: error: unrecognized arguments: 1</span><br></pre></td></tr></table></figure>

<p>这样以来，<code>瞎逼逼</code>参数就变成了一个开关，而用户如果多传入无关参数就会报错。事实上，在使用<code>action=&quot;store_true&quot;</code>选项后，当用户传入<code>--瞎逼逼</code>选项时，<code>瞎逼逼</code>这个参数就为<code>True</code>，而如果没有传入这个选项，参数值为<code>False</code>。</p>
<h3 id="添加可选参数的缩写"><a href="#添加可选参数的缩写" class="headerlink" title="添加可选参数的缩写"></a>添加可选参数的缩写</h3><p>在上方🌰中，我们将<code>--瞎逼逼</code>作为可选参数，但有时候我们比较懒，不想要写“瞎逼逼”这三个字。此时，我们可以选择添加可选参数的缩写：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">"-瞎"</span>, <span class="string">"--瞎逼逼"</span>, help=<span class="string">"输出信息更加详细"</span>, action=<span class="string">"store_true"</span>)</span><br><span class="line">args = parser.parse_args()</span><br><span class="line"><span class="keyword">if</span> args.瞎逼逼:</span><br><span class="line">    print(<span class="string">"瞎逼逼模式已被打开"</span>)</span><br></pre></td></tr></table></figure>

<p>运行结果:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ python parse_test.py -瞎</span><br><span class="line">瞎逼逼模式已被打开</span><br></pre></td></tr></table></figure>

<p>从结果可以看出，通过添加”-瞎”，我们只写一个字就能打开瞎逼逼开关。</p>
<h2 id="结合可选参数与必选参数"><a href="#结合可选参数与必选参数" class="headerlink" title="结合可选参数与必选参数"></a>结合可选参数与必选参数</h2><p>现在咱们把“添加必选参数”部分的程序与“添加可选参数”部分的代码放到一起：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">"square"</span>, type=int,</span><br><span class="line">                    help=<span class="string">"display a square of a given number"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"-瞎"</span>, <span class="string">"--瞎逼逼"</span>, help=<span class="string">"输出信息更加详细"</span>, action=<span class="string">"store_true"</span>)</span><br><span class="line">args = parser.parse_args()</span><br><span class="line">answer = args.square**<span class="number">2</span></span><br><span class="line"><span class="keyword">if</span> args.瞎逼逼:</span><br><span class="line">    print(<span class="string">"the square of &#123;&#125; equals &#123;&#125;"</span>.format(args.square, answer))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(answer)</span><br></pre></td></tr></table></figure>

<p>运行结果:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ python parse_test.py</span><br><span class="line">usage: parse_test.py [-h] [-瞎] square</span><br><span class="line">parse_test.py: error: the following arguments are required: square</span><br><span class="line">$ python parse_test.py -瞎</span><br><span class="line">usage: parse_test.py [-h] [-瞎] square</span><br><span class="line">parse_test.py: error: the following arguments are required: square</span><br><span class="line">$ python parse_test.py -瞎 4</span><br><span class="line">the square of 4 equals 16</span><br></pre></td></tr></table></figure>

<h3 id="添加“副”开关"><a href="#添加“副”开关" class="headerlink" title="添加“副”开关"></a>添加“副”开关</h3><p>虽然咱们在上面添加了<code>--瞎逼逼</code>开关来决定程序输出信息的详细程度，但有时，我们添加的开关也需要参数。举个🌰，现在我们不仅需要指定程序是否要瞎逼逼，我们还要指定其瞎逼逼的程度，我们希望程序的瞎逼逼度为0、1或2，瞎逼逼程度随数字大小而增加。此时，我们可以这样写：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">"square"</span>, type=int,</span><br><span class="line">                    help=<span class="string">"display a square of a given number"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"-瞎"</span>, <span class="string">"--瞎逼逼"</span>, type=int, choices=[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">                    help=<span class="string">"increase output verbosity"</span>)</span><br><span class="line">args = parser.parse_args()</span><br><span class="line">answer = args.square**<span class="number">2</span></span><br><span class="line"><span class="keyword">if</span> args.瞎逼逼 == <span class="number">2</span>:</span><br><span class="line">    print(<span class="string">"the square of &#123;&#125; equals &#123;&#125;"</span>.format(args.square, answer))</span><br><span class="line"><span class="keyword">elif</span> args.瞎逼逼 == <span class="number">1</span>:</span><br><span class="line">    print(<span class="string">"&#123;&#125;^2 == &#123;&#125;"</span>.format(args.square, answer))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(answer)</span><br></pre></td></tr></table></figure>

<p>通过在添加参数时加入<code>choices=[0, 1, 2]</code>将瞎逼逼程度的传入参数限制在这三个数字以防用户传进来一些奇奇怪怪的东西，在下方通过if else来区分用户传入的不同瞎逼逼程度，并以相应程度输出结果。</p>
<p>结果:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ python parse_test.py 4 -瞎 3 <span class="comment">#不在choices中，因此报错。</span></span><br><span class="line">usage: parse_test.py [-h] [-瞎 &#123;0,1,2&#125;] square</span><br><span class="line">parse_test.py: error: argument -瞎/--瞎逼逼: invalid choice: 3 (choose from 0, 1, 2)</span><br><span class="line">$ python parse_test.py 4 -h</span><br><span class="line">usage: parse_test.py [-h] [-瞎 &#123;0,1,2&#125;] square</span><br><span class="line"></span><br><span class="line">positional arguments:</span><br><span class="line">  square                display a square of a given number</span><br><span class="line"></span><br><span class="line">optional arguments:</span><br><span class="line">  -h, --<span class="built_in">help</span>            show this <span class="built_in">help</span> message and <span class="built_in">exit</span></span><br><span class="line">  -瞎 &#123;0,1,2&#125;, --瞎逼逼 &#123;0,1,2&#125;</span><br><span class="line">                        increase output verbosity</span><br><span class="line">$ python parse_test.py 4 -瞎 1<span class="comment">#以一级瞎逼逼程度输出</span></span><br><span class="line">4^2 == 16</span><br><span class="line">(base) butyuhao@Yuhaos-MBP <span class="built_in">test</span> %</span><br></pre></td></tr></table></figure>

<p>试试上还有另一种指定瞎逼逼程度的方式，那就是用重复参数次数的方式：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">"square"</span>, type=int,</span><br><span class="line">                    help=<span class="string">"display a square of a given number"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"-瞎"</span>, <span class="string">"--瞎逼逼"</span>,action=<span class="string">"count"</span>,</span><br><span class="line">                    help=<span class="string">"increase output verbosity"</span>)</span><br><span class="line">args = parser.parse_args()</span><br><span class="line">answer = args.square**<span class="number">2</span></span><br><span class="line"><span class="keyword">if</span> args.瞎逼逼 &gt;= <span class="number">2</span>:</span><br><span class="line">    print(<span class="string">"the square of &#123;&#125; equals &#123;&#125;"</span>.format(args.square, answer))</span><br><span class="line"><span class="keyword">elif</span> args.瞎逼逼 == <span class="number">1</span>:</span><br><span class="line">    print(<span class="string">"&#123;&#125;^2 == &#123;&#125;"</span>.format(args.square, answer))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(answer)</span><br></pre></td></tr></table></figure>

<p>通过添加<code>action=&quot;count&quot;</code>选项，解析器将存储该参数在传入时的重复次数，这样一来就可以通过重复次数来表示瞎逼逼程度。</p>
<p>输出：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ python parse_test.py -瞎 4</span><br><span class="line">4^2 == 16</span><br><span class="line">$ python parse_test.py -瞎瞎 4</span><br><span class="line">the square of 4 equals 16</span><br><span class="line">$ python parse_test.py -瞎瞎瞎 4</span><br><span class="line">the square of 4 equals 16</span><br></pre></td></tr></table></figure>

<p>在结果中可以看到，随着参数重复次数的改变，输出也作相应改变。</p>
<p>不过，你真的觉得这样就行了么？你还是太naive了，如果这时候用户不指定这个参数就会报错：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ python parse_test.py 4</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">"parse_test.py"</span>, line 9, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    <span class="keyword">if</span> args.瞎逼逼 &gt;= 2:</span><br><span class="line">TypeError: <span class="string">'&gt;='</span> not supported between instances of <span class="string">'NoneType'</span> and <span class="string">'int'</span></span><br></pre></td></tr></table></figure>

<p>如果用户不传入这个参数，则该参数取值将为None，而&gt;=操作符不可以比较一个None和一个数字，因此报错。我们可以通过为该参数指定默认值解决该问题，这样在用户不指定该参数时，该参数为默认值，通过添加default参数指定：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">"square"</span>, type=int,</span><br><span class="line">                    help=<span class="string">"display a square of a given number"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"-瞎"</span>, <span class="string">"--瞎逼逼"</span>,action=<span class="string">"count"</span>,</span><br><span class="line">                    default=<span class="number">0</span>, help=<span class="string">"increase output verbosity"</span>)</span><br><span class="line">args = parser.parse_args()</span><br><span class="line">answer = args.square**<span class="number">2</span></span><br><span class="line"><span class="keyword">if</span> args.瞎逼逼 &gt;= <span class="number">2</span>:</span><br><span class="line">    print(<span class="string">"the square of &#123;&#125; equals &#123;&#125;"</span>.format(args.square, answer))</span><br><span class="line"><span class="keyword">elif</span> args.瞎逼逼 == <span class="number">1</span>:</span><br><span class="line">    print(<span class="string">"&#123;&#125;^2 == &#123;&#125;"</span>.format(args.square, answer))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(answer)</span><br></pre></td></tr></table></figure>

<h2 id="互斥的参数"><a href="#互斥的参数" class="headerlink" title="互斥的参数"></a>互斥的参数</h2><p>有时，咱们需要让用户知道，某些参数间只能选一个。举个🌰，此时我们不仅有<code>--瞎逼逼</code>选项，还有<code>--安静</code>选项。很明显，你并不能既让程序瞎逼逼，又让程序安静，这很明显会让你的程序精神分裂。这时候我们需要将它们设置为互斥，这样用户只能从中选一个指定。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"></span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">group = parser.add_mutually_exclusive_group()</span><br><span class="line">group.add_argument(<span class="string">"-瞎"</span>, <span class="string">"--瞎逼逼"</span>, action=<span class="string">"store_true"</span>)</span><br><span class="line">group.add_argument(<span class="string">"-安"</span>, <span class="string">"--安静"</span>, action=<span class="string">"store_true"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"x"</span>, type=int, help=<span class="string">"the base"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"y"</span>, type=int, help=<span class="string">"the exponent"</span>)</span><br><span class="line">args = parser.parse_args()</span><br><span class="line">answer = args.x**args.y</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> args.安静:</span><br><span class="line">    print(answer)</span><br><span class="line"><span class="keyword">elif</span> args.瞎逼逼:</span><br><span class="line">    print(<span class="string">"&#123;&#125; to the power &#123;&#125; equals &#123;&#125;"</span>.format(args.x, args.y, answer))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">"&#123;&#125;^&#123;&#125; == &#123;&#125;"</span>.format(args.x, args.y, answer))</span><br></pre></td></tr></table></figure>

<p>我们通过<code>group = parser.add_mutually_exclusive_group()</code>创建了一个互斥组，然后将两个参数添加进去，这样，用户就只能选择其一指定。</p>
<p>结果：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ python parse_test.py</span><br><span class="line">usage: parse_test.py [-h] [-v | -q] x y</span><br><span class="line">parse_test.py: error: the following arguments are required: x, y</span><br><span class="line">$ python parse_test.py 4 2</span><br><span class="line">4^2 == 16</span><br><span class="line">$ python parse_test.py 4 2 -安</span><br><span class="line">16</span><br><span class="line">$ python parse_test.py 4 2 -瞎</span><br><span class="line">4 to the power 2 equals 16</span><br></pre></td></tr></table></figure>

<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>事实上，argparser模块有更多复杂有趣的功能，可以在<a href="https://docs.python.org/3/library/argparse.html#module-argparse" target="_blank" rel="noopener"><code>官方文档</code></a>中详细了解。本文是在<a href="https://docs.python.org/3/howto/argparse.html#id1" target="_blank" rel="noopener"><code>官方教程</code></a>的基础上改编而成，如果觉得写的不好，也请不要在网络上逼逼赖赖，有本事现实碰一碰，你看我扎不扎你就完了。</p>
]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch教学</title>
    <url>/2020/03/21/PyTorch%E6%95%99%E5%AD%A6/</url>
    <content><![CDATA[<h2 id="PyTorch-介绍"><a href="#PyTorch-介绍" class="headerlink" title="PyTorch 介绍"></a>PyTorch 介绍</h2><p>PyTorch是一款强有力的深度学习框架，不管是科研还是生产部署的需求它都能满足。<br>本教程仅提供粗略的介绍，如果有任何疑问可以上网查查或是去咨询朋友。（小声bb：多google少百度）<br>一些用得上的point：</p>
<ul>
<li><p>自动differentiation工具非常强大</p>
</li>
<li><p>PyTorch为你实现好了深度学习中的常用功能</p>
</li>
<li><p>使用PyTorch DataSet来处理数据</p>
</li>
</ul>
<h2 id="Tensor与numpy的关系"><a href="#Tensor与numpy的关系" class="headerlink" title="Tensor与numpy的关系"></a>Tensor与numpy的关系</h2><p>PyTorch的基本组成部分，block与numpy的ndarray相似。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># we create tensors in a similar way to numpy nd arrays</span></span><br><span class="line">x_numpy = np.array([<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>])</span><br><span class="line">x_torch = torch.tensor([<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>])</span><br><span class="line">print(<span class="string">'x_numpy, x_torch'</span>)</span><br><span class="line">print(x_numpy, x_torch)</span><br><span class="line">print()</span><br><span class="line"></span><br><span class="line"><span class="comment"># to and from numpy, pytorch</span></span><br><span class="line">print(<span class="string">'to and from numpy and pytorch'</span>)</span><br><span class="line">print(torch.from_numpy(x_numpy), x_torch.numpy())</span><br><span class="line">print()</span><br><span class="line"></span><br><span class="line"><span class="comment"># we can do basic operations like +-*/</span></span><br><span class="line">y_numpy = np.array([<span class="number">3</span>,<span class="number">4</span>,<span class="number">5.</span>])</span><br><span class="line">y_torch = torch.tensor([<span class="number">3</span>,<span class="number">4</span>,<span class="number">5.</span>])</span><br><span class="line">print(<span class="string">"x+y"</span>)</span><br><span class="line">print(x_numpy + y_numpy, x_torch + y_torch)</span><br><span class="line">print()</span><br><span class="line"></span><br><span class="line"><span class="comment"># many functions that are in numpy are also in pytorch</span></span><br><span class="line">print(<span class="string">"norm"</span>)</span><br><span class="line">print(np.linalg.norm(x_numpy), torch.norm(x_torch))</span><br><span class="line">print()</span><br><span class="line"></span><br><span class="line"><span class="comment"># to apply an operation along a dimension,</span></span><br><span class="line"><span class="comment"># we use the dim keyword argument instead of axis</span></span><br><span class="line">print(<span class="string">"mean along the 0th dimension"</span>)</span><br><span class="line">x_numpy = np.array([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4.</span>]])</span><br><span class="line">x_torch = torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4.</span>]])</span><br><span class="line">print(np.mean(x_numpy, axis=<span class="number">0</span>), torch.mean(x_torch, dim=<span class="number">0</span>))</span><br></pre></td></tr></table></figure>
<h2 id="Tensor-view"><a href="#Tensor-view" class="headerlink" title="Tensor.view"></a>Tensor.view</h2><p>和numpy.reshape()一样，我们可以使用tensor.view()来改变tensor的形状。当使用-1时，其自动计算剩下维度的值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># "MNIST"</span></span><br><span class="line">N, C, W, H = <span class="number">10000</span>, <span class="number">3</span>, <span class="number">28</span>, <span class="number">28</span></span><br><span class="line">X = torch.randn((N, C, W, H))</span><br><span class="line"></span><br><span class="line">print(X.shape)</span><br><span class="line">print(X.view(N, C, <span class="number">784</span>).shape)</span><br><span class="line">print(X.view(<span class="number">-1</span>, C, <span class="number">784</span>).shape) <span class="comment"># automatically choose the 0th dimension</span></span><br></pre></td></tr></table></figure>
<h2 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h2><p>PyTorch的tensor有一个特别的地方，当tensor对象时，Pytorch自动在后台创建“计算图”。计算图是使用图的方式来表达一个数学表达式。其带有一种算法来计算一个计算图中所有变量的梯度，计算顺序和函数本身的顺序一致。<br>假设表达式为$e=(a+b)*(b+1)$,其中，$a=2, b=1$我们可以画出如下的计算图：</p>
<p><img src="/" alt class="lazyload" data-src="/2020/03/21/PyTorch%E6%95%99%E5%AD%A6/tree-eval.png"></p>
<p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.tensor(<span class="number">2.0</span>, requires_grad=<span class="literal">True</span>) <span class="comment"># we set requires_grad=True to let PyTorch know to keep the graph</span></span><br><span class="line">b = torch.tensor(<span class="number">1.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">c = a + b</span><br><span class="line">d = b + <span class="number">1</span></span><br><span class="line">e = c * d</span><br><span class="line">print(<span class="string">'c'</span>, c)</span><br><span class="line">print(<span class="string">'d'</span>, d)</span><br><span class="line">print(<span class="string">'e'</span>, e)</span><br></pre></td></tr></table></figure>

<p>执行结果为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt;</span><br><span class="line">c tensor(<span class="number">3.</span>, grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">d tensor(<span class="number">2.</span>, grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">e tensor(<span class="number">6.</span>, grad_fn=&lt;MulBackward0&gt;)</span><br></pre></td></tr></table></figure>

<p>可以看到，PyTorch为我们记录了计算图。</p>
<h2 id="使用Pytorch来计算梯度"><a href="#使用Pytorch来计算梯度" class="headerlink" title="使用Pytorch来计算梯度"></a>使用Pytorch来计算梯度</h2><p>我们已经发现PyTorch会自动为我们记录计算图，现在，让我们用它来求一求梯度。</p>
<p>考虑这样一个方程：$f(x)=(x-2)^2$。</p>
<p>问题：计算${df(x)}\over{dx}$，求$f’(1)$。</p>
<p>我们对leaf variable <code>y</code>调用<code>backward()</code>方法，一次性计算<code>y</code>的所有梯度。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (x<span class="number">-2</span>)**<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fp</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span>*(x<span class="number">-2</span>)</span><br><span class="line"></span><br><span class="line">x = torch.tensor([<span class="number">1.0</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">y = f(x)</span><br><span class="line">y.backward()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Analytical f\'(x):'</span>, fp(x))</span><br><span class="line">print(<span class="string">'PyTorch\'s f\'(x):'</span>, x.grad)</span><br></pre></td></tr></table></figure>

<p>输出为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt;</span><br><span class="line">Analytical <span class="string">f'(x): tensor([-2.], grad_fn=&lt;MulBackward0&gt;) #带有grad_fn</span></span><br><span class="line"><span class="string">PyTorch'</span>s <span class="string">f'(x): tensor([-2.])</span></span><br></pre></td></tr></table></figure>

<p><code>backward()</code>也可以计算带有数学函数的表达式的梯度：</p>
<p>现有$w=[w_1,w_2]^T$，</p>
<p>$g(w)=2w_1w_2+w_2cos(w_1)$</p>
<p>问题：计算&nabla;$_w g(w)$然后验证&nabla;$_w g([\pi,1]) = [2,\pi-1]^T$。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">g</span><span class="params">(w)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span>*w[<span class="number">0</span>]*w[<span class="number">1</span>] + w[<span class="number">1</span>]*torch.cos(w[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grad_g</span><span class="params">(w)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> torch.tensor([<span class="number">2</span>*w[<span class="number">1</span>] - w[<span class="number">1</span>]*torch.sin(w[<span class="number">0</span>]), <span class="number">2</span>*w[<span class="number">0</span>] + torch.cos(w[<span class="number">0</span>])])</span><br><span class="line"></span><br><span class="line">w = torch.tensor([np.pi, <span class="number">1</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">z = g(w)</span><br><span class="line">z.backward()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Analytical grad g(w)'</span>, grad_g(w))</span><br><span class="line">print(<span class="string">'PyTorch\'s grad g(w)'</span>, w.grad)</span><br></pre></td></tr></table></figure>

<p>输出如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Analytical grad g(w) tensor([<span class="number">2.0000</span>, <span class="number">5.2832</span>])</span><br><span class="line">PyTorch<span class="string">'s grad g(w) tensor([2.0000, 5.2832])</span></span><br></pre></td></tr></table></figure>

<h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>有了梯度，咱们就可以试试看梯度下降：</p>
<p>假设$f$就是上部分定义的函数</p>
<p>问题：找出使得$f$最小的$x$。</p>
<p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">5.0</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">step_size = <span class="number">0.25</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'iter,\tx,\tf(x),\tf\'(x),\tf\'(x) pytorch'</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">15</span>):</span><br><span class="line">    y = f(x)</span><br><span class="line">    y.backward() <span class="comment"># compute the gradient</span></span><br><span class="line">    </span><br><span class="line">    print(<span class="string">'&#123;&#125;,\t&#123;:.3f&#125;,\t&#123;:.3f&#125;,\t&#123;:.3f&#125;,\t&#123;:.3f&#125;'</span>.format(i, x.item(), f(x).item(), fp(x).item(), x.grad.item()))</span><br><span class="line">    </span><br><span class="line">    x.data = x.data - step_size * x.grad <span class="comment"># perform a GD update step</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># We need to zero the grad variable since the backward()</span></span><br><span class="line">    <span class="comment"># call accumulates the gradients in .grad instead of overwriting.</span></span><br><span class="line">    <span class="comment"># The detach_() is for efficiency. You do not need to worry too much about it.</span></span><br><span class="line">    x.grad.detach_()</span><br><span class="line">    x.grad.zero_()</span><br></pre></td></tr></table></figure>

<p>结果为：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">iter,	x,	f(x),	f<span class="string">'(x),	f'</span>(x) pytorch</span><br><span class="line">0,	5.000,	9.000,	6.000,	6.000</span><br><span class="line">1,	3.500,	2.250,	3.000,	3.000</span><br><span class="line">2,	2.750,	0.562,	1.500,	1.500</span><br><span class="line">3,	2.375,	0.141,	0.750,	0.750</span><br><span class="line">4,	2.188,	0.035,	0.375,	0.375</span><br><span class="line">5,	2.094,	0.009,	0.188,	0.188</span><br><span class="line">6,	2.047,	0.002,	0.094,	0.094</span><br><span class="line">7,	2.023,	0.001,	0.047,	0.047</span><br><span class="line">8,	2.012,	0.000,	0.023,	0.023</span><br><span class="line">9,	2.006,	0.000,	0.012,	0.012</span><br><span class="line">10,	2.003,	0.000,	0.006,	0.006</span><br><span class="line">11,	2.001,	0.000,	0.003,	0.003</span><br><span class="line">12,	2.001,	0.000,	0.001,	0.001</span><br><span class="line">13,	2.000,	0.000,	0.001,	0.001</span><br><span class="line">14,	2.000,	0.000,	0.000,	0.000</span><br></pre></td></tr></table></figure>

<h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><p>刚刚咱们最小化了一个瞎掰出来的函数，现在让我们最小化一个loss函数，并放入一些瞎掰的data。</p>
<p>现在咱用梯度下降解决一下线性回归问题：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># make a simple linear dataset with some noise</span></span><br><span class="line"></span><br><span class="line">d = <span class="number">2</span></span><br><span class="line">n = <span class="number">50</span></span><br><span class="line">X = torch.randn(n,d)</span><br><span class="line">true_w = torch.tensor([[<span class="number">-1.0</span>], [<span class="number">2.0</span>]])</span><br><span class="line">y = X @ true_w + torch.randn(n,<span class="number">1</span>) * <span class="number">0.1</span> <span class="comment"># @是矩阵乘法，*是元素逐个相乘。</span></span><br><span class="line">print(<span class="string">'X shape'</span>, X.shape)</span><br><span class="line">print(<span class="string">'y shape'</span>, y.shape)</span><br><span class="line">print(<span class="string">'w shape'</span>, true_w.shape)</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X shape torch.Size([<span class="number">50</span>, <span class="number">2</span>])</span><br><span class="line">y shape torch.Size([<span class="number">50</span>, <span class="number">1</span>])</span><br><span class="line">w shape torch.Size([<span class="number">2</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<h2 id="验证梯度计算的正确性"><a href="#验证梯度计算的正确性" class="headerlink" title="验证梯度计算的正确性"></a>验证梯度计算的正确性</h2><p>本部分我们验证PyTorch梯度计算的正确性。</p>
<p>Loss的梯度公式如下：</p>
<p>$\nabla _w \frac{1}{n}||y-Xw||_2^2=-\frac{2}{n}X^T(y-Xw)$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># define a linear model with no bias</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X, w)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> X @ w</span><br><span class="line"></span><br><span class="line"><span class="comment"># the residual sum of squares loss function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rss</span><span class="params">(y, y_hat)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> torch.norm(y - y_hat)**<span class="number">2</span> / n <span class="comment">#torch.norm是范数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># analytical expression for the gradient</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grad_rss</span><span class="params">(X, y, w)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">-2</span>*X.t() @ (y - X @ w) / n</span><br><span class="line"></span><br><span class="line">w = torch.tensor([[<span class="number">1.</span>], [<span class="number">0</span>]], requires_grad=<span class="literal">True</span>)</span><br><span class="line">y_hat = model(X, w)</span><br><span class="line"></span><br><span class="line">loss = rss(y, y_hat)</span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Analytical gradient'</span>, grad_rss(X, y, w).detach().view(<span class="number">2</span>).numpy())</span><br><span class="line">print(<span class="string">'PyTorch\'s gradient'</span>, w.grad.view(<span class="number">2</span>).numpy())</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Analytical gradient [ <span class="number">3.974099</span> <span class="number">-4.494481</span>]</span><br><span class="line">PyTorch<span class="string">'s gradient [ 3.9740987 -4.4944816]</span></span><br></pre></td></tr></table></figure>

<p>从结果中，我们可以看出PyTorch计算出了正确的梯度，现在让我们把梯度用起来吧。</p>
<h2 id="使用梯度下降算法，基于自动求导功能的线性回归"><a href="#使用梯度下降算法，基于自动求导功能的线性回归" class="headerlink" title="使用梯度下降算法，基于自动求导功能的线性回归"></a>使用梯度下降算法，基于自动求导功能的线性回归</h2><p>现在咱们用得出的梯度来实现梯度下降算法。</p>
<p>Note:这个例子只是单纯展示如何用PyTorch将之前的想法实现出来，后面还会介绍如何用PyTorchic的方式来做同样的事情。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">step_size = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'iter,\tloss,\tw'</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">    y_hat = model(X, w)</span><br><span class="line">    loss = rss(y, y_hat)</span><br><span class="line">    </span><br><span class="line">    loss.backward() <span class="comment"># compute the gradient of the loss</span></span><br><span class="line">    </span><br><span class="line">    w.data = w.data - step_size * w.grad <span class="comment"># do a gradient descent step</span></span><br><span class="line">    </span><br><span class="line">    print(<span class="string">'&#123;&#125;,\t&#123;:.2f&#125;,\t&#123;&#125;'</span>.format(i, loss.item(), w.view(<span class="number">2</span>).detach().numpy()))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># We need to zero the grad variable since the backward()</span></span><br><span class="line">    <span class="comment"># call accumulates the gradients in .grad instead of overwriting.</span></span><br><span class="line">    <span class="comment"># The detach_() is for efficiency. You do not need to worry too much about it.</span></span><br><span class="line">    w.grad.detach()</span><br><span class="line">    w.grad.zero_()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'\ntrue w\t\t'</span>, true_w.view(<span class="number">2</span>).numpy())</span><br><span class="line">print(<span class="string">'estimated w\t'</span>, w.view(<span class="number">2</span>).detach().numpy())</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">iter,	loss,	w</span><br><span class="line"><span class="number">0</span>,	<span class="number">8.47</span>,	[<span class="number">0.20518023</span> <span class="number">0.89889634</span>]</span><br><span class="line"><span class="number">1</span>,	<span class="number">2.80</span>,	[<span class="number">-0.03049211</span>  <span class="number">1.1496693</span> ]</span><br><span class="line"><span class="number">2</span>,	<span class="number">1.75</span>,	[<span class="number">-0.21864393</span>  <span class="number">1.3446302</span> ]</span><br><span class="line"><span class="number">3</span>,	<span class="number">1.09</span>,	[<span class="number">-0.3690024</span>  <span class="number">1.4960643</span>]</span><br><span class="line"><span class="number">4</span>,	<span class="number">0.68</span>,	[<span class="number">-0.4892798</span>  <span class="number">1.6135726</span>]</span><br><span class="line"><span class="number">5</span>,	<span class="number">0.43</span>,	[<span class="number">-0.5855947</span>  <span class="number">1.7046558</span>]</span><br><span class="line"><span class="number">6</span>,	<span class="number">0.27</span>,	[<span class="number">-0.66280454</span>  <span class="number">1.7751708</span> ]</span><br><span class="line"><span class="number">7</span>,	<span class="number">0.18</span>,	[<span class="number">-0.7247683</span>  <span class="number">1.8296891</span>]</span><br><span class="line"><span class="number">8</span>,	<span class="number">0.11</span>,	[<span class="number">-0.7745538</span>  <span class="number">1.8717768</span>]</span><br><span class="line"><span class="number">9</span>,	<span class="number">0.08</span>,	[<span class="number">-0.8146021</span>  <span class="number">1.9042141</span>]</span><br><span class="line"><span class="number">10</span>,	<span class="number">0.05</span>,	[<span class="number">-0.84685683</span>  <span class="number">1.9291675</span> ]</span><br><span class="line"><span class="number">11</span>,	<span class="number">0.04</span>,	[<span class="number">-0.87286717</span>  <span class="number">1.9483235</span> ]</span><br><span class="line"><span class="number">12</span>,	<span class="number">0.03</span>,	[<span class="number">-0.89386874</span>  <span class="number">1.9629945</span> ]</span><br><span class="line"><span class="number">13</span>,	<span class="number">0.02</span>,	[<span class="number">-0.91084814</span>  <span class="number">1.9742005</span> ]</span><br><span class="line"><span class="number">14</span>,	<span class="number">0.02</span>,	[<span class="number">-0.9245938</span>  <span class="number">1.982734</span> ]</span><br><span class="line"><span class="number">15</span>,	<span class="number">0.02</span>,	[<span class="number">-0.93573654</span>  <span class="number">1.9892098</span> ]</span><br><span class="line"><span class="number">16</span>,	<span class="number">0.01</span>,	[<span class="number">-0.9447814</span>  <span class="number">1.9941043</span>]</span><br><span class="line"><span class="number">17</span>,	<span class="number">0.01</span>,	[<span class="number">-0.9521335</span>  <span class="number">1.9977864</span>]</span><br><span class="line"><span class="number">18</span>,	<span class="number">0.01</span>,	[<span class="number">-0.9581177</span>  <span class="number">2.0005412</span>]</span><br><span class="line"><span class="number">19</span>,	<span class="number">0.01</span>,	[<span class="number">-0.96299535</span>  <span class="number">2.002589</span>  ]</span><br><span class="line"></span><br><span class="line">true w		 [<span class="number">-1.</span>  <span class="number">2.</span>]</span><br><span class="line">estimated w	 [<span class="number">-0.96299535</span>  <span class="number">2.002589</span>  ]</span><br></pre></td></tr></table></figure>

<h2 id="torch-nn-Module"><a href="#torch-nn-Module" class="headerlink" title="torch.nn.Module"></a>torch.nn.Module</h2><p><code>Module</code>是PyTorch对tensor施加操作的一种方式，各模块被作为torch.nn.Module的子类实现。所有模块都可以被调用，并可以被组合起来形成更加复杂的功能。</p>
<p><code>torch.nn</code><a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">docs</a></p>
<p>Note：多数为module实现的功能也可以通过<code>torch.nn.functional</code>来访问，但需要用户自己维护权重tensor。</p>
<p><code>torch.nn.functional</code><a href="https://pytorch.org/docs/stable/nn.html#torch-nn-functional" target="_blank" rel="noopener">docs</a></p>
<h2 id="Linear-Module"><a href="#Linear-Module" class="headerlink" title="Linear Module"></a>Linear Module</h2><p>Linear Module是一个常用模块，它很方便地实现带有一个bias的线性变换模型。你只需要指定输入结点的个数与输出节点的个数，它将自动为你生成中间权重与bias，创建出相应的线性模型。</p>
<p>和我们手动初始化$w$不同的是，Linear Module自动为我们初始化权重。对于最小化一个非凸的loss函数来说，权重的初始化是非常重要的。如果训练出来的模型没有想象中的好，可以尝试手动初始化权重，使其与默认的权重不同。<code>torch.nn.init</code>中有一些常用的参数初始化方式。</p>
<p><code>torch.nn.init</code><a href="https://pytorch.org/docs/stable/nn.html#torch-nn-init" target="_blank" rel="noopener">docs</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d_in = <span class="number">3</span></span><br><span class="line">d_out = <span class="number">4</span></span><br><span class="line">linear_module = nn.Linear(d_in, d_out)</span><br><span class="line"></span><br><span class="line">example_tensor = torch.tensor([[<span class="number">1.</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line"><span class="comment"># applys a linear transformation to the data</span></span><br><span class="line">transformed = linear_module(example_tensor)</span><br><span class="line">print(<span class="string">'example_tensor'</span>, example_tensor.shape)</span><br><span class="line">print(<span class="string">'transormed'</span>, transformed.shape)</span><br><span class="line">print()</span><br><span class="line">print(<span class="string">'We can see that the weights exist in the background\n'</span>)</span><br><span class="line">print(<span class="string">'W:'</span>, linear_module.weight)</span><br><span class="line">print(<span class="string">'b:'</span>, linear_module.bias)</span><br></pre></td></tr></table></figure>



<p>结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt;</span><br><span class="line">print(<span class="string">'We can see that the weights exist in the background\n'</span>)</span><br><span class="line">print(<span class="string">'W:'</span>, linear_module.weight)</span><br><span class="line">print(<span class="string">'b:'</span>, linear_module.bias)</span><br><span class="line">example_tensor torch.Size([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">transormed torch.Size([<span class="number">2</span>, <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">We can see that the weights exist <span class="keyword">in</span> the background</span><br><span class="line"></span><br><span class="line">W: Parameter containing:</span><br><span class="line">tensor([[ <span class="number">0.2383</span>, <span class="number">-0.0450</span>,  <span class="number">0.2986</span>],</span><br><span class="line">        [<span class="number">-0.0828</span>, <span class="number">-0.0900</span>,  <span class="number">0.2475</span>],</span><br><span class="line">        [<span class="number">-0.4174</span>,  <span class="number">0.3788</span>,  <span class="number">0.5005</span>],</span><br><span class="line">        [<span class="number">-0.3601</span>, <span class="number">-0.4104</span>, <span class="number">-0.0584</span>]], requires_grad=<span class="literal">True</span>)</span><br><span class="line">b: Parameter containing:</span><br><span class="line">tensor([<span class="number">-0.0385</span>, <span class="number">-0.0826</span>,  <span class="number">0.0033</span>,  <span class="number">0.4773</span>], requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>可以看到，咱们只是指定了输入和输出的维度，Linear Module就替我们把weight和bias全创建好了。</p>
<h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><p>PyTorch里头预先实现了一大票的激活函数，包括但不限于ReLU、Tanh和Sigmoid。因为他们都是模块，所以使用时需要先实例化。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">activation_fn = nn.ReLU() <span class="comment"># we instantiate an instance of the ReLU module</span></span><br><span class="line">example_tensor = torch.tensor([<span class="number">-1.0</span>, <span class="number">1.0</span>, <span class="number">0.0</span>])</span><br><span class="line">activated = activation_fn(example_tensor)</span><br><span class="line">print(<span class="string">'example_tensor'</span>, example_tensor)</span><br><span class="line">print(<span class="string">'activated'</span>, activated)</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">example_tensor tensor([<span class="number">-1.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>])</span><br><span class="line">activated tensor([<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>])</span><br></pre></td></tr></table></figure>

<h2 id="Sequential"><a href="#Sequential" class="headerlink" title="Sequential"></a>Sequential</h2><p>Sequential提供给咱们一个绝佳的解决方案，用于将多个简单模块组合在一起。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d_in = <span class="number">3</span></span><br><span class="line">d_hidden = <span class="number">4</span></span><br><span class="line">d_out = <span class="number">1</span></span><br><span class="line">model = torch.nn.Sequential(</span><br><span class="line">                            nn.Linear(d_in, d_hidden),</span><br><span class="line">                            nn.Tanh(),</span><br><span class="line">                            nn.Linear(d_hidden, d_out),</span><br><span class="line">                            nn.Sigmoid()</span><br><span class="line">                           )</span><br><span class="line"></span><br><span class="line">example_tensor = torch.tensor([[<span class="number">1.</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line">transformed = model(example_tensor)</span><br><span class="line">print(<span class="string">'transformed'</span>, transformed.shape)</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt;</span><br><span class="line">transformed torch.Size([<span class="number">2</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<p>小技巧：咱们可以使用<code>parameters()</code>方法来得到得到任何<code>nn.Module( )</code>的参数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">params = model.parameters()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">    print(param)</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Parameter containing:</span><br><span class="line">tensor([[<span class="number">-0.5554</span>,  <span class="number">0.0456</span>, <span class="number">-0.3115</span>],</span><br><span class="line">        [ <span class="number">0.0697</span>, <span class="number">-0.1629</span>,  <span class="number">0.3342</span>],</span><br><span class="line">        [ <span class="number">0.1340</span>, <span class="number">-0.1353</span>,  <span class="number">0.1261</span>],</span><br><span class="line">        [ <span class="number">0.0624</span>,  <span class="number">0.3285</span>, <span class="number">-0.4536</span>]], requires_grad=<span class="literal">True</span>)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([ <span class="number">0.3684</span>, <span class="number">-0.0760</span>, <span class="number">-0.2277</span>, <span class="number">-0.0276</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([[ <span class="number">0.0345</span>, <span class="number">-0.0294</span>, <span class="number">-0.1481</span>,  <span class="number">0.4977</span>]], requires_grad=<span class="literal">True</span>)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([<span class="number">0.1952</span>], requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>PyTorch中为我们预先实现好了很多loss函数，比方说<code>MSELoss</code>和<code>CrossEntropyLoss</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mse_loss_fn = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">input = torch.tensor([[<span class="number">0.</span>, <span class="number">0</span>, <span class="number">0</span>]])</span><br><span class="line">target = torch.tensor([[<span class="number">1.</span>, <span class="number">0</span>, <span class="number">-1</span>]])</span><br><span class="line"></span><br><span class="line">loss = mse_loss_fn(input, target)</span><br><span class="line"></span><br><span class="line">print(loss)</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor(<span class="number">0.6667</span>)</span><br></pre></td></tr></table></figure>

<h2 id="torch-optim"><a href="#torch-optim" class="headerlink" title="torch.optim"></a>torch.optim</h2><p>PyTorch实现了很多优化方法。在使用时，你最少也要指定模型参数和学习率。</p>
<p>优化器虽好，但它并不会自动帮你计算梯度。so，你自己要记得调用一下<code>backward()</code>奥对了，在运行这个之前还要记得调用一下<code>optim.zero_grad()&#39;初始化一下.grad里头的变量。这相当于对所有.grad里头的变量做</code>detach_()<code>和</code>zero_()`</p>
<p><code>torch.optim</code><a href="https://pytorch.org/docs/stable/optim.html" target="_blank" rel="noopener">docs</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># create a simple model</span></span><br><span class="line">model = nn.Linear(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create a simple dataset</span></span><br><span class="line">X_simple = torch.tensor([[<span class="number">1.</span>]])</span><br><span class="line">y_simple = torch.tensor([[<span class="number">2.</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># create our optimizer</span></span><br><span class="line">optim = torch.optim.SGD(model.parameters(), lr=<span class="number">1e-2</span>)</span><br><span class="line">mse_loss_fn = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">y_hat = model(X_simple)</span><br><span class="line">print(<span class="string">'model params before:'</span>, model.weight)</span><br><span class="line">loss = mse_loss_fn(y_hat, y_simple)</span><br><span class="line">optim.zero_grad()</span><br><span class="line">loss.backward()</span><br><span class="line">optim.step()</span><br><span class="line">print(<span class="string">'model params after:'</span>, model.weight)</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model params before: Parameter containing:</span><br><span class="line">tensor([[<span class="number">0.7107</span>]], requires_grad=<span class="literal">True</span>)</span><br><span class="line">model params after: Parameter containing:</span><br><span class="line">tensor([[<span class="number">0.7547</span>]], requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>可以看到，参数向正确的方向变化。</p>
<h2 id="用梯度下降、自动求导与PyTorch模块实现线性回归"><a href="#用梯度下降、自动求导与PyTorch模块实现线性回归" class="headerlink" title="用梯度下降、自动求导与PyTorch模块实现线性回归"></a>用梯度下降、自动求导与PyTorch模块实现线性回归</h2><p>现在，咱们把前面学的一堆东西揉在一起，用PyTorchic的方式实现线性回归。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">step_size = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line">linear_module = nn.Linear(d, <span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">loss_func = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">optim = torch.optim.SGD(linear_module.parameters(), lr=step_size)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'iter,\tloss,\tw'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">    y_hat = linear_module(X)</span><br><span class="line">    loss = loss_func(y_hat, y)</span><br><span class="line">    optim.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optim.step()</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">'&#123;&#125;,\t&#123;:.2f&#125;,\t&#123;&#125;'</span>.format(i, loss.item(), linear_module.weight.view(<span class="number">2</span>).detach().numpy()))</span><br><span class="line"></span><br><span class="line">print(<span class="string">'\ntrue w\t\t'</span>, true_w.view(<span class="number">2</span>).numpy())</span><br><span class="line">print(<span class="string">'estimated w\t'</span>, linear_module.weight.view(<span class="number">2</span>).detach().numpy())</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">iter,	loss,	w</span><br><span class="line"><span class="number">0</span>,	<span class="number">3.38</span>,	[<span class="number">-0.06555872</span>  <span class="number">0.9564365</span> ]</span><br><span class="line"><span class="number">1</span>,	<span class="number">2.09</span>,	[<span class="number">-0.25302264</span>  <span class="number">1.1884048</span> ]</span><br><span class="line"><span class="number">2</span>,	<span class="number">1.30</span>,	[<span class="number">-0.40178707</span>  <span class="number">1.3695917</span> ]</span><br><span class="line"><span class="number">3</span>,	<span class="number">0.81</span>,	[<span class="number">-0.5199211</span>  <span class="number">1.5110494</span>]</span><br><span class="line"><span class="number">4</span>,	<span class="number">0.50</span>,	[<span class="number">-0.61379874</span>  <span class="number">1.6214342</span> ]</span><br><span class="line"><span class="number">5</span>,	<span class="number">0.32</span>,	[<span class="number">-0.68845654</span>  <span class="number">1.7075248</span> ]</span><br><span class="line"><span class="number">6</span>,	<span class="number">0.20</span>,	[<span class="number">-0.74787635</span>  <span class="number">1.774628</span>  ]</span><br><span class="line"><span class="number">7</span>,	<span class="number">0.13</span>,	[<span class="number">-0.79520756</span>  <span class="number">1.8268977</span> ]</span><br><span class="line"><span class="number">8</span>,	<span class="number">0.08</span>,	[<span class="number">-0.83294225</span>  <span class="number">1.8675839</span> ]</span><br><span class="line"><span class="number">9</span>,	<span class="number">0.06</span>,	[<span class="number">-0.8630534</span>  <span class="number">1.8992288</span>]</span><br><span class="line"><span class="number">10</span>,	<span class="number">0.04</span>,	[<span class="number">-0.8871039</span>  <span class="number">1.9238206</span>]</span><br><span class="line"><span class="number">11</span>,	<span class="number">0.03</span>,	[<span class="number">-0.9063326</span>  <span class="number">1.9429132</span>]</span><br><span class="line"><span class="number">12</span>,	<span class="number">0.02</span>,	[<span class="number">-0.921722</span>   <span class="number">1.9577209</span>]</span><br><span class="line"><span class="number">13</span>,	<span class="number">0.02</span>,	[<span class="number">-0.9340517</span>  <span class="number">1.9691923</span>]</span><br><span class="line"><span class="number">14</span>,	<span class="number">0.02</span>,	[<span class="number">-0.9439409</span>  <span class="number">1.9780676</span>]</span><br><span class="line"><span class="number">15</span>,	<span class="number">0.01</span>,	[<span class="number">-0.95188165</span>  <span class="number">1.9849249</span> ]</span><br><span class="line"><span class="number">16</span>,	<span class="number">0.01</span>,	[<span class="number">-0.9582653</span>  <span class="number">1.9902146</span>]</span><br><span class="line"><span class="number">17</span>,	<span class="number">0.01</span>,	[<span class="number">-0.9634034</span>  <span class="number">1.9942878</span>]</span><br><span class="line"><span class="number">18</span>,	<span class="number">0.01</span>,	[<span class="number">-0.9675441</span>  <span class="number">1.9974183</span>]</span><br><span class="line"><span class="number">19</span>,	<span class="number">0.01</span>,	[<span class="number">-0.9708851</span>  <span class="number">1.9998189</span>]</span><br><span class="line"></span><br><span class="line">true w		 [<span class="number">-1.</span>  <span class="number">2.</span>]</span><br><span class="line">estimated w	 [<span class="number">-0.9708851</span>  <span class="number">1.9998189</span>]</span><br></pre></td></tr></table></figure>

<h2 id="使用SGD完成线性回归"><a href="#使用SGD完成线性回归" class="headerlink" title="使用SGD完成线性回归"></a>使用SGD完成线性回归</h2><p>在上一个例子中，咱们计算在整个数据集上的平均梯度(Gradient Decent)。我们可以通过简单修改实现随机梯度下降(Stochastic Gradient Descent)。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">step_size = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line">linear_module = nn.Linear(d, <span class="number">1</span>)</span><br><span class="line">loss_func = nn.MSELoss()</span><br><span class="line">optim = torch.optim.SGD(linear_module.parameters(), lr=step_size)</span><br><span class="line">print(<span class="string">'iter,\tloss,\tw'</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">200</span>):</span><br><span class="line">    rand_idx = np.random.choice(n) <span class="comment"># take a random point from the dataset</span></span><br><span class="line">    x = X[rand_idx] </span><br><span class="line">    y_hat = linear_module(x)</span><br><span class="line">    loss = loss_func(y_hat, y[rand_idx]) <span class="comment"># only compute the loss on the single point</span></span><br><span class="line">    optim.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optim.step()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'&#123;&#125;,\t&#123;:.2f&#125;,\t&#123;&#125;'</span>.format(i, loss.item(), linear_module.weight.view(<span class="number">2</span>).detach().numpy()))</span><br><span class="line"></span><br><span class="line">print(<span class="string">'\ntrue w\t\t'</span>, true_w.view(<span class="number">2</span>).numpy())</span><br><span class="line">print(<span class="string">'estimated w\t'</span>, linear_module.weight.view(<span class="number">2</span>).detach().numpy())</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">iter,	loss,	w</span><br><span class="line"><span class="number">0</span>,	<span class="number">17.04</span>,	[ <span class="number">0.5124521</span>  <span class="number">-0.06253883</span>]</span><br><span class="line"><span class="number">20</span>,	<span class="number">0.52</span>,	[<span class="number">0.05047676</span> <span class="number">0.4122801</span> ]</span><br><span class="line"><span class="number">40</span>,	<span class="number">0.10</span>,	[<span class="number">-0.29989475</span>  <span class="number">0.9859146</span> ]</span><br><span class="line"><span class="number">60</span>,	<span class="number">2.08</span>,	[<span class="number">-0.65285033</span>  <span class="number">1.3815426</span> ]</span><br><span class="line"><span class="number">80</span>,	<span class="number">0.07</span>,	[<span class="number">-0.787713</span>   <span class="number">1.4951732</span>]</span><br><span class="line"><span class="number">100</span>,	<span class="number">0.28</span>,	[<span class="number">-0.90760225</span>  <span class="number">1.683282</span>  ]</span><br><span class="line"><span class="number">120</span>,	<span class="number">0.01</span>,	[<span class="number">-0.92447585</span>  <span class="number">1.8040558</span> ]</span><br><span class="line"><span class="number">140</span>,	<span class="number">0.00</span>,	[<span class="number">-0.9492291</span>  <span class="number">1.8816731</span>]</span><br><span class="line"><span class="number">160</span>,	<span class="number">0.01</span>,	[<span class="number">-0.9679263</span>  <span class="number">1.9195584</span>]</span><br><span class="line"><span class="number">180</span>,	<span class="number">0.01</span>,	[<span class="number">-0.9705014</span>  <span class="number">1.9513198</span>]</span><br><span class="line"></span><br><span class="line">true w		 [<span class="number">-1.</span>  <span class="number">2.</span>]</span><br><span class="line">estimated w	 [<span class="number">-0.9696742</span>  <span class="number">1.9616756</span>]</span><br></pre></td></tr></table></figure>

<p>其中，咱们通过<code>np.random.choice(n)</code>随机选取一个数据用以更新而不是基于整个数据集更新。（这边相当于是针对data的stachastic）。</p>
<h2 id="CrossEntropyLoss"><a href="#CrossEntropyLoss" class="headerlink" title="CrossEntropyLoss"></a>CrossEntropyLoss</h2><p>现在，咱们用CrossEntropy作为loss函数来进行一个分类任务。</p>
<p>PyTorch在<a href="https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss" target="_blank" rel="noopener">CrossEntropyLoss</a>模块中预先实现了一个版本的cross entropy，它的用法和MSE稍微有点不同，所以这儿详细瞎逼逼一下它的参数：</p>
<ul>
<li><p>input:第一个参数就是咱们的分类神经网络的原始输出，它应该是一个维度为(N, C)的张量。其中，N是minibatch的大小，而C是class的数量。其中，第二维的数据是当前输入被分到各类别的没有normalize过的原始打分。CrossEntropyLoss模块自动为我们计算softmax，因此我们不需要自己算。</p>
</li>
<li><p>output:第二个参数是数据对应的label，用的时候应该输入一个长度为N的张量。每个维度上是正确的类别。</p>
</li>
</ul>
<p>下方代码展示三个预测在CrossEntropyLoss上的打分。正确的label为$y=[1, 1, 0]$。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loss = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">input = torch.tensor([[<span class="number">-1.</span>, <span class="number">1</span>],[<span class="number">-1</span>, <span class="number">1</span>],[<span class="number">1</span>, <span class="number">-1</span>]]) <span class="comment"># raw scores correspond to the correct class</span></span><br><span class="line"><span class="comment"># input = torch.tensor([[-3., 3],[-3, 3],[3, -3]]) # raw scores correspond to the correct class with higher confidence</span></span><br><span class="line"><span class="comment"># input = torch.tensor([[1., -1],[1, -1],[-1, 1]]) # raw scores correspond to the incorrect class</span></span><br><span class="line"><span class="comment"># input = torch.tensor([[3., -3],[3, -3],[-3, 3]]) # raw scores correspond to the incorrect class with incorrectly placed confidence</span></span><br><span class="line"></span><br><span class="line">target = torch.tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>])</span><br><span class="line">output = loss(input, target)</span><br><span class="line">print(output)</span><br></pre></td></tr></table></figure>



<p>输出:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt;</span><br><span class="line">tensor(<span class="number">0.1269</span>)</span><br></pre></td></tr></table></figure>

<h2 id="动态改变学习速率"><a href="#动态改变学习速率" class="headerlink" title="动态改变学习速率"></a>动态改变学习速率</h2><p>通常，我们不希望在训练过程中全程采用相同的学习速率。PyTorch提供相应组件支持根据训练进度自动调整学习速率。通常的策略包括每个epoch对学习速率lr乘以一个比率（比如0.9），并在训练loss下降地没那么快时将学习速率减半。</p>
<p>查看<a href="https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate" target="_blank" rel="noopener">learning rate scheduler docs</a>获取更多信息。</p>
<h2 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h2><p>当数据是图片时，我们通常希望使用卷积操作来怼它。PyTorh在<code>torch.nn.Conv2d</code>模块中实现了卷积操作。用户需要传入一个$(N,C_{in},H_{in},W_{in})$。其中，$N$是batch大小，$C_{in}$是通道数，$H_{in}$和$W_{in}$分别是输入图片的高和宽。</p>
<p>通过自定义以下参数，我们可以修改卷积操作：</p>
<ul>
<li>卷积核大小</li>
<li>步长</li>
<li>填充</li>
</ul>
<p>这些参数对输出张量的维度有影响，所以应该小心。</p>
<p>在<a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d" target="_blank" rel="noopener"><code>torch.nn.Conv2d</code> docs</a>中查看更多信息。</p>
<p>栗子：</p>
<p>在图片上怼一个gaussian blur kernel：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># a gaussian blur kernel</span></span><br><span class="line">gaussian_kernel = torch.tensor([[<span class="number">1.</span>, <span class="number">2</span>, <span class="number">1</span>],[<span class="number">2</span>, <span class="number">4</span>, <span class="number">2</span>],[<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>]]) / <span class="number">16.0</span></span><br><span class="line"></span><br><span class="line">conv = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># manually set the conv weight</span></span><br><span class="line">conv.weight.data[:] = gaussian_kernel</span><br><span class="line"></span><br><span class="line">convolved = conv(image_torch)</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">'original image'</span>)</span><br><span class="line">plt.imshow(image_torch.view(<span class="number">28</span>,<span class="number">28</span>).detach().numpy())</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">'blurred image'</span>)</span><br><span class="line">plt.imshow(convolved.view(<span class="number">26</span>,<span class="number">26</span>).detach().numpy())</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>读取一张RGB图片，输入通道数为3，输出通道数为16，在经过一个激活函数处理后，代码部分展示的输出结果又可以作为一个<code>Conv2d</code>的输入。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">im_channels = <span class="number">3</span> <span class="comment"># if we are working with RGB images, there are 3 input channels, with black and white, 1</span></span><br><span class="line">out_channels = <span class="number">16</span> <span class="comment"># this is a hyperparameter we can tune</span></span><br><span class="line">kernel_size = <span class="number">3</span> <span class="comment"># this is another hyperparameter we can tune</span></span><br><span class="line">batch_size = <span class="number">4</span></span><br><span class="line">image_width = <span class="number">32</span></span><br><span class="line">image_height = <span class="number">32</span></span><br><span class="line"></span><br><span class="line">im = torch.randn(batch_size, im_channels, image_width, image_height)</span><br><span class="line"></span><br><span class="line">m = nn.Conv2d(im_channels, out_channels, kernel_size)</span><br><span class="line">convolved = m(im) <span class="comment"># it is a module so we can call it</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'im shape'</span>, im.shape)</span><br><span class="line">print(<span class="string">'convolved im shape'</span>, convolved.shape)</span><br></pre></td></tr></table></figure>

<h2 id="一些蛮有用的链接"><a href="#一些蛮有用的链接" class="headerlink" title="一些蛮有用的链接"></a>一些蛮有用的链接</h2><ul>
<li><p><a href="https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html" target="_blank" rel="noopener">60 minute PyTorch Tutorial</a></p>
</li>
<li><p><a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">PyTorch Docs</a></p>
</li>
<li><p><a href="https://courses.cs.washington.edu/courses/cse446/19wi/notes/auto-diff.pdf" target="_blank" rel="noopener">Lecture notes on Auto-Diff</a></p>
</li>
</ul>
<h2 id="数据类"><a href="#数据类" class="headerlink" title="数据类"></a>数据类</h2><p><code>torch.utils.data.Dataset</code>是一个抽象类，咱们如果要弄一个自己的数据集的话就需要继承<code>Dataset</code>，然后覆盖以下的方法：</p>
<ul>
<li><code>__len__</code>，确保<code>len(dataset)</code>可以正确返回数据集的大小。</li>
<li><code>__getitem__</code>用来实现索引，确保dataset[i]可以正确拿到第i个数据。</li>
</ul>
<p>咱们现在来搞一个face landmarks数据集。咱们在<code>__init__</code>里头读取csv文件，但把读取图片的任务留给<code>__getitem__</code>。这样做的好处是节省内存，不需要将所有数据一下子全读到内存里去。</p>
<p>咱们数据集的sample是一个dict:<code>{&#39;image&#39;: image, &#39;landmarks&#39;: landmarks}</code>。咱们数据集还要再弄一个<code>transform</code>方法，这样任何需要的操作都可以被施加在图片上。咱们在下一节里头看看它的用处。</p>
<p>栗子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FaceLandmarksDataset</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    <span class="string">"""Face Landmarks dataset."""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, csv_file, root_dir, transform=None)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            csv_file (string): Path to the csv file with annotations.</span></span><br><span class="line"><span class="string">            root_dir (string): Directory with all the images.</span></span><br><span class="line"><span class="string">            transform (callable, optional): Optional transform to be applied</span></span><br><span class="line"><span class="string">                on a sample.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.landmarks_frame = pd.read_csv(csv_file)</span><br><span class="line">        self.root_dir = root_dir</span><br><span class="line">        self.transform = transform</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.landmarks_frame)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, idx)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> torch.is_tensor(idx):</span><br><span class="line">            idx = idx.tolist()</span><br><span class="line"></span><br><span class="line">        img_name = os.path.join(self.root_dir,</span><br><span class="line">                                self.landmarks_frame.iloc[idx, <span class="number">0</span>])</span><br><span class="line">        image = io.imread(img_name)</span><br><span class="line">        landmarks = self.landmarks_frame.iloc[idx, <span class="number">1</span>:]</span><br><span class="line">        landmarks = np.array([landmarks])</span><br><span class="line">        landmarks = landmarks.astype(<span class="string">'float'</span>).reshape(<span class="number">-1</span>, <span class="number">2</span>)</span><br><span class="line">        sample = &#123;<span class="string">'image'</span>: image, <span class="string">'landmarks'</span>: landmarks&#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.transform:</span><br><span class="line">            sample = self.transform(sample)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> sample</span><br></pre></td></tr></table></figure>

<p>然鹅，如果咱们直接用<code>for</code>把整个数据走一遍会少掉很多东西，比如:</p>
<ul>
<li>按batch读取data</li>
<li>随机读取data</li>
<li>多线程读取data</li>
</ul>
<p>不过施主先不要着急，这些功能<code>torch.utils.data.DataLoader</code>都有。通过修改<code>collate_fn</code>参数，你可以指定数据按什么样的batch方式被读取，不过默认的读取方法已经可以cover大部分的需求了。</p>
<p>🌰：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataloader = DataLoader(transformed_dataset, batch_size=<span class="number">4</span>,</span><br><span class="line">                        shuffle=<span class="literal">True</span>, num_workers=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i_batch, sample_batched <span class="keyword">in</span> enumerate(dataloader):</span><br><span class="line">    print(i_batch, sample_batched[<span class="string">'image'</span>].size(),</span><br><span class="line">          sample_batched[<span class="string">'landmarks'</span>].size())</span><br></pre></td></tr></table></figure>

<h2 id="混合精度训练"><a href="#混合精度训练" class="headerlink" title="混合精度训练"></a>混合精度训练</h2><p>作者: <code>Chi-Liang Liu</code> 引用: <a href="https://github.com/NVIDIA/apex。采用混合精度训练你的模型，训练出来的神经网络可以：" target="_blank" rel="noopener">https://github.com/NVIDIA/apex。采用混合精度训练你的模型，训练出来的神经网络可以：</a></p>
<ul>
<li>运行速度快2-4倍</li>
<li>几行代码就可以省下一笔买新内存的钱</li>
</ul>
<h2 id="Apex"><a href="#Apex" class="headerlink" title="Apex"></a>Apex</h2><p>nvidia维护的工具简化了Pytorch中的混合精度和分布式培训。这里的一些代码最终将包含在上游Pytorch中。Apex的目的是尽快为用户提供最新的实用工具。</p>
<h2 id="apex-amp"><a href="#apex-amp" class="headerlink" title="apex.amp"></a>apex.amp</h2><p>Amp允许用户轻松地尝试不同的纯模式和混合精度模式。通过选择“优化级”或opt_level选择常用的默认模式;每个opt_level建立一组属性来管理Amp实现的纯精度或混合精度训练。通过将特定属性的值直接传递给amp.initialize，可以实现对给定opt_level行为方式的细粒度控制。这些手动指定的值覆盖opt_level建立的默认值。</p>
<p>🌰:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Declare model and optimizer as usual, with default (FP32) precision</span></span><br><span class="line">model = torch.nn.Linear(D_in, D_out).cuda()</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">1e-3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Allow Amp to perform casts as required by the opt_level</span></span><br><span class="line">model, optimizer = amp.initialize(model, optimizer, opt_level=<span class="string">"O1"</span>)</span><br><span class="line">...</span><br><span class="line"><span class="comment"># loss.backward() becomes:</span></span><br><span class="line"><span class="keyword">with</span> amp.scale_loss(loss, optimizer) <span class="keyword">as</span> scaled_loss:</span><br><span class="line">    scaled_loss.backward()</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

]]></content>
      <tags>
        <tag>李宏毅</tag>
      </tags>
  </entry>
  <entry>
    <title>Pandas入门</title>
    <url>/2020/03/20/Pandas%E5%85%A5%E9%97%A8/</url>
    <content><![CDATA[<h1 id="Pandas"><a href="#Pandas" class="headerlink" title="Pandas"></a>Pandas</h1><h2 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h2><p>Pandas中的基础数据类型有Series和DataFrame，其中，Series具有一维索引（行索引），DataFrame具有二维索引（行索引和列索引）。</p>
<h3 id="Series"><a href="#Series" class="headerlink" title="Series"></a>Series</h3><p>既然是一维索引，就要有key,value对，可以直接传入一个dict。通过一个key，就可以确定一个value。<br><code>s = pd.Series({&#39;a&#39;: 10, &#39;b&#39;: 20, &#39;c&#39;: 30})</code></p>
<h3 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h3><p>需要行和列才可以确定一个元素。<br>可以按列传入</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">12df &#x3D; pd.DataFrame(&#123;&#39;one&#39;: pd.Series([1, 2, 3]),</span><br><span class="line">                   &#39;two&#39;: pd.Series([4, 5, 6])&#125;)</span><br></pre></td></tr></table></figure>

<p>也可以按行传入</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">123df &#x3D; pd.DataFrame([&#123;&#39;one&#39;: 1, &#39;two&#39;: 4&#125;,</span><br><span class="line">                   &#123;&#39;one&#39;: 2, &#39;two&#39;: 5&#125;,</span><br><span class="line">                   &#123;&#39;one&#39;: 3, &#39;two&#39;: 6&#125;])</span><br></pre></td></tr></table></figure>

<h2 id="数据读取"><a href="#数据读取" class="headerlink" title="数据读取"></a>数据读取</h2><p><a href="https://pandas.pydata.org/pandas-docs/stable/reference/io.html" target="_blank" rel="noopener">Input/output — pandas 1.0.1 documentation</a></p>
<h3 id="读取csv"><a href="#读取csv" class="headerlink" title="读取csv"></a>读取csv</h3><p><code>f = pd.read_csv()</code><br><img src="/" alt class="lazyload" data-src="/2020/03/20/Pandas%E5%85%A5%E9%97%A8/1.png"></p>
<h2 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h2><ul>
<li><code>df.head()</code>显示文件头部</li>
<li><code>df.tail()</code>显示文件尾部</li>
<li><code>df.describe()</code>显示数据概览</li>
<li><code>df.values</code>将DataFrame转换为numpy数组</li>
<li><code>df.index</code>行索引</li>
<li><code>df.columns</code>列索引</li>
<li><code>df.shape</code>DataFrame形状<br>所有属性：<a href="https://pandas.pydata.org/pandas-docs/stable/reference/frame.html#attributes-and-underlying-data" target="_blank" rel="noopener">DataFrame — pandas 1.0.1 documentation</a></li>
</ul>
<h2 id="数据选择"><a href="#数据选择" class="headerlink" title="数据选择"></a>数据选择</h2><p>数据选择就是将原始数据中的一部分拿出来，用于后续处理步骤</p>
<h3 id="基于索引数字选择"><a href="#基于索引数字选择" class="headerlink" title="基于索引数字选择"></a>基于索引数字选择</h3><ul>
<li><code>df.iloc[:3]</code> 选择前三行</li>
<li><code>df.iloc[5]</code>选择某一行</li>
<li><code>df.iloc[[行], [列]]</code> 其中，行和列都可以是list</li>
<li><code>df.iloc[:，1:4]</code>选择2到4列（切片时左闭右开）</li>
</ul>
<h3 id="基于标签的选择-切片时左右都闭"><a href="#基于标签的选择-切片时左右都闭" class="headerlink" title="基于标签的选择(切片时左右都闭)"></a>基于标签的选择(切片时左右都闭)</h3><ul>
<li><code>df.loc[0:2]</code>选择前三行</li>
<li><code>df.loc[[0,2,4]]</code>选择1、3、5行</li>
<li><code>df.loc[:,&#39;Total Population&#39;:&#39;Total Males&#39;]</code></li>
<li><code>df.loc[[0, 2], &#39;Median Age&#39;:]</code>选择1、3行的’Median Age’后面的列</li>
</ul>
<h3 id="数据删减"><a href="#数据删减" class="headerlink" title="数据删减"></a>数据删减</h3><ul>
<li><p><code>df.drop(labels=[&#39;Median Age&#39;, &#39;Total Males&#39;], axis = 1)</code>删除两个指定列</p>
</li>
<li><p><code>df.drop(labels=[0, 1], axis = 0)</code>删除0、1行</p>
</li>
<li><p><code>df.drop_dupicates()</code>数据去重</p>
</li>
<li><p><code>df.dropna()</code>数据去空值</p>
</li>
<li><p><code>df.insert(value = 要插入的值, loc=列号，column = &#39;列名&#39;)</code>在DataFrame中插入名字叫’列名’，值为value的一列。</p>
</li>
<li><p><code>df.isna()</code>返回bool</p>
</li>
<li><p><code>df.notna()</code>返回bool列表<br>填充缺失值</p>
</li>
<li><p><code>df.fillna()</code></p>
</li>
<li><p><code>df.fillna(method=&#39;pad&#39;)</code>使用前面的值填充空值</p>
</li>
<li><p>‘df.fillna(method=‘bfill’)’使用后面的值填充</p>
</li>
<li><p><code>df.fillna(df.mean()[&#39;C&#39;:&#39;E&#39;])</code>使用平均值填充<br>插值填充</p>
</li>
<li><pre><code>df.interpolate(method=)</code></pre><ul>
<li><code>method=&#39;quadratic&#39;</code>数据增长速率越来越快</li>
<li><code>method=&#39;pchip&#39;</code>累计分布</li>
<li><code>method=&#39;akima&#39;</code>以平滑绘图为目的<br>绘图</li>
</ul>
</li>
<li><p>df.plot(kind=‘bar’…)直接绘图<br>其它</p>
</li>
<li><p>数据计算 <a href="https://pandas.pydata.org/pandas-docs/stable/reference/frame.html#binary-operator-functions" target="_blank" rel="noopener">DataFrame — pandas 1.0.1 documentation</a></p>
</li>
<li><p>数据聚合 <a href="https://pandas.pydata.org/pandas-docs/stable/reference/frame.html#function-application-groupby-window" target="_blank" rel="noopener">DataFrame — pandas 1.0.1 documentation</a></p>
</li>
<li><p>统计分析 <a href="https://pandas.pydata.org/pandas-docs/stable/reference/frame.html#computations-descriptive-stats" target="_blank" rel="noopener">DataFrame — pandas 1.0.1 documentation</a></p>
</li>
<li><p>时间序列 <a href="https://pandas.pydata.org/pandas-docs/stable/reference/frame.html#time-series-related" target="_blank" rel="noopener">DataFrame — pandas 1.0.1 documentation</a></p>
</li>
</ul>
]]></content>
      <tags>
        <tag>Pandas</tag>
      </tags>
  </entry>
</search>
