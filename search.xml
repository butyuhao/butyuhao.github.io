<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>PyTorch教学</title>
    <url>/2020/03/21/PyTorch%E6%95%99%E5%AD%A6/</url>
    <content><![CDATA[<h2 id="PyTorch-介绍"><a href="#PyTorch-介绍" class="headerlink" title="PyTorch 介绍"></a>PyTorch 介绍</h2><p>PyTorch是一款强有力的深度学习框架，不管是科研还是生产部署的需求它都能满足。<br>本教程仅提供粗略的介绍，如果有任何疑问可以上网查查或是去咨询朋友。（小声bb：多google少百度）<br>一些用得上的point：</p>
<ul>
<li><p>自动differentiation工具非常强大</p>
</li>
<li><p>PyTorch为你实现好了深度学习中的常用功能</p>
</li>
<li><p>使用PyTorch DataSet来处理数据</p>
</li>
</ul>
<h2 id="Tensor与numpy的关系"><a href="#Tensor与numpy的关系" class="headerlink" title="Tensor与numpy的关系"></a>Tensor与numpy的关系</h2><p>PyTorch的基本组成部分，block与numpy的ndarray相似。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># we create tensors in a similar way to numpy nd arrays</span></span><br><span class="line">x_numpy = np.array([<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>])</span><br><span class="line">x_torch = torch.tensor([<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>])</span><br><span class="line">print(<span class="string">'x_numpy, x_torch'</span>)</span><br><span class="line">print(x_numpy, x_torch)</span><br><span class="line">print()</span><br><span class="line"></span><br><span class="line"><span class="comment"># to and from numpy, pytorch</span></span><br><span class="line">print(<span class="string">'to and from numpy and pytorch'</span>)</span><br><span class="line">print(torch.from_numpy(x_numpy), x_torch.numpy())</span><br><span class="line">print()</span><br><span class="line"></span><br><span class="line"><span class="comment"># we can do basic operations like +-*/</span></span><br><span class="line">y_numpy = np.array([<span class="number">3</span>,<span class="number">4</span>,<span class="number">5.</span>])</span><br><span class="line">y_torch = torch.tensor([<span class="number">3</span>,<span class="number">4</span>,<span class="number">5.</span>])</span><br><span class="line">print(<span class="string">"x+y"</span>)</span><br><span class="line">print(x_numpy + y_numpy, x_torch + y_torch)</span><br><span class="line">print()</span><br><span class="line"></span><br><span class="line"><span class="comment"># many functions that are in numpy are also in pytorch</span></span><br><span class="line">print(<span class="string">"norm"</span>)</span><br><span class="line">print(np.linalg.norm(x_numpy), torch.norm(x_torch))</span><br><span class="line">print()</span><br><span class="line"></span><br><span class="line"><span class="comment"># to apply an operation along a dimension,</span></span><br><span class="line"><span class="comment"># we use the dim keyword argument instead of axis</span></span><br><span class="line">print(<span class="string">"mean along the 0th dimension"</span>)</span><br><span class="line">x_numpy = np.array([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4.</span>]])</span><br><span class="line">x_torch = torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4.</span>]])</span><br><span class="line">print(np.mean(x_numpy, axis=<span class="number">0</span>), torch.mean(x_torch, dim=<span class="number">0</span>))</span><br></pre></td></tr></table></figure>
<h2 id="Tensor-view"><a href="#Tensor-view" class="headerlink" title="Tensor.view"></a>Tensor.view</h2><p>和numpy.reshape()一样，我们可以使用tensor.view()来改变tensor的形状。当使用-1时，其自动计算剩下维度的值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># "MNIST"</span></span><br><span class="line">N, C, W, H = <span class="number">10000</span>, <span class="number">3</span>, <span class="number">28</span>, <span class="number">28</span></span><br><span class="line">X = torch.randn((N, C, W, H))</span><br><span class="line"></span><br><span class="line">print(X.shape)</span><br><span class="line">print(X.view(N, C, <span class="number">784</span>).shape)</span><br><span class="line">print(X.view(<span class="number">-1</span>, C, <span class="number">784</span>).shape) <span class="comment"># automatically choose the 0th dimension</span></span><br></pre></td></tr></table></figure>
<h2 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h2><p>PyTorch的tensor有一个特别的地方，当tensor对象时，Pytorch自动在后台创建“计算图”。计算图是使用图的方式来表达一个数学表达式。其带有一种算法来计算一个计算图中所有变量的梯度，计算顺序和函数本身的顺序一致。<br>假设表达式为$e=(a+b)*(b+1)$,其中，$a=2, b=1$我们可以画出如下的计算图：</p>
<p><img src="/" alt class="lazyload" data-src="/2020/03/21/PyTorch%E6%95%99%E5%AD%A6/tree-eval.png"></p>
<p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.tensor(<span class="number">2.0</span>, requires_grad=<span class="literal">True</span>) <span class="comment"># we set requires_grad=True to let PyTorch know to keep the graph</span></span><br><span class="line">b = torch.tensor(<span class="number">1.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">c = a + b</span><br><span class="line">d = b + <span class="number">1</span></span><br><span class="line">e = c * d</span><br><span class="line">print(<span class="string">'c'</span>, c)</span><br><span class="line">print(<span class="string">'d'</span>, d)</span><br><span class="line">print(<span class="string">'e'</span>, e)</span><br></pre></td></tr></table></figure>

<p>执行结果为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt;</span><br><span class="line">c tensor(<span class="number">3.</span>, grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">d tensor(<span class="number">2.</span>, grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">e tensor(<span class="number">6.</span>, grad_fn=&lt;MulBackward0&gt;)</span><br></pre></td></tr></table></figure>

<p>可以看到，PyTorch为我们记录了计算图。</p>
<h2 id="使用Pytorch来计算梯度"><a href="#使用Pytorch来计算梯度" class="headerlink" title="使用Pytorch来计算梯度"></a>使用Pytorch来计算梯度</h2><p>我们已经发现PyTorch会自动为我们记录计算图，现在，让我们用它来求一求梯度。</p>
<p>考虑这样一个方程：$f(x)=(x-2)^2$。</p>
<p>问题：计算${df(x)}\over{dx}$，求$f’(1)$。</p>
<p>我们对leaf variable <code>y</code>调用<code>backward()</code>方法，一次性计算<code>y</code>的所有梯度。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (x<span class="number">-2</span>)**<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fp</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span>*(x<span class="number">-2</span>)</span><br><span class="line"></span><br><span class="line">x = torch.tensor([<span class="number">1.0</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">y = f(x)</span><br><span class="line">y.backward()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Analytical f\'(x):'</span>, fp(x))</span><br><span class="line">print(<span class="string">'PyTorch\'s f\'(x):'</span>, x.grad)</span><br></pre></td></tr></table></figure>

<p>输出为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt;</span><br><span class="line">Analytical <span class="string">f'(x): tensor([-2.], grad_fn=&lt;MulBackward0&gt;) #带有grad_fn</span></span><br><span class="line"><span class="string">PyTorch'</span>s <span class="string">f'(x): tensor([-2.])</span></span><br></pre></td></tr></table></figure>

<p><code>backward()</code>也可以计算带有数学函数的表达式的梯度：</p>
<p>现有$w=[w_1,w_2]^T$，</p>
<p>$g(w)=2w_1w_2+w_2cos(w_1)$</p>
<p>问题：计算&nabla;$_w g(w)$然后验证&nabla;$_w g([\pi,1]) = [2,\pi-1]^T$。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">g</span><span class="params">(w)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span>*w[<span class="number">0</span>]*w[<span class="number">1</span>] + w[<span class="number">1</span>]*torch.cos(w[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grad_g</span><span class="params">(w)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> torch.tensor([<span class="number">2</span>*w[<span class="number">1</span>] - w[<span class="number">1</span>]*torch.sin(w[<span class="number">0</span>]), <span class="number">2</span>*w[<span class="number">0</span>] + torch.cos(w[<span class="number">0</span>])])</span><br><span class="line"></span><br><span class="line">w = torch.tensor([np.pi, <span class="number">1</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">z = g(w)</span><br><span class="line">z.backward()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Analytical grad g(w)'</span>, grad_g(w))</span><br><span class="line">print(<span class="string">'PyTorch\'s grad g(w)'</span>, w.grad)</span><br></pre></td></tr></table></figure>

<p>输出如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Analytical grad g(w) tensor([<span class="number">2.0000</span>, <span class="number">5.2832</span>])</span><br><span class="line">PyTorch<span class="string">'s grad g(w) tensor([2.0000, 5.2832])</span></span><br></pre></td></tr></table></figure>

<h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>有了梯度，咱们就可以试试看梯度下降：</p>
<p>假设$f$就是上部分定义的函数</p>
<p>问题：找出使得$f$最小的$x$。</p>
<p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">5.0</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">step_size = <span class="number">0.25</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'iter,\tx,\tf(x),\tf\'(x),\tf\'(x) pytorch'</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">15</span>):</span><br><span class="line">    y = f(x)</span><br><span class="line">    y.backward() <span class="comment"># compute the gradient</span></span><br><span class="line">    </span><br><span class="line">    print(<span class="string">'&#123;&#125;,\t&#123;:.3f&#125;,\t&#123;:.3f&#125;,\t&#123;:.3f&#125;,\t&#123;:.3f&#125;'</span>.format(i, x.item(), f(x).item(), fp(x).item(), x.grad.item()))</span><br><span class="line">    </span><br><span class="line">    x.data = x.data - step_size * x.grad <span class="comment"># perform a GD update step</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># We need to zero the grad variable since the backward()</span></span><br><span class="line">    <span class="comment"># call accumulates the gradients in .grad instead of overwriting.</span></span><br><span class="line">    <span class="comment"># The detach_() is for efficiency. You do not need to worry too much about it.</span></span><br><span class="line">    x.grad.detach_()</span><br><span class="line">    x.grad.zero_()</span><br></pre></td></tr></table></figure>

<p>结果为：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">iter,	x,	f(x),	f<span class="string">'(x),	f'</span>(x) pytorch</span><br><span class="line">0,	5.000,	9.000,	6.000,	6.000</span><br><span class="line">1,	3.500,	2.250,	3.000,	3.000</span><br><span class="line">2,	2.750,	0.562,	1.500,	1.500</span><br><span class="line">3,	2.375,	0.141,	0.750,	0.750</span><br><span class="line">4,	2.188,	0.035,	0.375,	0.375</span><br><span class="line">5,	2.094,	0.009,	0.188,	0.188</span><br><span class="line">6,	2.047,	0.002,	0.094,	0.094</span><br><span class="line">7,	2.023,	0.001,	0.047,	0.047</span><br><span class="line">8,	2.012,	0.000,	0.023,	0.023</span><br><span class="line">9,	2.006,	0.000,	0.012,	0.012</span><br><span class="line">10,	2.003,	0.000,	0.006,	0.006</span><br><span class="line">11,	2.001,	0.000,	0.003,	0.003</span><br><span class="line">12,	2.001,	0.000,	0.001,	0.001</span><br><span class="line">13,	2.000,	0.000,	0.001,	0.001</span><br><span class="line">14,	2.000,	0.000,	0.000,	0.000</span><br></pre></td></tr></table></figure>

<h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><p>刚刚咱们最小化了一个瞎掰出来的函数，现在让我们最小化一个loss函数，并放入一些瞎掰的data。</p>
<p>现在咱用梯度下降解决一下线性回归问题：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># make a simple linear dataset with some noise</span></span><br><span class="line"></span><br><span class="line">d = <span class="number">2</span></span><br><span class="line">n = <span class="number">50</span></span><br><span class="line">X = torch.randn(n,d)</span><br><span class="line">true_w = torch.tensor([[<span class="number">-1.0</span>], [<span class="number">2.0</span>]])</span><br><span class="line">y = X @ true_w + torch.randn(n,<span class="number">1</span>) * <span class="number">0.1</span> <span class="comment"># @是矩阵乘法，*是元素逐个相乘。</span></span><br><span class="line">print(<span class="string">'X shape'</span>, X.shape)</span><br><span class="line">print(<span class="string">'y shape'</span>, y.shape)</span><br><span class="line">print(<span class="string">'w shape'</span>, true_w.shape)</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X shape torch.Size([<span class="number">50</span>, <span class="number">2</span>])</span><br><span class="line">y shape torch.Size([<span class="number">50</span>, <span class="number">1</span>])</span><br><span class="line">w shape torch.Size([<span class="number">2</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<h2 id="验证梯度计算的正确性"><a href="#验证梯度计算的正确性" class="headerlink" title="验证梯度计算的正确性"></a>验证梯度计算的正确性</h2><p>本部分我们验证PyTorch梯度计算的正确性。</p>
<p>Loss的梯度公式如下：</p>
<p>$\nabla _w \frac{1}{n}||y-Xw||_2^2=-\frac{2}{n}X^T(y-Xw)$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># define a linear model with no bias</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X, w)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> X @ w</span><br><span class="line"></span><br><span class="line"><span class="comment"># the residual sum of squares loss function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rss</span><span class="params">(y, y_hat)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> torch.norm(y - y_hat)**<span class="number">2</span> / n <span class="comment">#torch.norm是范数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># analytical expression for the gradient</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grad_rss</span><span class="params">(X, y, w)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">-2</span>*X.t() @ (y - X @ w) / n</span><br><span class="line"></span><br><span class="line">w = torch.tensor([[<span class="number">1.</span>], [<span class="number">0</span>]], requires_grad=<span class="literal">True</span>)</span><br><span class="line">y_hat = model(X, w)</span><br><span class="line"></span><br><span class="line">loss = rss(y, y_hat)</span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Analytical gradient'</span>, grad_rss(X, y, w).detach().view(<span class="number">2</span>).numpy())</span><br><span class="line">print(<span class="string">'PyTorch\'s gradient'</span>, w.grad.view(<span class="number">2</span>).numpy())</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Analytical gradient [ <span class="number">3.974099</span> <span class="number">-4.494481</span>]</span><br><span class="line">PyTorch<span class="string">'s gradient [ 3.9740987 -4.4944816]</span></span><br></pre></td></tr></table></figure>

<p>从结果中，我们可以看出PyTorch计算出了正确的梯度，现在让我们把梯度用起来吧。</p>
<h2 id="使用梯度下降算法，基于自动求导功能的线性回归"><a href="#使用梯度下降算法，基于自动求导功能的线性回归" class="headerlink" title="使用梯度下降算法，基于自动求导功能的线性回归"></a>使用梯度下降算法，基于自动求导功能的线性回归</h2><p>现在咱们用得出的梯度来实现梯度下降算法。</p>
<p>Note:这个例子只是单纯展示如何用PyTorch将之前的想法实现出来，后面还会介绍如何用PyTorchic的方式来做同样的事情。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">step_size = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'iter,\tloss,\tw'</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">    y_hat = model(X, w)</span><br><span class="line">    loss = rss(y, y_hat)</span><br><span class="line">    </span><br><span class="line">    loss.backward() <span class="comment"># compute the gradient of the loss</span></span><br><span class="line">    </span><br><span class="line">    w.data = w.data - step_size * w.grad <span class="comment"># do a gradient descent step</span></span><br><span class="line">    </span><br><span class="line">    print(<span class="string">'&#123;&#125;,\t&#123;:.2f&#125;,\t&#123;&#125;'</span>.format(i, loss.item(), w.view(<span class="number">2</span>).detach().numpy()))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># We need to zero the grad variable since the backward()</span></span><br><span class="line">    <span class="comment"># call accumulates the gradients in .grad instead of overwriting.</span></span><br><span class="line">    <span class="comment"># The detach_() is for efficiency. You do not need to worry too much about it.</span></span><br><span class="line">    w.grad.detach()</span><br><span class="line">    w.grad.zero_()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'\ntrue w\t\t'</span>, true_w.view(<span class="number">2</span>).numpy())</span><br><span class="line">print(<span class="string">'estimated w\t'</span>, w.view(<span class="number">2</span>).detach().numpy())</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">iter,	loss,	w</span><br><span class="line"><span class="number">0</span>,	<span class="number">8.47</span>,	[<span class="number">0.20518023</span> <span class="number">0.89889634</span>]</span><br><span class="line"><span class="number">1</span>,	<span class="number">2.80</span>,	[<span class="number">-0.03049211</span>  <span class="number">1.1496693</span> ]</span><br><span class="line"><span class="number">2</span>,	<span class="number">1.75</span>,	[<span class="number">-0.21864393</span>  <span class="number">1.3446302</span> ]</span><br><span class="line"><span class="number">3</span>,	<span class="number">1.09</span>,	[<span class="number">-0.3690024</span>  <span class="number">1.4960643</span>]</span><br><span class="line"><span class="number">4</span>,	<span class="number">0.68</span>,	[<span class="number">-0.4892798</span>  <span class="number">1.6135726</span>]</span><br><span class="line"><span class="number">5</span>,	<span class="number">0.43</span>,	[<span class="number">-0.5855947</span>  <span class="number">1.7046558</span>]</span><br><span class="line"><span class="number">6</span>,	<span class="number">0.27</span>,	[<span class="number">-0.66280454</span>  <span class="number">1.7751708</span> ]</span><br><span class="line"><span class="number">7</span>,	<span class="number">0.18</span>,	[<span class="number">-0.7247683</span>  <span class="number">1.8296891</span>]</span><br><span class="line"><span class="number">8</span>,	<span class="number">0.11</span>,	[<span class="number">-0.7745538</span>  <span class="number">1.8717768</span>]</span><br><span class="line"><span class="number">9</span>,	<span class="number">0.08</span>,	[<span class="number">-0.8146021</span>  <span class="number">1.9042141</span>]</span><br><span class="line"><span class="number">10</span>,	<span class="number">0.05</span>,	[<span class="number">-0.84685683</span>  <span class="number">1.9291675</span> ]</span><br><span class="line"><span class="number">11</span>,	<span class="number">0.04</span>,	[<span class="number">-0.87286717</span>  <span class="number">1.9483235</span> ]</span><br><span class="line"><span class="number">12</span>,	<span class="number">0.03</span>,	[<span class="number">-0.89386874</span>  <span class="number">1.9629945</span> ]</span><br><span class="line"><span class="number">13</span>,	<span class="number">0.02</span>,	[<span class="number">-0.91084814</span>  <span class="number">1.9742005</span> ]</span><br><span class="line"><span class="number">14</span>,	<span class="number">0.02</span>,	[<span class="number">-0.9245938</span>  <span class="number">1.982734</span> ]</span><br><span class="line"><span class="number">15</span>,	<span class="number">0.02</span>,	[<span class="number">-0.93573654</span>  <span class="number">1.9892098</span> ]</span><br><span class="line"><span class="number">16</span>,	<span class="number">0.01</span>,	[<span class="number">-0.9447814</span>  <span class="number">1.9941043</span>]</span><br><span class="line"><span class="number">17</span>,	<span class="number">0.01</span>,	[<span class="number">-0.9521335</span>  <span class="number">1.9977864</span>]</span><br><span class="line"><span class="number">18</span>,	<span class="number">0.01</span>,	[<span class="number">-0.9581177</span>  <span class="number">2.0005412</span>]</span><br><span class="line"><span class="number">19</span>,	<span class="number">0.01</span>,	[<span class="number">-0.96299535</span>  <span class="number">2.002589</span>  ]</span><br><span class="line"></span><br><span class="line">true w		 [<span class="number">-1.</span>  <span class="number">2.</span>]</span><br><span class="line">estimated w	 [<span class="number">-0.96299535</span>  <span class="number">2.002589</span>  ]</span><br></pre></td></tr></table></figure>

<h2 id="torch-nn-Module"><a href="#torch-nn-Module" class="headerlink" title="torch.nn.Module"></a>torch.nn.Module</h2><p><code>Module</code>是PyTorch对tensor施加操作的一种方式，各模块被作为torch.nn.Module的子类实现。所有模块都可以被调用，并可以被组合起来形成更加复杂的功能。</p>
<p><code>torch.nn</code><a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">docs</a></p>
<p>Note：多数为module实现的功能也可以通过<code>torch.nn.functional</code>来访问，但需要用户自己维护权重tensor。</p>
<p><code>torch.nn.functional</code><a href="https://pytorch.org/docs/stable/nn.html#torch-nn-functional" target="_blank" rel="noopener">docs</a></p>
<h2 id="Linear-Module"><a href="#Linear-Module" class="headerlink" title="Linear Module"></a>Linear Module</h2><p>Linear Module是一个常用模块，它很方便地实现带有一个bias的线性变换模型。你只需要指定输入结点的个数与输出节点的个数，它将自动为你生成中间权重与bias，创建出相应的线性模型。</p>
<p>和我们手动初始化$w$不同的是，Linear Module自动为我们初始化权重。对于最小化一个非凸的loss函数来说，权重的初始化是非常重要的。如果训练出来的模型没有想象中的好，可以尝试手动初始化权重，使其与默认的权重不同。<code>torch.nn.init</code>中有一些常用的参数初始化方式。</p>
<p><code>torch.nn.init</code><a href="https://pytorch.org/docs/stable/nn.html#torch-nn-init" target="_blank" rel="noopener">docs</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d_in = <span class="number">3</span></span><br><span class="line">d_out = <span class="number">4</span></span><br><span class="line">linear_module = nn.Linear(d_in, d_out)</span><br><span class="line"></span><br><span class="line">example_tensor = torch.tensor([[<span class="number">1.</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line"><span class="comment"># applys a linear transformation to the data</span></span><br><span class="line">transformed = linear_module(example_tensor)</span><br><span class="line">print(<span class="string">'example_tensor'</span>, example_tensor.shape)</span><br><span class="line">print(<span class="string">'transormed'</span>, transformed.shape)</span><br><span class="line">print()</span><br><span class="line">print(<span class="string">'We can see that the weights exist in the background\n'</span>)</span><br><span class="line">print(<span class="string">'W:'</span>, linear_module.weight)</span><br><span class="line">print(<span class="string">'b:'</span>, linear_module.bias)</span><br></pre></td></tr></table></figure>



<p>结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt;</span><br><span class="line">print(<span class="string">'We can see that the weights exist in the background\n'</span>)</span><br><span class="line">print(<span class="string">'W:'</span>, linear_module.weight)</span><br><span class="line">print(<span class="string">'b:'</span>, linear_module.bias)</span><br><span class="line">example_tensor torch.Size([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">transormed torch.Size([<span class="number">2</span>, <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">We can see that the weights exist <span class="keyword">in</span> the background</span><br><span class="line"></span><br><span class="line">W: Parameter containing:</span><br><span class="line">tensor([[ <span class="number">0.2383</span>, <span class="number">-0.0450</span>,  <span class="number">0.2986</span>],</span><br><span class="line">        [<span class="number">-0.0828</span>, <span class="number">-0.0900</span>,  <span class="number">0.2475</span>],</span><br><span class="line">        [<span class="number">-0.4174</span>,  <span class="number">0.3788</span>,  <span class="number">0.5005</span>],</span><br><span class="line">        [<span class="number">-0.3601</span>, <span class="number">-0.4104</span>, <span class="number">-0.0584</span>]], requires_grad=<span class="literal">True</span>)</span><br><span class="line">b: Parameter containing:</span><br><span class="line">tensor([<span class="number">-0.0385</span>, <span class="number">-0.0826</span>,  <span class="number">0.0033</span>,  <span class="number">0.4773</span>], requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>可以看到，咱们只是指定了输入和输出的维度，Linear Module就替我们把weight和bias全创建好了。</p>
<h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><p>PyTorch里头预先实现了一大票的激活函数，包括但不限于ReLU、Tanh和Sigmoid。因为他们都是模块，所以使用时需要先实例化。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">activation_fn = nn.ReLU() <span class="comment"># we instantiate an instance of the ReLU module</span></span><br><span class="line">example_tensor = torch.tensor([<span class="number">-1.0</span>, <span class="number">1.0</span>, <span class="number">0.0</span>])</span><br><span class="line">activated = activation_fn(example_tensor)</span><br><span class="line">print(<span class="string">'example_tensor'</span>, example_tensor)</span><br><span class="line">print(<span class="string">'activated'</span>, activated)</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">example_tensor tensor([<span class="number">-1.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>])</span><br><span class="line">activated tensor([<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>])</span><br></pre></td></tr></table></figure>

<h2 id="Sequential"><a href="#Sequential" class="headerlink" title="Sequential"></a>Sequential</h2><p>Sequential提供给咱们一个绝佳的解决方案，用于将多个简单模块组合在一起。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d_in = <span class="number">3</span></span><br><span class="line">d_hidden = <span class="number">4</span></span><br><span class="line">d_out = <span class="number">1</span></span><br><span class="line">model = torch.nn.Sequential(</span><br><span class="line">                            nn.Linear(d_in, d_hidden),</span><br><span class="line">                            nn.Tanh(),</span><br><span class="line">                            nn.Linear(d_hidden, d_out),</span><br><span class="line">                            nn.Sigmoid()</span><br><span class="line">                           )</span><br><span class="line"></span><br><span class="line">example_tensor = torch.tensor([[<span class="number">1.</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line">transformed = model(example_tensor)</span><br><span class="line">print(<span class="string">'transformed'</span>, transformed.shape)</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt;</span><br><span class="line">transformed torch.Size([<span class="number">2</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<p>小技巧：咱们可以使用<code>parameters()</code>方法来得到得到任何<code>nn.Module( )</code>的参数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">params = model.parameters()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">    print(param)</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Parameter containing:</span><br><span class="line">tensor([[<span class="number">-0.5554</span>,  <span class="number">0.0456</span>, <span class="number">-0.3115</span>],</span><br><span class="line">        [ <span class="number">0.0697</span>, <span class="number">-0.1629</span>,  <span class="number">0.3342</span>],</span><br><span class="line">        [ <span class="number">0.1340</span>, <span class="number">-0.1353</span>,  <span class="number">0.1261</span>],</span><br><span class="line">        [ <span class="number">0.0624</span>,  <span class="number">0.3285</span>, <span class="number">-0.4536</span>]], requires_grad=<span class="literal">True</span>)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([ <span class="number">0.3684</span>, <span class="number">-0.0760</span>, <span class="number">-0.2277</span>, <span class="number">-0.0276</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([[ <span class="number">0.0345</span>, <span class="number">-0.0294</span>, <span class="number">-0.1481</span>,  <span class="number">0.4977</span>]], requires_grad=<span class="literal">True</span>)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([<span class="number">0.1952</span>], requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>PyTorch中为我们预先实现好了很多loss函数，比方说<code>MSELoss</code>和<code>CrossEntropyLoss</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mse_loss_fn = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">input = torch.tensor([[<span class="number">0.</span>, <span class="number">0</span>, <span class="number">0</span>]])</span><br><span class="line">target = torch.tensor([[<span class="number">1.</span>, <span class="number">0</span>, <span class="number">-1</span>]])</span><br><span class="line"></span><br><span class="line">loss = mse_loss_fn(input, target)</span><br><span class="line"></span><br><span class="line">print(loss)</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor(<span class="number">0.6667</span>)</span><br></pre></td></tr></table></figure>

<h2 id="torch-optim"><a href="#torch-optim" class="headerlink" title="torch.optim"></a>torch.optim</h2><p>PyTorch实现了很多优化方法。在使用时，你最少也要指定模型参数和学习率。</p>
<p>优化器虽好，但它并不会自动帮你计算梯度。so，你自己要记得调用一下<code>backward()</code>奥对了，在运行这个之前还要记得调用一下<code>optim.zero_grad()&#39;初始化一下.grad里头的变量。这相当于对所有.grad里头的变量做</code>detach_()<code>和</code>zero_()`</p>
<p><code>torch.optim</code><a href="https://pytorch.org/docs/stable/optim.html" target="_blank" rel="noopener">docs</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># create a simple model</span></span><br><span class="line">model = nn.Linear(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create a simple dataset</span></span><br><span class="line">X_simple = torch.tensor([[<span class="number">1.</span>]])</span><br><span class="line">y_simple = torch.tensor([[<span class="number">2.</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># create our optimizer</span></span><br><span class="line">optim = torch.optim.SGD(model.parameters(), lr=<span class="number">1e-2</span>)</span><br><span class="line">mse_loss_fn = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">y_hat = model(X_simple)</span><br><span class="line">print(<span class="string">'model params before:'</span>, model.weight)</span><br><span class="line">loss = mse_loss_fn(y_hat, y_simple)</span><br><span class="line">optim.zero_grad()</span><br><span class="line">loss.backward()</span><br><span class="line">optim.step()</span><br><span class="line">print(<span class="string">'model params after:'</span>, model.weight)</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model params before: Parameter containing:</span><br><span class="line">tensor([[<span class="number">0.7107</span>]], requires_grad=<span class="literal">True</span>)</span><br><span class="line">model params after: Parameter containing:</span><br><span class="line">tensor([[<span class="number">0.7547</span>]], requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>可以看到，参数向正确的方向变化。</p>
<h2 id="用梯度下降、自动求导与PyTorch模块实现线性回归"><a href="#用梯度下降、自动求导与PyTorch模块实现线性回归" class="headerlink" title="用梯度下降、自动求导与PyTorch模块实现线性回归"></a>用梯度下降、自动求导与PyTorch模块实现线性回归</h2><p>现在，咱们把前面学的一堆东西揉在一起，用PyTorchic的方式实现线性回归。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">step_size = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line">linear_module = nn.Linear(d, <span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">loss_func = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">optim = torch.optim.SGD(linear_module.parameters(), lr=step_size)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'iter,\tloss,\tw'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">    y_hat = linear_module(X)</span><br><span class="line">    loss = loss_func(y_hat, y)</span><br><span class="line">    optim.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optim.step()</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">'&#123;&#125;,\t&#123;:.2f&#125;,\t&#123;&#125;'</span>.format(i, loss.item(), linear_module.weight.view(<span class="number">2</span>).detach().numpy()))</span><br><span class="line"></span><br><span class="line">print(<span class="string">'\ntrue w\t\t'</span>, true_w.view(<span class="number">2</span>).numpy())</span><br><span class="line">print(<span class="string">'estimated w\t'</span>, linear_module.weight.view(<span class="number">2</span>).detach().numpy())</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">iter,	loss,	w</span><br><span class="line"><span class="number">0</span>,	<span class="number">3.38</span>,	[<span class="number">-0.06555872</span>  <span class="number">0.9564365</span> ]</span><br><span class="line"><span class="number">1</span>,	<span class="number">2.09</span>,	[<span class="number">-0.25302264</span>  <span class="number">1.1884048</span> ]</span><br><span class="line"><span class="number">2</span>,	<span class="number">1.30</span>,	[<span class="number">-0.40178707</span>  <span class="number">1.3695917</span> ]</span><br><span class="line"><span class="number">3</span>,	<span class="number">0.81</span>,	[<span class="number">-0.5199211</span>  <span class="number">1.5110494</span>]</span><br><span class="line"><span class="number">4</span>,	<span class="number">0.50</span>,	[<span class="number">-0.61379874</span>  <span class="number">1.6214342</span> ]</span><br><span class="line"><span class="number">5</span>,	<span class="number">0.32</span>,	[<span class="number">-0.68845654</span>  <span class="number">1.7075248</span> ]</span><br><span class="line"><span class="number">6</span>,	<span class="number">0.20</span>,	[<span class="number">-0.74787635</span>  <span class="number">1.774628</span>  ]</span><br><span class="line"><span class="number">7</span>,	<span class="number">0.13</span>,	[<span class="number">-0.79520756</span>  <span class="number">1.8268977</span> ]</span><br><span class="line"><span class="number">8</span>,	<span class="number">0.08</span>,	[<span class="number">-0.83294225</span>  <span class="number">1.8675839</span> ]</span><br><span class="line"><span class="number">9</span>,	<span class="number">0.06</span>,	[<span class="number">-0.8630534</span>  <span class="number">1.8992288</span>]</span><br><span class="line"><span class="number">10</span>,	<span class="number">0.04</span>,	[<span class="number">-0.8871039</span>  <span class="number">1.9238206</span>]</span><br><span class="line"><span class="number">11</span>,	<span class="number">0.03</span>,	[<span class="number">-0.9063326</span>  <span class="number">1.9429132</span>]</span><br><span class="line"><span class="number">12</span>,	<span class="number">0.02</span>,	[<span class="number">-0.921722</span>   <span class="number">1.9577209</span>]</span><br><span class="line"><span class="number">13</span>,	<span class="number">0.02</span>,	[<span class="number">-0.9340517</span>  <span class="number">1.9691923</span>]</span><br><span class="line"><span class="number">14</span>,	<span class="number">0.02</span>,	[<span class="number">-0.9439409</span>  <span class="number">1.9780676</span>]</span><br><span class="line"><span class="number">15</span>,	<span class="number">0.01</span>,	[<span class="number">-0.95188165</span>  <span class="number">1.9849249</span> ]</span><br><span class="line"><span class="number">16</span>,	<span class="number">0.01</span>,	[<span class="number">-0.9582653</span>  <span class="number">1.9902146</span>]</span><br><span class="line"><span class="number">17</span>,	<span class="number">0.01</span>,	[<span class="number">-0.9634034</span>  <span class="number">1.9942878</span>]</span><br><span class="line"><span class="number">18</span>,	<span class="number">0.01</span>,	[<span class="number">-0.9675441</span>  <span class="number">1.9974183</span>]</span><br><span class="line"><span class="number">19</span>,	<span class="number">0.01</span>,	[<span class="number">-0.9708851</span>  <span class="number">1.9998189</span>]</span><br><span class="line"></span><br><span class="line">true w		 [<span class="number">-1.</span>  <span class="number">2.</span>]</span><br><span class="line">estimated w	 [<span class="number">-0.9708851</span>  <span class="number">1.9998189</span>]</span><br></pre></td></tr></table></figure>

<h2 id="使用SGD完成线性回归"><a href="#使用SGD完成线性回归" class="headerlink" title="使用SGD完成线性回归"></a>使用SGD完成线性回归</h2><p>在上一个例子中，咱们计算在整个数据集上的平均梯度(Gradient Decent)。我们可以通过简单修改实现随机梯度下降(Stochastic Gradient Descent)。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">step_size = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line">linear_module = nn.Linear(d, <span class="number">1</span>)</span><br><span class="line">loss_func = nn.MSELoss()</span><br><span class="line">optim = torch.optim.SGD(linear_module.parameters(), lr=step_size)</span><br><span class="line">print(<span class="string">'iter,\tloss,\tw'</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">200</span>):</span><br><span class="line">    rand_idx = np.random.choice(n) <span class="comment"># take a random point from the dataset</span></span><br><span class="line">    x = X[rand_idx] </span><br><span class="line">    y_hat = linear_module(x)</span><br><span class="line">    loss = loss_func(y_hat, y[rand_idx]) <span class="comment"># only compute the loss on the single point</span></span><br><span class="line">    optim.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optim.step()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'&#123;&#125;,\t&#123;:.2f&#125;,\t&#123;&#125;'</span>.format(i, loss.item(), linear_module.weight.view(<span class="number">2</span>).detach().numpy()))</span><br><span class="line"></span><br><span class="line">print(<span class="string">'\ntrue w\t\t'</span>, true_w.view(<span class="number">2</span>).numpy())</span><br><span class="line">print(<span class="string">'estimated w\t'</span>, linear_module.weight.view(<span class="number">2</span>).detach().numpy())</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">iter,	loss,	w</span><br><span class="line"><span class="number">0</span>,	<span class="number">17.04</span>,	[ <span class="number">0.5124521</span>  <span class="number">-0.06253883</span>]</span><br><span class="line"><span class="number">20</span>,	<span class="number">0.52</span>,	[<span class="number">0.05047676</span> <span class="number">0.4122801</span> ]</span><br><span class="line"><span class="number">40</span>,	<span class="number">0.10</span>,	[<span class="number">-0.29989475</span>  <span class="number">0.9859146</span> ]</span><br><span class="line"><span class="number">60</span>,	<span class="number">2.08</span>,	[<span class="number">-0.65285033</span>  <span class="number">1.3815426</span> ]</span><br><span class="line"><span class="number">80</span>,	<span class="number">0.07</span>,	[<span class="number">-0.787713</span>   <span class="number">1.4951732</span>]</span><br><span class="line"><span class="number">100</span>,	<span class="number">0.28</span>,	[<span class="number">-0.90760225</span>  <span class="number">1.683282</span>  ]</span><br><span class="line"><span class="number">120</span>,	<span class="number">0.01</span>,	[<span class="number">-0.92447585</span>  <span class="number">1.8040558</span> ]</span><br><span class="line"><span class="number">140</span>,	<span class="number">0.00</span>,	[<span class="number">-0.9492291</span>  <span class="number">1.8816731</span>]</span><br><span class="line"><span class="number">160</span>,	<span class="number">0.01</span>,	[<span class="number">-0.9679263</span>  <span class="number">1.9195584</span>]</span><br><span class="line"><span class="number">180</span>,	<span class="number">0.01</span>,	[<span class="number">-0.9705014</span>  <span class="number">1.9513198</span>]</span><br><span class="line"></span><br><span class="line">true w		 [<span class="number">-1.</span>  <span class="number">2.</span>]</span><br><span class="line">estimated w	 [<span class="number">-0.9696742</span>  <span class="number">1.9616756</span>]</span><br></pre></td></tr></table></figure>

<p>其中，咱们通过<code>np.random.choice(n)</code>随机选取一个数据用以更新而不是基于整个数据集更新。（这边相当于是针对data的stachastic）。</p>
<h2 id="CrossEntropyLoss"><a href="#CrossEntropyLoss" class="headerlink" title="CrossEntropyLoss"></a>CrossEntropyLoss</h2><p>现在，咱们用CrossEntropy作为loss函数来进行一个分类任务。</p>
<p>PyTorch在<a href="https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss" target="_blank" rel="noopener">CrossEntropyLoss</a>模块中预先实现了一个版本的cross entropy，它的用法和MSE稍微有点不同，所以这儿详细瞎逼逼一下它的参数：</p>
<ul>
<li><p>input:第一个参数就是咱们的分类神经网络的原始输出，它应该是一个维度为(N, C)的张量。其中，N是minibatch的大小，而C是class的数量。其中，第二维的数据是当前输入被分到各类别的没有normalize过的原始打分。CrossEntropyLoss模块自动为我们计算softmax，因此我们不需要自己算。</p>
</li>
<li><p>output:第二个参数是数据对应的label，用的时候应该输入一个长度为N的张量。每个维度上是正确的类别。</p>
</li>
</ul>
<p>下方代码展示三个预测在CrossEntropyLoss上的打分。正确的label为$y=[1, 1, 0]$。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loss = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">input = torch.tensor([[<span class="number">-1.</span>, <span class="number">1</span>],[<span class="number">-1</span>, <span class="number">1</span>],[<span class="number">1</span>, <span class="number">-1</span>]]) <span class="comment"># raw scores correspond to the correct class</span></span><br><span class="line"><span class="comment"># input = torch.tensor([[-3., 3],[-3, 3],[3, -3]]) # raw scores correspond to the correct class with higher confidence</span></span><br><span class="line"><span class="comment"># input = torch.tensor([[1., -1],[1, -1],[-1, 1]]) # raw scores correspond to the incorrect class</span></span><br><span class="line"><span class="comment"># input = torch.tensor([[3., -3],[3, -3],[-3, 3]]) # raw scores correspond to the incorrect class with incorrectly placed confidence</span></span><br><span class="line"></span><br><span class="line">target = torch.tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>])</span><br><span class="line">output = loss(input, target)</span><br><span class="line">print(output)</span><br></pre></td></tr></table></figure>



<p>输出:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt;</span><br><span class="line">tensor(<span class="number">0.1269</span>)</span><br></pre></td></tr></table></figure>

<h2 id="动态改变学习速率"><a href="#动态改变学习速率" class="headerlink" title="动态改变学习速率"></a>动态改变学习速率</h2><p>通常，我们不希望在训练过程中全程采用相同的学习速率。PyTorch提供相应组件支持根据训练进度自动调整学习速率。通常的策略包括每个epoch对学习速率lr乘以一个比率（比如0.9），并在训练loss下降地没那么快时将学习速率减半。</p>
<p>查看<a href="https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate" target="_blank" rel="noopener">learning rate scheduler docs</a>获取更多信息。</p>
<h2 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h2><p>当数据是图片时，我们通常希望使用卷积操作来怼它。PyTorh在<code>torch.nn.Conv2d</code>模块中实现了卷积操作。用户需要传入一个$(N,C_{in},H_{in},W_{in})$。其中，$N$是batch大小，$C_{in}$是通道数，$H_{in}$和$W_{in}$分别是输入图片的高和宽。</p>
<p>通过自定义以下参数，我们可以修改卷积操作：</p>
<ul>
<li>卷积核大小</li>
<li>步长</li>
<li>填充</li>
</ul>
<p>这些参数对输出张量的维度有影响，所以应该小心。</p>
<p>在<a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d" target="_blank" rel="noopener"><code>torch.nn.Conv2d</code> docs</a>中查看更多信息。</p>
<p>栗子：</p>
<p>在图片上怼一个gaussian blur kernel：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># a gaussian blur kernel</span></span><br><span class="line">gaussian_kernel = torch.tensor([[<span class="number">1.</span>, <span class="number">2</span>, <span class="number">1</span>],[<span class="number">2</span>, <span class="number">4</span>, <span class="number">2</span>],[<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>]]) / <span class="number">16.0</span></span><br><span class="line"></span><br><span class="line">conv = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># manually set the conv weight</span></span><br><span class="line">conv.weight.data[:] = gaussian_kernel</span><br><span class="line"></span><br><span class="line">convolved = conv(image_torch)</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">'original image'</span>)</span><br><span class="line">plt.imshow(image_torch.view(<span class="number">28</span>,<span class="number">28</span>).detach().numpy())</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">'blurred image'</span>)</span><br><span class="line">plt.imshow(convolved.view(<span class="number">26</span>,<span class="number">26</span>).detach().numpy())</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>读取一张RGB图片，输入通道数为3，输出通道数为16，在经过一个激活函数处理后，代码部分展示的输出结果又可以作为一个<code>Conv2d</code>的输入。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">im_channels = <span class="number">3</span> <span class="comment"># if we are working with RGB images, there are 3 input channels, with black and white, 1</span></span><br><span class="line">out_channels = <span class="number">16</span> <span class="comment"># this is a hyperparameter we can tune</span></span><br><span class="line">kernel_size = <span class="number">3</span> <span class="comment"># this is another hyperparameter we can tune</span></span><br><span class="line">batch_size = <span class="number">4</span></span><br><span class="line">image_width = <span class="number">32</span></span><br><span class="line">image_height = <span class="number">32</span></span><br><span class="line"></span><br><span class="line">im = torch.randn(batch_size, im_channels, image_width, image_height)</span><br><span class="line"></span><br><span class="line">m = nn.Conv2d(im_channels, out_channels, kernel_size)</span><br><span class="line">convolved = m(im) <span class="comment"># it is a module so we can call it</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'im shape'</span>, im.shape)</span><br><span class="line">print(<span class="string">'convolved im shape'</span>, convolved.shape)</span><br></pre></td></tr></table></figure>

<h2 id="一些蛮有用的链接"><a href="#一些蛮有用的链接" class="headerlink" title="一些蛮有用的链接"></a>一些蛮有用的链接</h2><ul>
<li><p><a href="https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html" target="_blank" rel="noopener">60 minute PyTorch Tutorial</a></p>
</li>
<li><p><a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">PyTorch Docs</a></p>
</li>
<li><p><a href="https://courses.cs.washington.edu/courses/cse446/19wi/notes/auto-diff.pdf" target="_blank" rel="noopener">Lecture notes on Auto-Diff</a></p>
</li>
</ul>
<h2 id="数据类"><a href="#数据类" class="headerlink" title="数据类"></a>数据类</h2><p><code>torch.utils.data.Dataset</code>是一个抽象类，咱们如果要弄一个自己的数据集的话就需要继承<code>Dataset</code>，然后覆盖以下的方法：</p>
<ul>
<li><code>__len__</code>，确保<code>len(dataset)</code>可以正确返回数据集的大小。</li>
<li><code>__getitem__</code>用来实现索引，确保dataset[i]可以正确拿到第i个数据。</li>
</ul>
<p>咱们现在来搞一个face landmarks数据集。咱们在<code>__init__</code>里头读取csv文件，但把读取图片的任务留给<code>__getitem__</code>。这样做的好处是节省内存，不需要将所有数据一下子全读到内存里去。</p>
<p>咱们数据集的sample是一个dict:<code>{&#39;image&#39;: image, &#39;landmarks&#39;: landmarks}</code>。咱们数据集还要再弄一个<code>transform</code>方法，这样任何需要的操作都可以被施加在图片上。咱们在下一节里头看看它的用处。</p>
<p>栗子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FaceLandmarksDataset</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    <span class="string">"""Face Landmarks dataset."""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, csv_file, root_dir, transform=None)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            csv_file (string): Path to the csv file with annotations.</span></span><br><span class="line"><span class="string">            root_dir (string): Directory with all the images.</span></span><br><span class="line"><span class="string">            transform (callable, optional): Optional transform to be applied</span></span><br><span class="line"><span class="string">                on a sample.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.landmarks_frame = pd.read_csv(csv_file)</span><br><span class="line">        self.root_dir = root_dir</span><br><span class="line">        self.transform = transform</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.landmarks_frame)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, idx)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> torch.is_tensor(idx):</span><br><span class="line">            idx = idx.tolist()</span><br><span class="line"></span><br><span class="line">        img_name = os.path.join(self.root_dir,</span><br><span class="line">                                self.landmarks_frame.iloc[idx, <span class="number">0</span>])</span><br><span class="line">        image = io.imread(img_name)</span><br><span class="line">        landmarks = self.landmarks_frame.iloc[idx, <span class="number">1</span>:]</span><br><span class="line">        landmarks = np.array([landmarks])</span><br><span class="line">        landmarks = landmarks.astype(<span class="string">'float'</span>).reshape(<span class="number">-1</span>, <span class="number">2</span>)</span><br><span class="line">        sample = &#123;<span class="string">'image'</span>: image, <span class="string">'landmarks'</span>: landmarks&#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.transform:</span><br><span class="line">            sample = self.transform(sample)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> sample</span><br></pre></td></tr></table></figure>

<p>然鹅，如果咱们直接用<code>for</code>把整个数据走一遍会少掉很多东西，比如:</p>
<ul>
<li>按batch读取data</li>
<li>随机读取data</li>
<li>多线程读取data</li>
</ul>
<p>不过施主先不要着急，这些功能<code>torch.utils.data.DataLoader</code>都有。通过修改<code>collate_fn</code>参数，你可以指定数据按什么样的batch方式被读取，不过默认的读取方法已经可以cover大部分的需求了。</p>
<p>🌰：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataloader = DataLoader(transformed_dataset, batch_size=<span class="number">4</span>,</span><br><span class="line">                        shuffle=<span class="literal">True</span>, num_workers=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i_batch, sample_batched <span class="keyword">in</span> enumerate(dataloader):</span><br><span class="line">    print(i_batch, sample_batched[<span class="string">'image'</span>].size(),</span><br><span class="line">          sample_batched[<span class="string">'landmarks'</span>].size())</span><br></pre></td></tr></table></figure>

<h2 id="混合精度训练"><a href="#混合精度训练" class="headerlink" title="混合精度训练"></a>混合精度训练</h2><p>作者: <code>Chi-Liang Liu</code> 引用: <a href="https://github.com/NVIDIA/apex。采用混合精度训练你的模型，训练出来的神经网络可以：" target="_blank" rel="noopener">https://github.com/NVIDIA/apex。采用混合精度训练你的模型，训练出来的神经网络可以：</a></p>
<ul>
<li>运行速度快2-4倍</li>
<li>几行代码就可以省下一笔买新内存的钱</li>
</ul>
<h2 id="Apex"><a href="#Apex" class="headerlink" title="Apex"></a>Apex</h2><p>nvidia维护的工具简化了Pytorch中的混合精度和分布式培训。这里的一些代码最终将包含在上游Pytorch中。Apex的目的是尽快为用户提供最新的实用工具。</p>
<h2 id="apex-amp"><a href="#apex-amp" class="headerlink" title="apex.amp"></a>apex.amp</h2><p>Amp允许用户轻松地尝试不同的纯模式和混合精度模式。通过选择“优化级”或opt_level选择常用的默认模式;每个opt_level建立一组属性来管理Amp实现的纯精度或混合精度训练。通过将特定属性的值直接传递给amp.initialize，可以实现对给定opt_level行为方式的细粒度控制。这些手动指定的值覆盖opt_level建立的默认值。</p>
<p>🌰:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Declare model and optimizer as usual, with default (FP32) precision</span></span><br><span class="line">model = torch.nn.Linear(D_in, D_out).cuda()</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">1e-3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Allow Amp to perform casts as required by the opt_level</span></span><br><span class="line">model, optimizer = amp.initialize(model, optimizer, opt_level=<span class="string">"O1"</span>)</span><br><span class="line">...</span><br><span class="line"><span class="comment"># loss.backward() becomes:</span></span><br><span class="line"><span class="keyword">with</span> amp.scale_loss(loss, optimizer) <span class="keyword">as</span> scaled_loss:</span><br><span class="line">    scaled_loss.backward()</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

]]></content>
      <tags>
        <tag>李宏毅</tag>
      </tags>
  </entry>
  <entry>
    <title>Pandas入门</title>
    <url>/2020/03/20/Pandas%E5%85%A5%E9%97%A8/</url>
    <content><![CDATA[<h1 id="Pandas"><a href="#Pandas" class="headerlink" title="Pandas"></a>Pandas</h1><h2 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h2><p>Pandas中的基础数据类型有Series和DataFrame，其中，Series具有一维索引（行索引），DataFrame具有二维索引（行索引和列索引）。</p>
<h3 id="Series"><a href="#Series" class="headerlink" title="Series"></a>Series</h3><p>既然是一维索引，就要有key,value对，可以直接传入一个dict。通过一个key，就可以确定一个value。<br><code>s = pd.Series({&#39;a&#39;: 10, &#39;b&#39;: 20, &#39;c&#39;: 30})</code></p>
<h3 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h3><p>需要行和列才可以确定一个元素。<br>可以按列传入</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">12df &#x3D; pd.DataFrame(&#123;&#39;one&#39;: pd.Series([1, 2, 3]),</span><br><span class="line">                   &#39;two&#39;: pd.Series([4, 5, 6])&#125;)</span><br></pre></td></tr></table></figure>

<p>也可以按行传入</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">123df &#x3D; pd.DataFrame([&#123;&#39;one&#39;: 1, &#39;two&#39;: 4&#125;,</span><br><span class="line">                   &#123;&#39;one&#39;: 2, &#39;two&#39;: 5&#125;,</span><br><span class="line">                   &#123;&#39;one&#39;: 3, &#39;two&#39;: 6&#125;])</span><br></pre></td></tr></table></figure>

<h2 id="数据读取"><a href="#数据读取" class="headerlink" title="数据读取"></a>数据读取</h2><p><a href="https://pandas.pydata.org/pandas-docs/stable/reference/io.html" target="_blank" rel="noopener">Input/output — pandas 1.0.1 documentation</a></p>
<h3 id="读取csv"><a href="#读取csv" class="headerlink" title="读取csv"></a>读取csv</h3><p><code>f = pd.read_csv()</code><br><img src="/" alt class="lazyload" data-src="/2020/03/20/Pandas%E5%85%A5%E9%97%A8/1.png"></p>
<h2 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h2><ul>
<li><code>df.head()</code>显示文件头部</li>
<li><code>df.tail()</code>显示文件尾部</li>
<li><code>df.describe()</code>显示数据概览</li>
<li><code>df.values</code>将DataFrame转换为numpy数组</li>
<li><code>df.index</code>行索引</li>
<li><code>df.columns</code>列索引</li>
<li><code>df.shape</code>DataFrame形状<br>所有属性：<a href="https://pandas.pydata.org/pandas-docs/stable/reference/frame.html#attributes-and-underlying-data" target="_blank" rel="noopener">DataFrame — pandas 1.0.1 documentation</a></li>
</ul>
<h2 id="数据选择"><a href="#数据选择" class="headerlink" title="数据选择"></a>数据选择</h2><p>数据选择就是将原始数据中的一部分拿出来，用于后续处理步骤</p>
<h3 id="基于索引数字选择"><a href="#基于索引数字选择" class="headerlink" title="基于索引数字选择"></a>基于索引数字选择</h3><ul>
<li><code>df.iloc[:3]</code> 选择前三行</li>
<li><code>df.iloc[5]</code>选择某一行</li>
<li><code>df.iloc[[行], [列]]</code> 其中，行和列都可以是list</li>
<li><code>df.iloc[:，1:4]</code>选择2到4列（切片时左闭右开）</li>
</ul>
<h3 id="基于标签的选择-切片时左右都闭"><a href="#基于标签的选择-切片时左右都闭" class="headerlink" title="基于标签的选择(切片时左右都闭)"></a>基于标签的选择(切片时左右都闭)</h3><ul>
<li><code>df.loc[0:2]</code>选择前三行</li>
<li><code>df.loc[[0,2,4]]</code>选择1、3、5行</li>
<li><code>df.loc[:,&#39;Total Population&#39;:&#39;Total Males&#39;]</code></li>
<li><code>df.loc[[0, 2], &#39;Median Age&#39;:]</code>选择1、3行的’Median Age’后面的列</li>
</ul>
<h3 id="数据删减"><a href="#数据删减" class="headerlink" title="数据删减"></a>数据删减</h3><ul>
<li><p><code>df.drop(labels=[&#39;Median Age&#39;, &#39;Total Males&#39;], axis = 1)</code>删除两个指定列</p>
</li>
<li><p><code>df.drop(labels=[0, 1], axis = 0)</code>删除0、1行</p>
</li>
<li><p><code>df.drop_dupicates()</code>数据去重</p>
</li>
<li><p><code>df.dropna()</code>数据去空值</p>
</li>
<li><p><code>df.insert(value = 要插入的值, loc=列号，column = &#39;列名&#39;)</code>在DataFrame中插入名字叫’列名’，值为value的一列。</p>
</li>
<li><p><code>df.isna()</code>返回bool</p>
</li>
<li><p><code>df.notna()</code>返回bool列表<br>填充缺失值</p>
</li>
<li><p><code>df.fillna()</code></p>
</li>
<li><p><code>df.fillna(method=&#39;pad&#39;)</code>使用前面的值填充空值</p>
</li>
<li><p>‘df.fillna(method=‘bfill’)’使用后面的值填充</p>
</li>
<li><p><code>df.fillna(df.mean()[&#39;C&#39;:&#39;E&#39;])</code>使用平均值填充<br>插值填充</p>
</li>
<li><pre><code>df.interpolate(method=)</code></pre><ul>
<li><code>method=&#39;quadratic&#39;</code>数据增长速率越来越快</li>
<li><code>method=&#39;pchip&#39;</code>累计分布</li>
<li><code>method=&#39;akima&#39;</code>以平滑绘图为目的<br>绘图</li>
</ul>
</li>
<li><p>df.plot(kind=‘bar’…)直接绘图<br>其它</p>
</li>
<li><p>数据计算 <a href="https://pandas.pydata.org/pandas-docs/stable/reference/frame.html#binary-operator-functions" target="_blank" rel="noopener">DataFrame — pandas 1.0.1 documentation</a></p>
</li>
<li><p>数据聚合 <a href="https://pandas.pydata.org/pandas-docs/stable/reference/frame.html#function-application-groupby-window" target="_blank" rel="noopener">DataFrame — pandas 1.0.1 documentation</a></p>
</li>
<li><p>统计分析 <a href="https://pandas.pydata.org/pandas-docs/stable/reference/frame.html#computations-descriptive-stats" target="_blank" rel="noopener">DataFrame — pandas 1.0.1 documentation</a></p>
</li>
<li><p>时间序列 <a href="https://pandas.pydata.org/pandas-docs/stable/reference/frame.html#time-series-related" target="_blank" rel="noopener">DataFrame — pandas 1.0.1 documentation</a></p>
</li>
</ul>
]]></content>
      <tags>
        <tag>Pandas</tag>
      </tags>
  </entry>
</search>
